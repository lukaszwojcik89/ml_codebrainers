{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasyfikacja obrazów za pomocą CNN i zbioru danych Imagenette\n",
    "\n",
    "&#x20;\n",
    "\n",
    "## Wprowadzenie\n",
    "\n",
    "Konwolucyjne sieci neuronowe (ang. *Convolutional Neural Networks*, **CNN**) to obecnie standardowa technika w zadaniach wizji komputerowej opartych na uczeniu głębokim. CNN wykorzystują warstwy splotowe (konwolucyjne), które automatycznie uczą się wychwytywać istotne cechy z obrazów (np. krawędzie, wzory, kształty) poprzez **filtry konwolucyjne**. Dzięki operacji **poolingu** (najczęściej *maksymalnego poolingowania*), sieć redukuje wymiarowość danych, zachowując najważniejsze informacje. Na końcu sieć zwykle posiada jedną lub więcej **warstw gęstych** (*fully connected*), które na podstawie wyekstrahowanych cech dokonują klasyfikacji do określonych klas. Taka architektura sprawia, że CNN świetnie radzą sobie z rozpoznawaniem obrazów, przewyższając tradycyjne metody opierające się na ręcznie tworzonych cechach.\n",
    "\n",
    "**Imagenette** to niewielki zbiór danych obrazowych przygotowany przez zespół [fast.ai](http://fast.ai) jako uproszczona wersja słynnego zestawu **ImageNet**. Zawiera on około 13 tysięcy obrazów należących do 10 różnych, łatwo rozróżnialnych klas pochodzących z oryginalnego ImageNetu. Dzięki ograniczeniu liczby klas i przykładów, Imagenette umożliwia szybkie eksperymenty z modelami rozpoznawania obrazów, zanim zastosuje się je na pełnym, znacznie większym zbiorze ImageNet. Wśród kategorii obrazów w Imagenette znajdują się między innymi:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.researchgate.net/publication/368652757/figure/fig2/AS:11431281121137743@1676862920482/Example-images-of-the-Imagenette-dataset-with-their-labels.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "W kolejnych sekcjach przejdziemy przez następujące kroki:\n",
    "\n",
    "1. Pobranie i przygotowanie zbioru danych **Imagenette**.\n",
    "2. Wczytanie danych obrazowych z dysku przy użyciu generatorów Keras (`ImageDataGenerator`).\n",
    "3. Zdefiniowanie architektury konwolucyjnej sieci neuronowej od podstaw.\n",
    "4. Kompilacja i trenowanie modelu .\n",
    "5. Ewaluacja modelu na danych walidacyjnych i ocena uzyskanej dokładności."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pobieranie i przygotowanie danych\n",
    "\n",
    "Zacznijmy od pobrania zbioru Imagenette. Dane są dostępne publicznie – zbiór w wersji **160 px** (najkrótszy bok obrazów zeskalowany do 160 pikseli) można pobrać jako spakowany plik `.tgz` z zasobów [fast.ai](http://fast.ai). W poniższej komórce użyjemy polecenia `wget`, aby pobrać ten plik, a następnie `tar`, aby go rozpakować do katalogu lokalnego. Komendy `!wget` i `!tar` zostaną wykonane w środowisku notebooka jako polecenia systemowe (poprzedzone wykrzyknikiem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobranie pliku z danymi Imagenette (wersja 160px)\n",
    "!wget -q --show-progress https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\n",
    "\n",
    "# Rozpakowanie archiwum .tgz do bieżącego katalogu\n",
    "!tar -xzf imagenette2-160.tgz\n",
    "\n",
    "# Usunięcie pobranego archiwum (opcjonalnie, aby zaoszczędzić miejsce)\n",
    "!rm -f imagenette2-160.tgz\n",
    "\n",
    "# Sprawdzenie zawartości rozpakowanego katalogu\n",
    "!find imagenette2-160 -maxdepth 2 -type d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*Powyżej:* Najpierw pobieramy plik `imagenette2-160.tgz` (ok. 155 MB) poleceniem `wget`. Następnie rozpakowujemy go poleceniem `tar -xzf`. Po rozpakowaniu powinniśmy otrzymać katalog `imagenette2-160`, zawierający dwa podkatalogi: `train` (obrazy treningowe) oraz `val` (obrazy walidacyjne), a także plik `noisy_imagenette.csv` z informacjami o etykietach (z którego tutaj nie będziemy korzystać). Na końcu, dla porządku, usuwamy zbędne już archiwum oraz wypisujemy strukturę katalogów (do dwóch poziomów w głąb) aby zweryfikować, że dane zostały pobrane poprawnie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Po wykonaniu powyższych poleceń, struktura katalogów powinna wyglądać następująco:\n",
    "\n",
    "```\n",
    "\n",
    "imagenette2-160/\n",
    "├── train/\n",
    "│   ├── n01440764/   (obrazy klasy \"tench\")\n",
    "│   ├── n02102040/   (obrazy klasy \"English springer\")\n",
    "│   ├── n02979186/   (obrazy klasy \"cassette player\")\n",
    "│   ├── n03000684/   (obrazy klasy \"chain saw\")\n",
    "│   ├── n03028079/   (obrazy klasy \"church\")\n",
    "│   ├── n03394916/   (obrazy klasy \"French horn\")\n",
    "│   ├── n03417042/   (obrazy klasy \"garbage truck\")\n",
    "│   ├── n03425413/   (obrazy klasy \"gas pump\")\n",
    "│   ├── n03445777/   (obrazy klasy \"golf ball\")\n",
    "│   └── n03888257/   (obrazy klasy \"parachute\")\n",
    "└── val/\n",
    "    ├── n01440764/   (obrazy klasy \"tench\")\n",
    "    ├── n02102040/   (obrazy klasy \"English springer\")\n",
    "    ├── n02979186/   (obrazy klasy \"cassette player\")\n",
    "    ├── n03000684/   (obrazy klasy \"chain saw\")\n",
    "    ├── n03028079/   (obrazy klasy \"church\")\n",
    "    ├── n03394916/   (obrazy klasy \"French horn\")\n",
    "    ├── n03417042/   (obrazy klasy \"garbage truck\")\n",
    "    ├── n03425413/   (obrazy klasy \"gas pump\")\n",
    "    ├── n03445777/   (obrazy klasy \"golf ball\")\n",
    "    └── n03888257/   (obrazy klasy \"parachute\")\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Widzimy, że w katalogach `train` i `val` znajdują się podfoldery nazwane kodami WordNet (np. `n01440764`), odpowiadającymi poszczególnym klasom obrazów. Katalog `train` zawiera obrazy treningowe (około 70% danych), a `val` obrazy walidacyjne (około 30% danych) zgodnie z przygotowanym podziałem. Teraz, gdy dane są już na dysku, możemy przejść do ich wczytania i przygotowania do treningu sieci neuronowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Przygotowanie danych z użyciem `ImageDataGenerator`\n",
    "\n",
    "Do wczytywania obrazów skorzystamy z klasy **ImageDataGenerator** dostarczanej przez API Keras (część biblioteki TensorFlow). `ImageDataGenerator` umożliwia **wczytywanie obrazów bezpośrednio z folderów** oraz opcjonalne zastosowanie przekształceń zwiększających różnorodność danych treningowych (*augmentacja danych*), takich jak odbicia lustrzane, rotacje, zmiany jasności itp. W naszym przypadku zastosujemy podstawowe przekształcenia, aby model uczył się większej odporności na zmienność danych.\n",
    "\n",
    "Najpierw zaimportujemy potrzebne moduły biblioteki TensorFlow i Keras, a następnie utworzymy dwa generatory:\n",
    "\n",
    "* **train\\_datagen** – generator dla danych treningowych, z augmentacją (np. losowe odbicie w poziomie) i normalizacją pikseli.\n",
    "* **valid\\_datagen** – generator dla danych walidacyjnych, bez augmentacji (służy jedynie do oceny modelu), z samą normalizacją pikseli.\n",
    "\n",
    "Normalizacja polega tu na przemnożeniu wartości pikseli przez współczynnik `1/255`, tak aby zakres wartości (0-255 dla obrazów RGB) został przeskalowany do przedziału \\[0,1]. Jest to standardowa operacja ułatwiająca trenowanie sieci neuronowej (dzięki temu wszystkie cechy wejściowe mają porównywalny zakres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Ustawienie podstawowych parametrów\n",
    "IMG_WIDTH = 160   # docelowa szerokość obrazów (piksele)\n",
    "IMG_HEIGHT = 160  # docelowa wysokość obrazów (piksele)\n",
    "BATCH_SIZE = 32   # wielkość paczki (batch) wykorzystywanej podczas treningu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Powyżej zdefiniowaliśmy rozmiar obrazów (`160x160` pikseli) oraz wielkość batcha (32). Ponieważ korzystamy z wersji datasetu, gdzie najkrótszy bok obrazów ma 160px, przyjmiemy jednorodny rozmiar 160x160 dla wszystkich wejść sieci (obrazy zostaną automatycznie przeskalowane przez generator, jeśli mają inny rozmiar). Batch size określa, ile próbek będzie przetwarzanych jednocześnie przed zaktualizowaniem wag modelu podczas trenowania.\n",
    "\n",
    "Teraz utworzymy obiekty `ImageDataGenerator` dla zbioru treningowego i walidacyjnego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzenie generatora dla danych treningowych z augmentacją\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0,   # normalizacja pikseli\n",
    "                                   horizontal_flip=True,  # losowe odbicie obrazu w poziomie\n",
    "                                   rotation_range=20,     # losowa rotacja obrazów o maks. 20 stopni\n",
    "                                   zoom_range=0.2,        # losowe przybliżenie obrazu\n",
    "                                   width_shift_range=0.1, # losowe przesunięcie w poziomie o ±10% szerokości\n",
    "                                   height_shift_range=0.1)# losowe przesunięcie w pionie o ±10% wysokości\n",
    "\n",
    "# Generator dla danych walidacyjnych (tylko normalizacja, bez augmentacji)\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0/255.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "W powyższym kodzie:\n",
    "\n",
    "* `rescale=1.0/255.0` – przeskalowuje wartości pikseli.\n",
    "* `horizontal_flip=True` – losowo odwraca część obrazów w poziomie.\n",
    "* `rotation_range=20` – losowo obraca obrazy w zakresie ±20 stopni.\n",
    "* `zoom_range=0.2` – losowo przybliża (zoom) obrazy o maksymalnie 20%.\n",
    "* `width_shift_range` i `height_shift_range` – losowo przesuwa obrazy w poziomie i pionie (do 10% wymiaru).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Te operacje augmentacji będą **losowo** stosowane do obrazów treningowych przy każdym kroku epoki, co efektywnie zwiększa różnorodność danych treningowych i może poprawić uogólnianie się modelu. W zbiorze walidacyjnym natomiast nie chcemy dokonywać żadnych losowych zmian (walidacja powinna odbywać się na stałych danych), dlatego `valid_datagen` zawiera tylko normalizację.\n",
    "\n",
    "Następnie za pomocą metody `flow_from_directory` utworzymy generatory danych obrazowych, które będą bezpośrednio dostarczać obrazy z odpowiednich folderów:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ścieżki do katalogów z danymi\n",
    "train_dir = \"imagenette2-160/train\"\n",
    "valid_dir = \"imagenette2-160/val\"\n",
    "\n",
    "# Tworzenie iteratora/generatora dla danych treningowych\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),  # dopasowanie obrazów do rozmiaru 160x160\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'  # typ etykiet: kategorie (one-hot vektory dla klasyfikacji wieloklasowej)\n",
    ")\n",
    "\n",
    "# Tworzenie iteratora/generatora dla danych walidacyjnych\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Po wykonaniu powyższych poleceń, `flow_from_directory` przeskanuje foldery i wyświetli informacje, ile obrazów znalazł oraz ile klas zidentyfikował. Generatory przypiszą automatycznie etykiety numeryczne dla poszczególnych podfolderów (np. 0 dla `n01440764`, 1 dla `n02102040`, itd.), a dzięki `class_mode='categorical'` etykiety te będą zwracane w postaci wektorów one-hot o długości 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy opcjonalnie sprawdzić, jakie klasy zostały rozpoznane przez generator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mapowanie klas:\", train_generator.class_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Budowa modelu CNN od podstaw\n",
    "\n",
    "Zbudujemy konwolucyjną sieć neuronową korzystając z interfejsu **Keras** (API wysokiego poziomu dostępnego w TensorFlow). Użyjemy najprostszej formy definiowania modelu – sekwencyjnie (*Sequential*), ponieważ nasza architektura będzie stanowić liniowy stos warstw (od wejścia do wyjścia każda warstwa jest podłączona do następnej).\n",
    "\n",
    "Architektura naszej sieci będzie zawierać następujące elementy:\n",
    "\n",
    "* **Warstwy konwolucyjne (Conv2D)** – będą filtrować obrazy przy pomocy zestawu learnowalnych filtrów 2D, wykrywając lokalne wzorce. Użyjemy niewielkich filtrów (np. 3x3) i funkcji aktywacji **ReLU** (*Rectified Linear Unit*) po każdej konwolucji, co nadaje nieliniowość modelowi.\n",
    "* **Warstwy poolingujące (MaxPooling2D)** – będą zmniejszać rozmiary map cech, agregując informacje (weźmiemy maksymalną wartość z okna 2x2). Pooling co pewien czas redukuje rozdzielczość reprezentacji, zmniejszając liczbę parametrów i uwzględniając pewną odporność na przesunięcia obrazu.\n",
    "* **Warstwy gęste (Dense)** – na końcu sieci zastosujemy kilka neuronów w pełni połączonych. Szczególnie ostatnia warstwa będzie miała 10 neuronów (po jednym na każdą klasę) z aktywacją **softmax**, która przekształca wyjścia w prawdopodobieństwa klas.\n",
    "* **Dropout** – dodamy warstwę *dropout* pomiędzy warstwami gęstymi, by przeciwdziałać overfittingowi. Warstwa ta losowo wyłącza pewien procent neuronów podczas treningu (np. 50%), co zmusza sieć do bardziej uogólnionego uczenia się cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Inicjalizacja modelu sequential\n",
    "model = Sequential()\n",
    "\n",
    "# 1. Warstwa konwolucyjna + aktywacja ReLU\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu',\n",
    "                 input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
    "#    - 32 filtrów konwolucyjnych rozmiaru 3x3 pixeli\n",
    "#    - funkcja aktywacji ReLU (Rectified Linear Unit)\n",
    "#    - input_shape = (160, 160, 3) określa wymiar wejścia (kolorowe obrazy 160x160)\n",
    "\n",
    "# 2. Druga warstwa konwolucyjna + ReLU\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n",
    "#    - kolejna warstwa konwolucyjna również z 32 filtrami 3x3\n",
    "\n",
    "# 3. Warstwa pooling (maksymalne poolingowanie 2x2)\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#    - zmniejsza wymiary map cech o połowę (160x160 -> 80x80)\n",
    "#    - bierze maksymalną wartość z każdej siatki 2x2\n",
    "\n",
    "# 4. Warstwa dropout\n",
    "model.add(Dropout(0.25))\n",
    "#    - wyłącza losowo 25% neuronów tej warstwy podczas treningu, co pomaga zapobiegać przeuczeniu\n",
    "\n",
    "# 5. Trzecia warstwa konwolucyjna + ReLU (z większą liczbą filtrów)\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
    "#    - 64 filtry 3x3, szuka bardziej złożonych cech w zredukowanych mapach cech\n",
    "\n",
    "# 6. Czwarta warstwa konwolucyjna + ReLU\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
    "\n",
    "# 7. Druga warstwa pooling 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#    - ponownie zmniejszamy wymiary (80x80 -> 40x40)\n",
    "\n",
    "# 8. Warstwa dropout\n",
    "model.add(Dropout(0.25))\n",
    "#    - znów losowo wyłączamy 25% neuronów na tej warstwie podczas treningu\n",
    "\n",
    "# 9. Spłaszczenie tensora (Flatten)\n",
    "model.add(Flatten())\n",
    "#    - przekształca 3-wymiarowe mapy cech (40x40 z 64 filtrami) na wektor 1D\n",
    "#      celem podłączenia do warstwy gęstej\n",
    "\n",
    "# 10. Warstwa gęsta (fully connected) + ReLU\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "#     - 128 neuronów w pełni połączonych z poprzednią warstwą\n",
    "#     - ReLU jako funkcja aktywacji\n",
    "\n",
    "# 11. Warstwa dropout (50%)\n",
    "model.add(Dropout(0.5))\n",
    "#     - wyłączamy losowo 50% neuronów warstwy Dense podczas treningu, żeby zredukować overfitting\n",
    "\n",
    "# 12. Warstwa wyjściowa (Dense z softmax)\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "#     - 10 neuronów odpowiadających 10 klasom; softmax zamienia wyjścia na prawdopodobieństwa klasy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Kilka uwag do powyższej architektury:\n",
    "\n",
    "* Pierwsza warstwa `Conv2D` określa `input_shape=(160,160,3)`. Kolejne warstwy Keras potrafią już same wywnioskować kształty na podstawie poprzednich, więc nie wymagają jawnego podawania rozmiaru wejścia.\n",
    "* Utrzymujemy względnie małą liczbę filtrów (32, potem 64) oraz dość agresywnie zmniejszamy wymiary za pomocą pooling (dwukrotnie). Dzięki temu liczba parametrów nie rośnie nadmiernie szybko. W ostatniej części sieci (po spłaszczeniu) mamy warstwę Dense 128 neuronów – to stosunkowo niewiele, co również ma ograniczyć złożoność modelu.\n",
    "* Warstwy *Dropout* z ustawieniami 0.25 i 0.5 oznaczają, że podczas każdego kroku uczenia, odpowiednio 1/4 lub 1/2 neuronów z tych warstw jest pomijana. To pomaga uniknąć sytuacji, w której model zbytnio dostosowuje się do danych treningowych kosztem zdolności generalizacji (przeciwdziałanie przeuczeniu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Po zdefiniowaniu modelu warto sprawdzić podsumowanie jego architektury, co możemy zrobić za pomocą metody `model.summary()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Wyświetlenie podsumowania architektury modelu\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ta komenda wyświetli tabelaryczne zestawienie warstw modelu, w tym wymiary wyjść każdej warstwy oraz łączną liczbę parametrów do wytrenowania. Powinniśmy zobaczyć kolejno warstwy konwolucyjne, pooling, dropout itp., aż do warstwy wyjściowej, a na końcu sumaryczną liczbę parametrów (kilkaset tysięcy, w zależności od dokładnej architektury).\n",
    "\n",
    "## Kompilacja i trenowanie modelu\n",
    "\n",
    "Mając zdefiniowany model, musimy go **skompilować**, podając następujące elementy:\n",
    "\n",
    "* **Funkcja kosztu (loss)** – miara błędu, którą sieć będzie minimalizować podczas treningu. W przypadku klasyfikacji wieloklasowej użyjemy *kategoriowej entropii krzyżowej* (**categorical crossentropy**), która porównuje rozkład prawdopodobieństw na wyjściu (softmax) z oczekiwaną etykietą (one-hot).\n",
    "* **Optymalizator** – algorytm aktualizujący wagi sieci na podstawie obliczonego błędu. Wybierzemy popularny optymalizator **Adam**, który adaptacyjnie dostosowuje kroki uczenia dla każdej wagi.\n",
    "* **Metryka oceny** – metryka, którą będziemy śledzić w trakcie treningu. Najbardziej naturalną metryką dla klasyfikacji jest **dokładność** (*accuracy*), czyli odsetek poprawnie sklasyfikowanych obrazów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Po kompilacji, rozpoczniemy trenowanie modelu (**fit**). Przekażemy generator danych treningowych, liczbę epok oraz generator walidacyjny do monitorowania jakości modelu na nieznanych danych podczas treningu. Trening przez co najmniej 30 epok pozwoli modelowi wielokrotnie zobaczyć wszystkie obrazy treningowe i stopniowo poprawiać swoje parametry. W trakcie uczenia Keras będzie wyświetlał postęp – stratę i dokładność na treningu oraz walidacji po każdej epoce, co pozwoli nam obserwować czy model się poprawia i czy nie zaczyna nadmiernie dopasowywać do danych treningowych (gdyby dokładność walidacji przestała rosnąć lub zaczęła spadać, mimo rosnącej dokładności treningowej, byłby to sygnał przeuczenia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kompilacja modelu\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Trenowanie modelu\n",
    "EPOCHS = 30  # liczba epok treningu\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "W powyższym kodzie:\n",
    "\n",
    "* `model.compile(...)` ustawia optymalizator, funkcję straty i metrykę.\n",
    "* `model.fit(...)` rozpoczyna proces uczenia:\n",
    "  * `train_generator` dostarcza batchy danych treningowych (obrazy i one-hot etykiety) w sposób ciągły.\n",
    "  * `epochs=EPOCHS` określa, ile pełnych przebiegów przez zbiór treningowy wykonamy.\n",
    "  * `validation_data=valid_generator` powoduje, że po każdej epoce model zostanie oceniony na danych walidacyjnych (to pozwala śledzić postępy na danych niewidzialnych podczas treningu).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "W miarę postępu epok oczekujemy, że **loss (strata)** będzie malała, a **accuracy (dokładność)** rosła, zarówno dla danych treningowych, jak i walidacyjnych. Jeśli model ma wystarczającą pojemność, to po 30 epokach powinien osiągnąć dość wysoką dokładność na zbiorze treningowym. Ważniejsze jest jednak, jaka będzie **val\\_accuracy** – to ona świadczy o zdolności uogólniania. Przy dobrze dobranej architekturze i parametrach, model powinien uzyskać dokładność walidacyjną prawdopodobnie w okolicach 80-90%. Gdyby dokładność walidacyjna była znacznie niższa od treningowej, oznaczałoby to przeuczenie i wymagało dalszych działań (np. bardziej złożonej augmentacji, mocniejszego dropout, prostszej architektury lub wczesnego zatrzymania treningu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Po zakończeniu treningu, możemy zwizualizować przebieg procesu ucznia (np. wykres strat i dokładności) za pomocą obiektu `history`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Ewaluacja modelu\n",
    "\n",
    "Ostatnim krokiem jest ocena wytrenowanego modelu na zbiorze walidacyjnym (który w naszym przypadku pełni rolę zestawu testowego, gdyż model nie widział tych obrazów podczas treningu). Do oceny użyjemy metody `model.evaluate`, która obliczy wartość funkcji kosztu oraz wskazane metryki dla podanych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ewaluacja modelu na zbiorze walidacyjnym\n",
    "val_loss, val_accuracy = model.evaluate(valid_generator)\n",
    "\n",
    "print(f\"Wynik na zbiorze walidacyjnym: strata = {val_loss:.4f}, dokładność = {val_accuracy*100:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
