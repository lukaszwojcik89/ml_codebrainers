{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b561d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Podstawy NLP – analiza sentymentu z użyciem podstawowych modeli ML&#x20;\n",
    "\n",
    "&#x20;![Natural Language Processing With Python's NLTK Package – Real Python](https://files.realpython.com/media/NLP-for-Beginners-Pythons-Natural-Language-Toolkit-NLTK_Watermarked.16a787c1e9c6.jpg \"Natural Language Processing With Python's NLTK Package – Real Python\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4453c755",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "## Cele edukacyjne\n",
    "\n",
    "* **Zrozumienie czym jest NLP (Przetwarzanie Języka Naturalnego)** – poznasz definicję NLP i przykłady jego zastosowań.\n",
    "* **Wprowadzenie do analizy sentymentu** – dowiesz się, na czym polega analiza sentymentu (opinie pozytywne vs negatywne) i gdzie się ją wykorzystuje.\n",
    "* **Przygotowanie środowiska pracy** – zainstalujesz i zaimportujesz podstawowe narzędzia potrzebne do pracy z danymi tekstowymi (biblioteki Python takie jak *pandas*, *scikit-learn*, *nltk* itp.).\n",
    "* **Eksploracja zbioru danych** – nauczysz się wczytać zbiór danych z recenzjami (np. **IMDB**), podejrzeć jego zawartość oraz przeprowadzić wstępną analizę: sprawdzić liczbę próbek, przykładowe dane i rozkład klas (pozytywne/negatywne opinie).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7c837",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "## Wprowadzenie – Czym jest NLP?\n",
    "\n",
    "![1.00](https://cdn.prod.website-files.com/5ec6a20095cdf182f108f666/5f22908f09f2341721cd8901_AI%20poster.png \"NLP and text mining: A natural fit for business growth\")\n",
    "\n",
    "**Przetwarzanie Języka Naturalnego (ang. Natural Language Processing, NLP)** to dziedzina informatyki i sztucznej inteligencji, której celem jest umożliwienie komputerom zrozumienia i przetwarzania języka naturalnego – takiego, jakim posługują się ludzie. Innymi słowy, NLP to tworzenie systemów, które potrafią analizować, interpretować, a nawet generować tekst lub mowę w języku np. polskim czy angielskim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94fb68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " Przykładowe zastosowania NLP na co dzień:\n",
    "\n",
    "* **Tłumaczenie maszynowe** – np. tłumacz Google przekładający tekst z jednego języka na inny.\n",
    "* **Asystenci głosowi** – Siri, Alexa czy Asystent Google rozumieją polecenia głosowe użytkownika.\n",
    "* **Analiza sentymentu** – automatyczne określanie, czy dany tekst (np. recenzja produktu, tweet) wyraża pozytywną, negatywną czy neutralną opinię.\n",
    "* **Chatboty i systemy dialogowe** – programy rozmawiające z użytkownikiem w języku naturalnym.\n",
    "* **Wyszukiwanie informacji** – wyszukiwarki internetowe interpretujące pytania zadane pełnym zdaniem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76319448",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "\n",
    "Skupimy się na **analizie sentymentu**, czyli na jednym z popularnych zadań NLP. Analiza sentymentu polega na automatycznym określaniu emocjonalnego wydźwięku tekstu. Najczęściej sprowadza się to do stwierdzenia, czy tekst jest **pozytywny** czy **negatywny** (czasem wyróżnia się też trzecią kategorię: neutralny). Dzięki takim modelom można np. analizować opinie klientów o produkcie, reakcje na kampanię marketingową albo nastroje wypowiedzi w mediach społecznościowych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af7d5ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "\n",
    "\n",
    "**Dlaczego to jest ważne?** Wyobraź sobie firmę, która wypuściła nowy produkt – codziennie pojawiają się setki recenzji i komentarzy w internecie. Przeczytanie ich wszystkich przez człowieka jest trudne, ale algorytm analizy sentymentu może w kilka chwil podsumować, ilu klientów jest zadowolonych, a ilu nie. Automatyczna analiza opinii pozwala firmom szybko reagować na negatywne głosy, a naukowcom badać społeczne reakcje na różne wydarzenia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487b1dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Analiza sentymentu – podejścia\n",
    "\n",
    "![1.00](https://www.researchgate.net/publication/336988754/figure/fig2/AS:928001474711553@1598264201745/Sentiment-analysis-methods.png \"Sentiment analysis methods | Download Scientific Diagram\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0ddce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "\n",
    "\n",
    "Istnieją różne podejścia do analizy sentymentu:\n",
    "\n",
    "* **Metody oparte na słownikach (rule-based)** – Najprostsze podejścia korzystają ze zdefiniowanych list słów o zabarwieniu pozytywnym lub negatywnym (tzw. *slowniki sentymentu*). Tekst oceniany jest na podstawie zliczania pozytywnych i negatywnych słów. Np. jeśli zdanie zawiera słowa \"świetny, wspaniały\" może zostać ocenione jako pozytywne. To podejście jest intuicyjne, ale bywa zawodne – nie uwzględnia kontekstu (np. zdanie \"to nie był świetny film\" zawiera słowo *świetny*, ale całość jest negatywna przez *nie*).\n",
    "* **Metody oparte na uczeniu maszynowym (machine learning)** – Model **uczy się** na przykładach tekstów oznaczonych jako pozytywne/negatywne. Taki model sam dobiera cechy językowe (np. częstość występowania określonych słów) pozwalające przewidywać sentyment nowego tekstu. W tej kategorii mieszczą się klasyczne algorytmy jak Naive Bayes czy Regresja Logistyczna.\n",
    "* **Metody głębokiego uczenia (deep learning)** – Współcześnie najlepsze wyniki często osiąga się modelami opartymi o sieci neuronowe (np. model BERT). Są one jednak bardziej złożone i wymagają dużych zasobów oraz danych.&#x20;\n",
    "\n",
    "Przejdziemy przez cały proces tworzenia prostego systemu analizy sentymentu opartego na uczeniu maszynowym. Zaczniemy od przygotowania danych, następnie przekształcimy teksty na cechy numeryczne, a na końcu zbudujemy model potrafiący klasyfikować nowe teksty jako pozytywne lub negatywne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53055a76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "## Przygotowanie środowiska pracy\n",
    "\n",
    "![1.00](https://media.geeksforgeeks.org/wp-content/uploads/20240402170207/NLP-Libraries-in-Python-copy.webp \"NLP Libraries in Python - GeeksforGeeks\")\n",
    "\n",
    "Zacznijmy od przygotowania środowiska. Upewnij się, że masz zainstalowane potrzebne biblioteki. W niniejszym kursie będziemy korzystać [m.in](http://m.in). z:\n",
    "\n",
    "* **pandas** – do wczytywania i manipulacji zbiorami danych (szczególnie przydatne do eksploracji).\n",
    "* **datasets (HuggingFace)** – do pobrania przykładowych otwartych zbiorów danych do analizy sentymentu.\n",
    "* **nltk (Natural Language Toolkit)** – do podstawowych operacji NLP (tokenizacja, stop words itp.).\n",
    "* **scikit-learn** – do wektorowych reprezentacji tekstu (CountVectorizer/TF-IDF) oraz implementacji klasycznych algorytmów ML (np. Naive Bayes, Logistic Regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d68ea60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "\n",
    "> &#x20;Zbiory, których użyjemy, są publicznie dostępne do celów edukacyjnych:\n",
    ">\n",
    "> * **IMDB Reviews** – 50 000 recenzji filmów w języku angielskim (pozytywne i negatywne) opublikowanych pierwotnie na IMDb.\n",
    ">\n",
    ">   [huggingface.co](https://huggingface.co/datasets/clarin-pl/polemo2-official#:~:text=Data%20splits)\n",
    "> * **Polski zbiór recenzji (PolEmo 2.0)** – ok. 8 216 recenzji w języku polskim (np. opinie o hotelach, produktach, usługach) z podziałem na opinie pozytywne, negatywne oraz neutralne/niejednoznaczne.\n",
    ">\n",
    ">   [huggingface.co](https://huggingface.co/datasets/clarin-pl/polemo2-official#:~:text=Class%20train%20dev%20test%20minus,1439)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e23eb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " Zainstalujmy i zaimportujmy potrzebne biblioteki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn nltk datasets --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee85e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Pobranie słownika stop words dla języka angielskiego (potrzebne w kolejnych etapach)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97519e5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "Po uruchomieniu powyższej komórki, biblioteki zostaną zainstalowane (jeśli jeszcze ich nie masz), a NLTK pobierze listę stop words oraz punktuator do tokenizacji zdań. Możemy teraz wczytać dane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff8c73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Wczytanie i eksploracja zbioru danych IMDB\n",
    "\n",
    "&#x20;                              ![IMDb\\_Logo\\_Rectangle\\_Gold.\\_CB443386186\\_.png](https://m.media-amazon.com/images/G/01/IMDb/brand/guidelines/imdb/IMDb_Logo_Rectangle_Gold._CB443386186_.png \"IMDb_Logo_Rectangle_Gold._CB443386186_.png\")\n",
    "\n",
    "Zaczniemy od zbioru danych **IMDb** z recenzjami filmów.&#x20;\n",
    "\n",
    "Korzystając z biblioteki 🤗 *datasets*, możemy łatwo pobrać ten zbiór. Wystarczy użyć `load_dataset(\"imdb\")`, a dane zostaną automatycznie pobrane. Zbiór IMDb zawiera 50 000 recenzji: 25 000 przeznaczonych do trenowania modeli i 25 000 do testowania, po połowie **pozytywnych** i **negatywnych**.\n",
    "\n",
    "Wczytajmy dane i zobaczmy podstawowe informacje:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# imdb_data=pd.read_csv('datasets/IMDB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Wczytanie zbioru IMDb. Zostaną utworzone podzbiory: 'train' i 'test'\n",
    "imdb_data =load_dataset(\"imdb\", verification_mode='no_checks')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28daeae",
   "metadata": {},
   "source": [
    "\n",
    "Po wykonaniu powyższego polecenia, powinniśmy mieć dostęp do danych. Sprawdźmy ile przykładów jest w zbiorze treningowym i testowym:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a3ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozmiary zbioru treningowego i testowego\n",
    "print(\"Liczba recenzji w train:\", len(imdb_data['train']))\n",
    "print(\"Liczba recenzji w test:\", len(imdb_data['test']))\n",
    "\n",
    "# Dostępne kolumny w danych\n",
    "print(\"Kolumny:\", imdb_data['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087b0dd",
   "metadata": {},
   "source": [
    "&#x20;Możemy potwierdzić, jak są zakodowane etykiety:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc0992",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sprawdźmy kilka pierwszych etykiet i ich znaczenie\n",
    "label_values = set(imdb_data['train']['label'])\n",
    "print(\"Unikalne etykiety w zbiorze:\", label_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a32031",
   "metadata": {},
   "source": [
    "&#x20;Z dokumentacji wiadomo, że w tym zbiorze:\n",
    "\n",
    "* **0** oznacza recenzję negatywną,\n",
    "* **1** oznacza recenzję pozytywną.\n",
    "\n",
    "Teraz przyjrzyjmy się przykładowym danym. Wyświetlimy przykładową recenzję i jej etykietę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119a146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobranie pierwszego przykładu ze zbioru treningowego\n",
    "first_review = imdb_data['train'][0]\n",
    "print(\"Etykieta:\", first_review['label'])\n",
    "print(\"Treść recenzji:\")\n",
    "print(first_review['text'][:500], \"...\")  # wypisz pierwsze 500 znaków"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ca1e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "**Pytanie:** Patrząc na powyższą recenzję, czy potrafisz na pierwszy rzut oka stwierdzić, czy jest pozytywna czy negatywna? Zwróć uwagę na słowa kluczowe, które mogą zdradzać sentyment autora.\n",
    "\n",
    "Spróbujmy jeszcze lepiej zrozumieć dane:\n",
    "\n",
    "* Jak długie są recenzje? Czy są to zdania, akapity, a może całe eseje?\n",
    "* Czy recenzje zawierają dużo znaków interpunkcyjnych, wielkich liter, liczb itp.?\n",
    "* W jakim stylu są pisane (formalny/nieformalny, slang)?\n",
    "\n",
    "Te obserwacje przydadzą nam się przy planowaniu **przetwarzania tekstu**&#x20;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9dc8ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Rozkład klas w zbiorze IMDb\n",
    "\n",
    "Zanim przejdziemy dalej, sprawdźmy, czy zbiór jest zrównoważony. Tzn. czy mamy mniej więcej tyle samo recenzji pozytywnych i negatywnych. Zrobimy szybkie zliczenie:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22769770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konwersja zbioru treningowego do pandas DataFrame dla łatwej eksploracji\n",
    "df_train = pd.DataFrame(imdb_data['train'])\n",
    "df_test = pd.DataFrame(imdb_data['test'])\n",
    "\n",
    "# Zliczenie etykiet pozytywnych vs negatywnych\n",
    "print(df_train['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84839a3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wynik powinien pokazać zbliżoną liczbę `0` i `1`w zbiorze treningowym. Oznacza to, że klasy są zbalansowane – to dobra informacja, bo model uczony na takich danych nie będzie faworyzował jednej klasy tylko dlatego, że jest jej więcej.\n",
    "\n",
    "Analogicznie możemy sprawdzić zbiór testowy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2f8831",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ### Eksploracja danych – przykłady recenzji\n",
    "\n",
    "Przeczytajmy kilka losowych recenzji, aby zorientować się w zawartości. Możemy wylosować po jednej recenzji oznaczonej jako pozytywna i negatywna:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a47ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Losowy pozytywny i losowy negatywny przykład z treningu\n",
    "positive_example = df_train[df_train['label'] == 1].sample(1)\n",
    "negative_example = df_train[df_train['label'] == 0].sample(1)\n",
    "\n",
    "print(\"Przykładowa pozytywna recenzja:\\n\", positive_example['text'].values[0][:300], \"...\\n\")\n",
    "print(\"Przykładowa negatywna recenzja:\\n\", negative_example['text'].values[0][:300], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e4944",
   "metadata": {},
   "source": [
    "Zwróć uwagę na słownictwo w obu recenzjach. Często w pozytywnych opiniach pojawiają się słowa typu *\"great\", \"excellent\", \"amazing\"*, a w negatywnych *\"boring\", \"bad\", \"terrible\"*. Oczywiście nie jest to reguła bezwyjątkowa – model ML powinien się tych zależności nauczyć automatycznie.\n",
    "\n",
    "### Podstawowa statystyka: długość recenzji\n",
    "\n",
    "Sprawdźmy jeszcze średnią długość recenzji w zbiorze IMDb. Możemy policzyć liczbę znaków lub słów w każdej recenzji i znaleźć średnią oraz medianę. To da nam obraz, z jak długim tekstem będziemy pracować (ważne np. dla wydajności przetwarzania)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4432a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oblicz długość recenzji (w znakach) dla kilku przykładów i średnią dla zbioru treningowego\n",
    "df_train['length_chars'] = df_train['text'].apply(len)\n",
    "print(\"Przykładowe długości recenzji (w znakach):\", df_train['length_chars'].head().tolist())\n",
    "print(\"Średnia długość recenzji:\", df_train['length_chars'].mean())\n",
    "print(\"Mediana długości recenzji:\", df_train['length_chars'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d652a43f",
   "metadata": {},
   "source": [
    "\n",
    "**Ćwiczenie:** Zamiast liczby znaków, spróbuj policzyć **liczbę słów** w recenzjach i ponownie policzyć średnią oraz medianę. Wskazówka: Możesz wykorzystać metodę `.split()` dzielącą tekst po spacjach, aby przybliżoną liczbę słów (choć nie jest to idealna metoda, o lepszej tokenizacji będziemy mówić później).\n",
    "\n",
    "*(Uzupełnij kod poniżej, aby policzyć średnią i medianę długości recenzji w słowach.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbba232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Oblicz długość recenzji w słowach dla zbioru treningowego i podaj średnią oraz medianę.\n",
    "# Podpowiedź: dla każdego text w df_train['text'] wykonaj text.split() i policz len() listy wynikowej.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9155af6",
   "metadata": {},
   "source": [
    "Zazwyczaj recenzje filmowe IMDb są dość obszerne (kilka zdań do kilku akapitów). Długość tekstu może wpływać na wybór metod przetwarzania – np. bardzo długie dokumenty mogą wymagać przycięcia lub podzielenia, ale w naszym przypadku raczej nie będzie takiej potrzeby."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb20aa82",
   "metadata": {},
   "source": [
    "### Podsumowanie eksploracji\n",
    "\n",
    "Do tej pory:\n",
    "\n",
    "* Załadowaliśmy zbiór danych **IMDb** zawierający recenzje filmów oznaczone jako pozytywne lub negatywne.\n",
    "* Potwierdziliśmy rozmiar zbioru i równowagę klas.\n",
    "* Obejrzeliśmy przykładowe recenzje – widać różnorodność języka, obecność emocjonalnych słów, a czasem sarkazmu czy negacji.\n",
    "* Policzyliśmy podstawowe statystyki (długość recenzji), co dało nam wyobrażenie o danych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dce291",
   "metadata": {},
   "source": [
    "## Quiz – sprawdź swoją wiedzę\n",
    "\n",
    "1. **Co oznacza skrót NLP?**\n",
    "2. **Wymień dwa przykłady zastosowań NLP (innych niż analiza sentymentu).**\n",
    "3. **Jakie znasz podstawowe podejścia do analizy sentymentu? Które z nich będziemy stosować w tym kursie?**\n",
    "4. **Dlaczego przy analizie zbioru danych ważne jest sprawdzenie rozkładu klas (np. liczby opinii pozytywnych vs negatywnych)?**\n",
    "\n",
    "*Zastanów się nad odpowiedziami. Odpowiedzi możesz zapisać poniżej we własnych słowach.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c612a8b",
   "metadata": {},
   "source": [
    "**Twoje odpowiedzi:**\n",
    "\n",
    "* 1: ...\n",
    "* 2: ...\n",
    "* 3: ...\n",
    "* 4: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e221a467",
   "metadata": {},
   "source": [
    "## Notatki własne\n",
    "\n",
    "*(Poniżej możesz zapisać własne notatki, spostrzeżenia lub podsumowanie. Ta sekcja jest dla Ciebie : ) .)*\n",
    "\n",
    "**Notatki:**\n",
    "\n",
    "* ...\n",
    "* ...\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df828d1c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Przetwarzanie tekstu – czyszczenie, tokenizacja, stop words, lematyzacja/stemming. Reprezentacja: Bag-of-Words i TF-IDF\n",
    "\n",
    "## Cele edukacyjne\n",
    "\n",
    "* **Dowiesz się, dlaczego i jak czyścić dane tekstowe** – usuwanie zbędnych znaków, standaryzacja (np. wielkości liter), podstawowe techniki przygotowania surowego tekstu.\n",
    "* **Nauczysz się tokenizacji** – podziału tekstu na tokeny (najczęściej słowa) jako pierwszy krok przekształcania tekstu.\n",
    "* **Poznasz pojęcie** ***stop words*** – zrozumiesz, czym są tzw. \"słowa nieniosące treści\" i jak je usuwać, by nie zaciemniały analizy.\n",
    "* **Stemming i lematyzacja** – dowiesz się, jak sprowadzać wyrazy do ich podstawowej formy (np. \"koty\" -> \"kot\") i po co to robimy.\n",
    "* **Reprezentacja Bag-of-Words (BoW)** – zrozumiesz, jak tekst można zamienić na wektor liczbowy poprzez zliczanie wystąpień słów.\n",
    "* **Reprezentacja TF-IDF** – poznasz ulepszenie Bag-of-Words ważone częstością w dokumentach (TF-IDF) i zobaczysz, czemu jest przydatne w modelowaniu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7d592",
   "metadata": {},
   "source": [
    "Mamy zbiór recenzji filmowych (IMDb) z odpowiadającymi im etykietami sentymentu. Jednak **surowy tekst** w takiej postaci nie jest bezpośrednio użyteczny dla algorytmu uczenia maszynowego. Musimy go najpierw przekształcić.\n",
    "\n",
    "Typowy **pipeline** (proces) w zadaniach NLP wygląda następująco:\n",
    "\n",
    "1. **Czyszczenie danych tekstowych** – usunięcie lub standaryzacja elementów, które mogą przeszkadzać w analizie (np. HTML, znaki specjalne, liczby, emotikony, itp.), sprowadzenie tekstu do jednolitej formy.\n",
    "2. **Tokenizacja** – rozbicie tekstu na mniejsze jednostki, najczęściej słowa (tokeny). Czasem stosuje się też tokenizację na znaki, sylaby lub grupy wyrazów (n-gramy), ale tutaj skupimy się na słowach.\n",
    "3. **Usuwanie stop words** – usunięcie z tokenów tzw. słów pospolitych, które nie niosą istotnego znaczenia dla zadania (np. \"i\", \"że\", \"to\"). Dzięki temu redukujemy szum w danych.\n",
    "4. **Stemming / Lematyzacja** – sprowadzenie odmienionych form wyrazów do ich rdzenia lub podstawowej formy słownikowej (np. \"pies\", \"psa\", \"psom\" -> \"pies\"). Umożliwia to traktowanie wariantów tego samego słowa jednakowo.\n",
    "5. **Tworzenie cech (feature extraction)** – zamiana listy tokenów na cechy liczbowe, na których może pracować model. W klasycznym ujęciu będzie to Bag-of-Words lub TF-IDF, które zaraz omówimy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e52ab8",
   "metadata": {},
   "source": [
    "##  Czyszczenie tekstu (text cleaning)\n",
    "\n",
    "Surowe dane tekstowe mogą zawierać elementy, które utrudniają analizę lub nie wnoszą nic istotnego. Przykłady:\n",
    "\n",
    "* Znaki interpunkcyjne (.,!?), które zwykle nie wpływają na sentyment (choć np. wielokropek czy wykrzyknienia mogą wzmacniać emocje, ale na początek je pominiemy).\n",
    "* Cyfry/liczby, daty – raczej nie przydatne w zadaniu sentymentu.\n",
    "* HTML czy znaki specjalne (np. `&nbsp;`, `<br>` w tekście recenzji webowych).\n",
    "* Wielkie litery na początku zdań – możemy ujednolicić tekst do małych liter, żeby \"Film\" i \"film\" nie były traktowane jako różne tokeny.\n",
    "* Tzw. **szum** (ang. noise): ciągi znaków niebędące słowami, np. emotikony, hashtagi, adresy URL. (W recenzjach filmowych może nie być takich rzeczy, ale np. w tweetach często występują)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a5576",
   "metadata": {},
   "source": [
    "\n",
    "**Nasze podejście:** Na potrzeby naszego modelu zrealizujemy prostą strategię czyszczenia:\n",
    "\n",
    "* Zamienimy tekst na same **małe litery**.\n",
    "* Usuniemy **znaki niealfanumeryczne** (wszystko poza literami i cyframi) lub zastąpimy je spacją. To pozwoli pozbyć się interpunkcji.\n",
    "* Opcjonalnie: możemy usunąć cyfry, ale w recenzjach filmów liczby (np. \"10/10\", \"1998\") mogą występować. Raczej nie są kluczowe dla sentymentu, więc możemy je usunąć, żeby uprościć słownik."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf7293",
   "metadata": {},
   "source": [
    "Powyższe możemy zrealizować np. za pomocą wyrażeń regularnych (biblioteka `re` w Pythonie) lub metod łańcuchowych Pythona.\n",
    "\n",
    "Stwórzmy funkcję `clean_text()`, która przyjmie ciąg znaków (recenzję) i zwróci wyczyszczony tekst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c76cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # 1. Zamiana na małe litery\n",
    "    text = text.lower()\n",
    "    # 2. Usunięcie znaczników HTML (jeśli są) poprzez usunięcie ciągów w < >\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # 3. Usunięcie znaków nie-alfanumerycznych (pozostaw litery i cyfry, resztę zastąp spacją)\n",
    "    text = re.sub(r'[^a-z0-9ąćęłńóśźż ]+', ' ', text)  # dodatkowo polskie znaki, żeby ich nie usuwać w polskich tekstach\n",
    "    # 4. Usunięcie nadmiarowych spacji (jeśli wskutek powyższych operacji pojawiły się wielokrotne spacje)\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Test funkcji clean_text na sztucznym przykładzie\n",
    "raw_example = \"To jest <b>przykładowy</b> TEKST!!! Zawiera %^ różne dziwne znaki... oraz liczby 12345.\"\n",
    "print(\"Przed czyszczeniem: \", raw_example)\n",
    "print(\"Po czyszczeniu:    \", clean_text(raw_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01516eff",
   "metadata": {},
   "source": [
    "\n",
    "Po uruchomieniu testu powyżej powinniśmy zobaczyć, że:\n",
    "\n",
    "* Wszystkie litery są małe.\n",
    "* HTML (`<b>...</b>`) zniknął.\n",
    "* Symbol `%^` i wielokrotne wykrzykniki zostały usunięte.\n",
    "* Liczby zostały zachowane (w powyższym kodzie zachowujemy cyfry `0-9` – można je usunąć zmieniając wyrażenie regularne, ale na razie niech zostaną).\n",
    "* Ciągi spacji zostały zredukowane do pojedynczej spacji.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b9630",
   "metadata": {},
   "source": [
    "**Uwaga:** Nasza funkcja `clean_text` jest dość prosta. W praktyce czyszczenie można rozszerzać:\n",
    "\n",
    "* Można by usunąć *stop words* już na etapie czyszczenia (my zrobimy to później).\n",
    "* Można by zamieniać emotikony na słowa oznaczające emocje (np. \":)\" -> \"emotikon\\_usmiech\").\n",
    "* Można by użyć biblioteki dedykowanej do czyszczenia tekstu (np. `beautifulsoup` do HTML, czy `emoji` do obsługi emotikon).\n",
    "* Dla języka polskiego: można pokusić się o zamianę polskich liter diakrytycznych na ich podstawowe formy (ą->a, ć->c, itd.), ale wtedy tracimy trochę informacji (lepiej tego nie robić, by np. \"los\" i \"łoś\" nie stały się tym samym słowem).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952db62",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Na potrzeby naszego przykłładu obecna funkcja wystarczy.\n",
    "\n",
    "Teraz zastosujmy czyszczenie do naszych danych. Weźmy kilka surowych recenzji z IMDb i je wyczyśćmy, żeby zobaczyć różnicę:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a93f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = df_train['text'].iloc[:2]\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"Oryginalna recenzja {i+1} (skrócona):\\n{text[:100]}...\\n\")\n",
    "    print(f\"Po czyszczeniu:\\n{clean_text(text)[:100]}...\\n{'-'*40}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78cbc90",
   "metadata": {},
   "source": [
    "Zwróć uwagę, że tekst po czyszczeniu nadal wygląda jak zdania, ale nie ma w nim wielkich liter ani specjalnych znaków.\n",
    "\n",
    "**Ćwiczenie:** Zastanów się, dlaczego usuwanie interpunkcji może czasem spowodować utratę informacji. Czy potrafisz wymyślić sytuację, w której interpunkcja albo emotikon niosłyby informację o sentymencie? (Podpowiedź: Wykrzyknienia lub pytajniki mogą wzmacniać wydźwięk, np. \"Film był świetny!!!\" vs \"Film był świetny\"). W naszej prostej metodzie pomijamy ten aspekt – możesz zapisać swoją odpowiedź/rozważania poniżej:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e694dc",
   "metadata": {},
   "source": [
    "\n",
    "**Twoje przemyślenia:** *(Czy interpunkcja może nieść informację sentymentu?)*\n",
    "\n",
    "... (tu wpisz własne uwagi) ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb07dd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Tokenizacja – podział tekstu na tokeny (słowa)\n",
    "\n",
    "&#x20;                                           ![Tokenization — A complete guide. Natural Language Processing — NLP From… |  by Utkarsh Kant | Medium](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSw-6MjlECiyaOc7aq2EDJJJbfowX2DDrhw3g\\&s \"Tokenization — A complete guide. Natural Language Processing — NLP From… |  by Utkarsh Kant | Medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e415df72",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Mając wyczyszczony tekst, następnym krokiem jest **tokenizacja**, czyli rozbicie ciągu znaków na listę tokenów. Najczęściej jako token przyjmujemy słowo (ciąg liter między spacjami), ale w praktyce definicja tokenu zależy od zadania:\n",
    "\n",
    "* W analize sentymentu zwykle tokenami są słowa lub emotikony, czasem pary wyrazów (bigramy) też traktuje się jako cechy.\n",
    "* W innych zastosowaniach tokenem może być pojedynczy znak (np. w modelach opartych o litery) lub całe frazy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868fc689",
   "metadata": {},
   "source": [
    "\n",
    "My skupimy się na tokenizacji słów. W języku angielskim sprawa jest dość prosta – można w większości przypadków podzielić tekst na spacjach i znakach interpunkcyjnych. W języku polskim podobnie. Problemy pojawiają się w językach takich jak chiński (gdzie nie ma spacji między wyrazami), ale tym nie będziemy się zajmować.\n",
    "\n",
    "Skorzystamy z narzędzi NLTK do tokenizacji zdań w jęz. angielskim. NLTK posiada funkcję `word_tokenize`, ale wymaga ona pobrania modelu tokenizatora (co zrobiliśmy wyżej `nltk.download('punkt')`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b762c29",
   "metadata": {},
   "source": [
    "Przetestujmy tokenizację na jednym zdaniu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c0397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "test_sentence = \"Film był niesamowity, bardzo mi się podobał!\"\n",
    "tokens = word_tokenize(clean_text(test_sentence))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66cf955",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Zwróć uwagę: tokenizacja NLTK:\n",
    "\n",
    "* Podzieli zdanie na pojedyncze słowa.\n",
    "* Domyślnie rozdzieli też znaki interpunkcyjne jako oddzielne tokeny, ale ponieważ wyczyściliśmy tekst z interpunkcji wcześniej, w tokenach będą tylko słowa.\n",
    "\n",
    "W powyższym przypadku (po czyszczeniu) zdanie `\"film był niesamowity bardzo mi się podobał\"` powinno dać listę tokenów podobną do `[\"film\", \"był\", \"niesamowity\", \"bardzo\", \"mi\", \"się\", \"podobał\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494bfa8",
   "metadata": {},
   "source": [
    "**Uwaga:** Widać tu, że tokenizacja \"po naszemu\" zachowała formy odmienione (\"był\", \"podobał\"). Później zajmiemy się sprowadzaniem do podstawowej formy, ale najpierw omówimy **stop words**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f18418e",
   "metadata": {},
   "source": [
    "### Tokenizacja całego zbioru\n",
    "\n",
    "Tokenizowanie każdej recenzji z osobna i przechowywanie list słów jest możliwe, ale niekonieczne w naszym pipeline'ie. Biblioteki takie jak scikit-learn mają swoje wewnętrzne tokenizatory (np. `CountVectorizer` potrafi sam podzielić tekst według wybranego wzorca, domyślnie po spacjach i podstawowej interpunkcji).\n",
    "\n",
    "**W tym notatniku** jednak wykonamy pewne kroki ręcznie, by dobrze zrozumieć proces. Na koniec pokażemy, jak Vectorizer robi to automatycznie.\n",
    "\n",
    "Na razie zróbmy tak: weźmy kilka recenzji i zastosujmy pełen proces czyszczenie + tokenizacja, aby zobaczyć, jak nasze dane będą wyglądały po tych operacjach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbce426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przekształcenie kilku przykładowych recenzji na listy tokenów\n",
    "for text in df_train['text'].iloc[:3]:\n",
    "    cleaned = clean_text(text)\n",
    "    tokens = word_tokenize(cleaned)\n",
    "    print(\"Oryginalny tekst (skrócony):\", text[:60], \"...\")\n",
    "    print(\"Po tokenizacji:\", tokens[:10], \"...\\n\")  # wypisz pierwsze 10 tokenów dla zwięzłości\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d381c",
   "metadata": {},
   "source": [
    "Wyniki pokazują nam, że tekst został rozbity na poszczególne słowa. Widać też, że mogą pojawić się **stop words**, np. \"the\", \"was\", \"it\" w angielskim czy \"to\", \"się\" w polskim – o tym za moment.\n",
    "\n",
    "**Zadanie:** Zastanów się, czy tokenizacja powinna być wykonywana przed czyszczeniem, czy po. W naszej kolejności najpierw czyścimy (np. usuwamy interpunkcję), potem tokenizujemy. Co by się stało, gdybyśmy odwrócili kolejność? *(Spróbuj odpowiedzieć w myślach lub zapisz poniżej.)*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577a4d8",
   "metadata": {},
   "source": [
    "**Odpowiedź:**&#x20; \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5ea13",
   "metadata": {},
   "source": [
    "## Stop words – słowa nieistotne\n",
    "\n",
    "&#x20;                           ![Stop Word Removal Techniques for NLP Models | Ifioque.com](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTWbjOhgpy4QOORgglw7DXzQb0FMSjIlwAqsQ\\&s \"Stop Word Removal Techniques for NLP Models | Ifioque.com\")\n",
    "\n",
    "**Stop words** to *często występujące słowa, które niosą mało treściowych informacji*. W języku polskim są to np. \"i\", \"oraz\", \"ale\", \"to\", \"się\", \"w\", \"na\". W angielskim: \"and\", \"the\", \"but\", \"to\", \"in\", \"on\", itp. Te wyrazy głównie pełnią funkcje gramatyczne. W kontekście analizy sentymentu zazwyczaj nie wskazują na pozytywną czy negatywną opinię.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bce33f",
   "metadata": {},
   "source": [
    "\n",
    "**Dlaczego je usuwać?**\n",
    "\n",
    "Ponieważ pojawiają się w niemal każdym dokumencie, mogą zdominować statystyki i sprawić, że wektorowa reprezentacja tekstu ma bardzo dużo cech, które nic nie wnoszą (np. liczbę wystąpień \"the\" czy \"i\"). Usuwając je, zmniejszamy wymiar danych i skupiamy się na słowach kluczowych (rzeczownikach, przymiotnikach, czasownikach związanych z opinią).\n",
    "\n",
    "Scikit-learn i NLTK udostępniają gotowe listy stop words dla różnych języków. Skorzystamy z listy angielskiej z NLTK, a dla polskiego stworzymy krótką listę samodzielnie (NLTK nie ma gotowej listy dla polskiego)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Lista angielskich stop words z NLTK\n",
    "stop_words_eng = set(stopwords.words('english'))\n",
    "print(\"Liczba stop words (angielski):\", len(stop_words_eng))\n",
    "print(\"Przykładowe stop words:\", list(stop_words_eng)[:10])\n",
    "\n",
    "# Definiujemy ręcznie krótką listę polskich stop words (istnieją biblioteki z gotowymi listami, np. stopwordsiso)\n",
    "stop_words_pl = {\"i\", \"w\", \"na\", \"się\", \"nie\", \"że\", \"to\", \"jest\", \"tam\", \"tu\", \"o\", \"a\", \"ale\", \"oraz\", \"jak\", \"tak\"}\n",
    "print(\"Przykładowe stop words (polski):\", list(stop_words_pl)[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76b9c02",
   "metadata": {},
   "source": [
    "Lista angielska ma zapewne ok. 200 słów. Nasza polska jest bardzo skrócona dla ilustracji – prawdziwa lista powinna być dłuższa.\n",
    "\n",
    "Teraz, aby usunąć stop words z tokenów, możemy po prostu przefiltrować listę tokenów i wyrzucić z niej te, które znajdują się na liście stop words.\n",
    "\n",
    "Przetestujmy to na krótkim tekście mieszanym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760607b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"to\", \"jest\", \"bardzo\", \"fajny\", \"film\"]\n",
    "tokens_filtered = [t for t in tokens if t not in stop_words_pl]\n",
    "print(\"Przed usunięciem stop words:\", tokens)\n",
    "print(\"Po usunięciu stop words:\", tokens_filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb471782",
   "metadata": {},
   "source": [
    "\n",
    "W powyższym przykładzie token \"to\" i \"jest\" powinny zostać usunięte jako stop words, pozostawiając \"bardzo\", \"fajny\", \"film\". Oczywiście \"bardzo\" to też częste słowo (przysłówek), które można by dodać do stop words, ale naszej krótkiej listy tam nie ma.\n",
    "\n",
    "Teraz zastosujmy usuwanie stop words w naszym pipeline. Połączmy dotychczasowe kroki: **czyszczenie -> tokenizacja -> usunięcie stop words** dla przykładowej recenzji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ac64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = df_train['text'].iloc[0]  # pierwsza recenzja\n",
    "cleaned = clean_text(sample_text)\n",
    "tokens = word_tokenize(cleaned)\n",
    "tokens_no_stop = [t for t in tokens if t not in stop_words_eng]\n",
    "print(\"Oryginalny tekst:\", sample_text[:60], \"...\")\n",
    "print(\"Po czyszczeniu:\", cleaned[:60], \"...\")\n",
    "print(\"Tokeny (kilka pierwszych):\", tokens[:10])\n",
    "print(\"Tokeny po usunięciu stop words (kilka pierwszych):\", tokens_no_stop[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63ba8b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Zaobserwuj, ile tokenów odpadło jako stop words. W języku angielskim prawdopodobnie wiele \"the\", \"and\", \"of\" itp. zostało usuniętych. Pozostały bardziej znaczące słowa.\n",
    "\n",
    "**Nota:** W bibliotekach takich jak `CountVectorizer` można ustawić parametr `stop_words='english'`, by automatycznie usuwać angielskie stop words podczas wektoryzacji, co często się robi. My jednak przechodzimy krok po kroku manualnie, by zrozumieć mechanizm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fb6b0c",
   "metadata": {},
   "source": [
    "\n",
    "**Ćwiczenie:** Usuń stop words ze zdania: *\"To był naprawdę świetny film, który bardzo mi się podobał.\"* Najpierw wyczyść to zdanie funkcją `clean_text`, potem stokenizuj i przefiltruj używając zarówno listy polskich stop words (dla słów \"to\", \"mi\", \"się\" etc.) jak i angielskich (to akurat polskie zdanie, więc użyj polskiej listy). Zapisz wynikowe tokeny.\n",
    "\n",
    "*(Możesz wykonać to ćwiczenie w poniższej komórce kodu.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51900e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wyczyść zdanie, tokenizuj i usuń stop words (PL) dla podanego przykładowego zdania.\n",
    "test_sentence = \"To był naprawdę świetny film, który bardzo mi się podobał.\"\n",
    "# Tu wpisz kolejne kroki: clean_text, word_tokenize, filter stop_words_pl\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90cc806",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Stemming i lematyzacja\n",
    "\n",
    "![1.00](https://cdn.botpenguin.com/assets/website/Stemming_IR_346db34f6c.webp \"Stemming in NLP: Key Concepts and Fundamentals Explained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cee819",
   "metadata": {},
   "source": [
    "\n",
    "Kolejnym krokiem, który **opcjonalnie** można zastosować, jest sprowadzenie tokenów do ich podstawowej formy. Są dwie główne techniki:\n",
    "\n",
    "* **Stemming** – ucina końcówki wyrazów według pewnych reguł, by sprowadzić je do rdzenia (niekoniecznie poprawnego słowa). Np. stemming w jęz. angielskim: *\"playing\", \"played\", \"plays\"* -> *\"play\"*. W polskim: *\"kot\", \"koty\", \"kota\", \"kotem\"* -> może dać *\"kot\"* (w zależności od algorytmu).\n",
    "* **Lematyzacja** – wykorzystuje słowniki i reguły języka, by znaleźć **lemma** czyli formę podstawową. Np. *\"went\"* -> *\"go\"*, *\"mice\"* -> *\"mouse\"*. W polskim *\"poszłam\"* -> *\"pójść\"*, *\"jabłek\"* -> *\"jabłko\"*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d58e60",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Stemming jest prostszy (nie wymaga znajomości języka), ale czasem obcina za dużo lub tworzy sztuczne formy. Lematyzacja jest dokładniejsza, ale wymaga więcej zasobów (słowniki morfologiczne lub modele).\n",
    "\n",
    "W naszym projekcie zastosujemy **stemming** dla języka angielskiego jako przykład. Dla polskiego pełna lematyzacja wymagałaby np. biblioteki *spaCy* z modelem pl lub *Morfeusz* – to dość ciężkie jak na nasze potrzeby, więc ograniczymy się do świadomości, że takie narzędzia istnieją. (Polskie teksty i tak przetworzymy w wersji bez lematyzacji, co też zadziała, tylko słownik cech będzie większy, bo np. \"film\" i \"filmu\" będą osobno).\n",
    "\n",
    "Skorzystamy z NLTK `PorterStemmer` lub `SnowballStemmer` dla angielskiego. SnowballStemmer jest uniwersalny i wspiera kilka języków (niestety polskiego brak). Użyjmy PorterStemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"playing\", \"plays\", \"played\", \"player\", \"playingly\"]\n",
    "stemmed = [stemmer.stem(w) for w in words]\n",
    "print(\"Słowa oryginalne:  \", words)\n",
    "print(\"Po stemmingu:     \", stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4bd948",
   "metadata": {},
   "source": [
    "Zobaczmy wynik:\n",
    "\n",
    "* \"playing\", \"plays\", \"played\" powinny wszystkie dać \"play\".\n",
    "* \"player\" może dać \"player\"&#x20;\n",
    "* \"playingly\" (niezbyt używane słowo) pewnie zostanie okrojone do \"playingli\" lub \"play\" – tu widać, że stemmer czasem robi dziwne rzeczy gdy słowo jest nietypowe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602a8ce",
   "metadata": {},
   "source": [
    "Teraz `WordNetLemmatizer` wymaga znajomości części mowy, inaczej lematyzuje do rzeczownika domyślnie:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "words2 = [\"playing\", \"played\", \"cats\", \"better\"]\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in words2]\n",
    "lemmas_verbs = [lemmatizer.lemmatize(w, pos='v') for w in words2]\n",
    "print(\"Oryginały: \", words2)\n",
    "print(\"Lematy domyślne (rzeczowniki):\", lemmas)\n",
    "print(\"Lematy jako czasowniki:\", lemmas_verbs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3fec37",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Dla powyższego:\n",
    "\n",
    "* Domyślnie: \"playing\" -> \"playing\" (bo traktuje jako rzeczownik, nie zmienia), \"played\" -> \"played\", \"cats\" -> \"cat\", \"better\" -> \"better\".\n",
    "* Jako czasowniki: \"playing\" -> \"play\", \"played\" -> \"play\", reszta bez zmian (\"better\" jako czasownik - \"better\" zostanie)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10632375",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Lematyzacja więc wymaga pipeline'u, który najpierw oznacza części mowy (POS tagging) – to wykracza poza nasz zakres. **W praktyce** w prostych modelach często wystarcza stemming zamiast pełnej lematyzacji, choć jest mniej precyzyjny.\n",
    "\n",
    "&#x20;Zdecydujmy, że dla angielskich danych zastosujemy stemming, a dla polskich – nie będziemy robić stemmingu (brak narzędzia out-of-the-box w NLTK). Bez obaw – modele i tak sobie poradzą, tylko słów będzie trochę więcej.\n",
    "\n",
    "Zaimplementujmy prostą funkcję, która wykona pełne przetworzenie pojedynczego tekstu: **czyszczenie -> tokenizacja -> usunięcie stopwords -> stemming (dla angielskiego)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9029ec1",
   "metadata": {},
   "source": [
    "\n",
    "W wyniku powinniśmy otrzymać listę tokenów, np. `[\"movi\", \"absolut\", \"wonder\", \"love\", \"act\", \"plot\"]`. Zauważ, że po stemmingu powstają czasem niepełne słowa (\"wonder\" zamiast \"wonderful\", \"movi\" zamiast \"movie\", \"love\" zamiast \"loved\"). Mimo to dla algorytmu liczącego frekwencje słów, te formy będą konsistentne w całym korpusie.\n",
    "\n",
    "Podobną funkcję moglibyśmy napisać dla polskiego, ale ograniczymy się do czyszczenia, tokenizacji i usunięcia stopwords (bez stemmingu):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_pl(text: str) -> list:\n",
    "    text_clean = clean_text(text)\n",
    "    tokens = word_tokenize(text_clean)\n",
    "    tokens = [t for t in tokens if t not in stop_words_pl]  # użyjemy naszej ograniczonej listy polskiej\n",
    "    # Brak stemmingu/lematyzacji dla PL\n",
    "    return tokens\n",
    "\n",
    "print(preprocess_text_pl(\"Ten film był niesamowity, naprawdę mi się podobał!\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a547fc",
   "metadata": {},
   "source": [
    "\n",
    "To powinno zwrócić np. `[\"ten\", \"film\", \"był\", \"niesamowity\", \"naprawdę\", \"podobał\"]` minus ewentualne stop words jeśli pasują do listy (np. \"mi\", \"się\" powinny zostać usunięte).\n",
    "\n",
    "**Ćwiczenie:** Pomyśl, jakie wyzwania wiążą się z lematyzacją polskiego tekstu w porównaniu do angielskiego. (Podpowiedź: Odmiana przez przypadki, rodzaje gramatyczne, złożona gramatyka). Dlaczego proste \"ucinanie\" końcówek (stemming) może być trudne dla polskiego?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270adc12",
   "metadata": {},
   "source": [
    "\n",
    "*(Wpisz swoje przemyślenia poniżej.)*\n",
    "\n",
    "**Twoje przemyślenia:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7d720",
   "metadata": {},
   "source": [
    "## Reprezentacja tekstu: Bag-of-Words\n",
    "![NLP: Bag of Words. The Bag of Words (BoW) model is a… | by Rahul S | Medium](https://miro.medium.com/v2/resize:fit:661/0*cf1wq8eIix-Z2qIf.png \"NLP: Bag of Words. The Bag of Words (BoW) model is a… | by Rahul S | Medium\")\n",
    "\n",
    "\n",
    "Mamy przygotowane i przetworzone tokeny – czas zamienić je na **cechy liczbowe**. Klasyczne podejście to **Bag-of-Words (torba słów)**. W modelu tym:\n",
    "\n",
    "* Tworzymy **słownik wszystkich unikalnych tokenów** w naszym korpusie treningowym.\n",
    "* Każdemu tokenowi przypisujemy indeks kolumny.\n",
    "* Reprezentujemy każdy dokument (recenzję) jako wektor długości = liczbie słów w słowniku. W tym wektorze w pozycji odpowiadającej danemu słowu wpisujemy np. liczbę wystąpień tego słowa w dokumencie (lub 0, jeśli nie występuje)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb8177",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### Tworzenie Bag-of-Words (BoW) w Pythonie\n",
    "\n",
    "\n",
    "Można to zrobić ręcznie, ale wygodniej skorzystać z `CountVectorizer` z scikit-learn, który:\n",
    "\n",
    "* Zbuduje słownik słów (faza `fit`).\n",
    "* Zamieni listy tekstów na macierz częstości słów (faza `transform`).\n",
    "\n",
    "Ponieważ `CountVectorizer` sam wewnętrznie dokonuje pewnej tokenizacji, możemy mu po prostu podać surowe (ale wyczyszczone) teksty, a on resztę zrobi. Jednak, jeśli chcemy mieć wpływ na tok preprocesingu (np. użyć naszego stemmera, itp.), możemy też przekazać wektorizerowi już przetworzone tokeny złączone znów w stringi.\n",
    "\n",
    "Aby nie komplikować, najpierw pokażemy prosty użycie `CountVectorizer` *bez* własnego preprocessingu (on sam zrobi podstawową tokenizację i może usunąć stop words jeśli chcemy). Potem porównamy z naszym własnym przygotowaniem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Weźmy próbkę 5 recenzji ze zbioru treningowego\n",
    "sample_texts = df_train['text'].iloc[:5].tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=20)\n",
    "# stop_words='english' spowoduje usunięcie angielskich stopwords automatycznie\n",
    "# max_features=20 ograniczy słownik do 20 najczęściej występujących słów (tylko dla demonstracji)\n",
    "\n",
    "X_sample = vectorizer.fit_transform(sample_texts)\n",
    "print(\"Rozmiar macierzy cech:\", X_sample.shape)\n",
    "print(\"Słowa w słowniku:\", vectorizer.get_feature_names_out())\n",
    "print(\"Macierz BoW (pierwsze 5 dokumentów, w formacie gęstym):\\\\n\", X_sample.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b71b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X_sample.toarray(),columns=vectorizer.get_feature_names_out(),index=[text[:100] for text in sample_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a21fff",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Wynik:\n",
    "\n",
    "* Rozmiar macierzy cech to (5 dokumentów, 20 cech).\n",
    "* Wypisane słowa w słowniku to 20 najpopularniejszych słów w tych 5 recenzjach (dające pogląd, co się często powtarza).\n",
    "* Macierz to pięć wektorów po 20 liczb. Np. jeśli słowo \"movie\" jest pierwsze w słowniku, to pierwsza kolumna zawiera zliczenia słowa \"movie\" w każdym z 5 dokumentów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e9f2bc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Warto zauważyć, że większość wartości w tej macierzy to zapewne zera (bo każde zdanie zawiera tylko kilka z możliwych słów). Taka macierz Bag-of-Words jest **rzadka (sparse)**. `CountVectorizer` zwraca ją jako sparse matrix (oszczędnie przechowywaną). Wywołanie `.toarray()` konwertuje ją do pełnej tablicy – robiąc to dla całego zbioru 25k dokumentów i tysięcy cech byłoby bardzo pamięciożerne, dlatego unikamy tego na dużych danych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52483361",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Bag-of-Words na całym zbiorze IMDb\n",
    "\n",
    "Teraz zróbmy wektoryzację dla całego naszego zbioru treningowego (25k recenzji). Słownik będzie spory – można ograniczyć do np. 10k najczęstszych słów (`max_features=10000`), co czasem pomaga modelom i wydajności. Tutaj zróbmy np. 5000 dla demonstracji, by nie było za ciężko."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e1659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wektorowanie całego zbioru treningowego IMDb\n",
    "vectorizer_full = CountVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_bow = vectorizer_full.fit_transform(df_train['text'])  # używamy oryginalnych tekstów, a vectorizer sam wyczyści/tokenizuje prosto\n",
    "\n",
    "print(\"Liczba cech (słów) w słowniku:\", len(vectorizer_full.get_feature_names_out()))\n",
    "print(\"Przykładowe słowa:\", vectorizer_full.get_feature_names_out()[:10])\n",
    "print(\"Macierz BoW - rozmiar:\", X_train_bow.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b7f6c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Ta operacja może chwilę potrwać (przetwarza 25k dokumentów, buduje słownik 5000 słów). Po wykonaniu:\n",
    "\n",
    "* Zobaczymy potwierdzenie, że cech jest 5000 (nasze ograniczenie).\n",
    "* Pierwsze 10 słów (zapewne najbardziej popularnych): zwykle w recenzjach filmowych to mogą być słowa typu \"film\", \"movie\", \"one\", \"like\", \"good\", \"bad\" itp. (choć uwaga: `stop_words='english'` usunęło \"the\", \"and\" itd., więc ich nie będzie).\n",
    "* Rozmiar macierzy: (25000, 5000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b665c70",
   "metadata": {},
   "source": [
    "\n",
    "Teraz możemy również przekształcić zbiór testowy do takiej samej reprezentacji (używając `transform` na już wytrenowanym vectorizerze – **nie fitujemy ponownie na teście!**, aby słownik pozostał stały):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9401bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bow = vectorizer_full.transform(df_test['text'])\n",
    "print(\"Rozmiar macierzy BoW dla testu:\", X_test_bow.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b609ad",
   "metadata": {},
   "source": [
    "Mając `X_train_bow` i `X_test_bow`, a także odpowiadające im etykiety `y_train` i `y_test` (które mamy w `df_train['label']` itd.), jesteśmy gotowi do trenowania modeli (co zrobimy w notatniku 3).\n",
    "\n",
    "Jednak zanim przejdziemy do modeli, warto poznać jeszcze jedną reprezentację, która często daje lepsze wyniki niż gołe BoW: **TF-IDF**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162db49",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Reprezentacja TF-IDF\n",
    "\n",
    "![TF-IDF Defined - KDnuggets](https://www.kdnuggets.com/wp-content/uploads/arya-tf-idf-defined-0-1024x573.png \"TF-IDF Defined - KDnuggets\")\n",
    "\n",
    "TF-IDF (Term Frequency–Inverse Document Frequency) to modyfikacja Bag-of-Words, która oprócz częstości słowa w dokumencie (TF) uwzględnia także **uniwersalność** słowa w całym korpusie (DF – document frequency).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f6e98",
   "metadata": {},
   "source": [
    "Intuicja:\n",
    "\n",
    "* Jeśli słowo pojawia się w **wielu dokumentach**, to jego zdolność do rozróżniania dokumentów jest mała (np. słowo \"film\" pojawi się prawie we wszystkich recenzjach filmu – nie mówi czy recenzja jest pozytywna czy negatywna).\n",
    "* TF-IDF obniża wagę takich słów poprzez mnożenie TF przez IDF (inverse document frequency), które jest niższe dla słów występujących w wielu dokumentach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef59aa",
   "metadata": {},
   "source": [
    "Wzór (nie musisz go pamiętać na pamięć, ale warto znać ogólny kształt):\n",
    "\n",
    "$$\n",
    "tf\\text{-}idf(t,d) = tf(t,d) \\times \\log \\frac{N}{df(t)}\n",
    "$$\n",
    "\n",
    "gdzie:\n",
    "\n",
    "- $tf(t, d)$ – liczba wystąpień słowa $t$ w dokumencie $d$ (czasem używa się częstotliwości względnej),\n",
    "- $df(t)$ – liczba dokumentów, w których słowo $t$ występuje,\n",
    "- $N$ – liczba dokumentów w korpusie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6380b591",
   "metadata": {},
   "source": [
    "Logarytm sprawia, że wpływ \\$df\\$ jest znoszony w skali log (im rzadsze słowo, tym większe \\$\\log(N/df)\\$).\n",
    "\n",
    "W praktyce nie liczymy tego ręcznie – znów posłuży nam narzędzie z scikit-learn: `TfidfVectorizer`, używany podobnie do CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525a548",
   "metadata": {},
   "source": [
    "Spróbujmy na małym przykładzie porównać BoW vs TF-IDF:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24926d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sample_texts = [\n",
    "    \"I love this movie movie movie\",\n",
    "    \"This movie was bad and boring\",\n",
    "    \"What a great movie!\"\n",
    "]\n",
    "vect_count = CountVectorizer(stop_words='english')\n",
    "vect_tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_count = vect_count.fit_transform(sample_texts).toarray()\n",
    "X_tfidf = vect_tfidf.fit_transform(sample_texts).toarray()\n",
    "\n",
    "print(\"Słownik:\", vect_count.get_feature_names_out())\n",
    "print(\"BoW zliczenia:\\n\", X_count)\n",
    "print(\"TF-IDF wartości:\\n\", np.round(X_tfidf, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e2b23",
   "metadata": {},
   "source": [
    "\n",
    "Porównując:\n",
    "\n",
    "* W BoW macierzy zobaczymy czyste liczby (np. słowo \"movie\" może mieć liczbę 3 w pierwszym zdaniu).\n",
    "* W TF-IDF macierzy zobaczymy wartości ułamkowe. Słowo \"movie\" w pierwszym dokumencie dostanie dość wysoką wartość TF-IDF (bo TF=3, ale też występuje w wszystkich 3 dokumentach, więc IDF trochę to obniży). W drugim i trzecim dokumencie \"movie\" ma TF=1, ale w trzecim dokumencie to jedyne słowo prawie, więc tam TF-IDF może być najwyższe.\n",
    "\n",
    "Widać też, że słowo, które jest unikalne dla jednego dokumentu dostanie wyższą wagę niż słowo, które pojawia się wszędzie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b773fa0",
   "metadata": {},
   "source": [
    "\n",
    "### TF-IDF na naszych danych\n",
    "\n",
    "Zastosujmy TfidfVectorizer podobnie jak CountVectorizer wcześniej, na pełnym zbiorze treningowym IMDb:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(df_train['text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test['text'])\n",
    "\n",
    "print(\"Macierz TF-IDF - rozmiar:\", X_train_tfidf.shape)\n",
    "print(\"Przykładowe cechy:\", tfidf_vectorizer.get_feature_names_out()[:10])\n",
    "print(\"Wartości TF-IDF (fragment dla 1. dokumentu):\", X_train_tfidf[0, :10].toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef483951",
   "metadata": {},
   "source": [
    "\n",
    "Macierz TF-IDF będzie tego samego wymiaru (25000, 5000). Różni się tylko wartościami wewnątrz.\n",
    "\n",
    "**Ważne:** TF-IDF również zwraca macierz rzadką. Nie przekształcamy jej do pełnej tablicy jeśli nie musimy.\n",
    "\n",
    "Z punktu widzenia modeli uczenia, TF-IDF to po prostu inny zestaw cech. Wiele algorytmów (w tym Naive Bayes, regresja logistyczna) często działa lepiej z TF-IDF niż czystym BoW, ale to zależy od danych. Można też próbować używać obu na raz lub dodatkowych pomysłów (np. cecha czy w tekscie wystąpił wykrzyknik, długość tekstu itp., ale nie będziemy tego tutaj robić)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbfa208",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Podsumowanie przetwarzania tekstu\n",
    "\n",
    "Przeszliśmy przez kluczowe etapy przygotowania danych tekstowych:\n",
    "\n",
    "* Wyczyściliśmy tekst (lowercase, usunięcie znaków specjalnych).\n",
    "* Podzieliliśmy na tokeny (słowa).\n",
    "* Usunęliśmy stop words, które najczęściej nie wnoszą informacji.\n",
    "* (Opcjonalnie) Zastosowaliśmy stemming/lematyzację, by ujednolicić formy wyrazów.\n",
    "* Zamieniliśmy przetworzone teksty na reprezentacje numeryczne:\n",
    "\n",
    "  * Bag-of-Words (wystąpienia słów),\n",
    "  * TF-IDF (ważone wystąpienia słów)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55cdbc6",
   "metadata": {},
   "source": [
    "\n",
    "Mamy teraz gotowe cechy, na podstawie których możemy trenować model uczący się odróżniać opinie pozytywne od negatywnych.\n",
    "\n",
    "Teraz zajmiemy się trenowaniem modeli: użyjemy **Naiwnego Bayesa** i **Regresji Logistycznej**, a następnie ocenimy ich skuteczność na zbiorze testowym.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a34873",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Pytania:**\n",
    "* Dlaczego usuwamy stop words i jaki to ma wpływ na wielkość słownika cech.\n",
    "* Czym różni się stemming od lematyzacji (i czemu nie zawsze jest łatwo lematyzować).\n",
    "* Jak w wektorze BoW/TF-IDF reprezentowane są dokumenty (co znaczą poszczególne wartości).\n",
    "* Co oznacza skrót TF-IDF na intuicyjnym poziomie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a788bd1",
   "metadata": {},
   "source": [
    "## Quiz – utrwalenie pojęć\n",
    "\n",
    "1. **Dlaczego warto sprowadzać tekst do małych liter przed analizą?**\n",
    "2. **Podaj trzy przykłady** ***stop words*** **w języku polskim.**\n",
    "3. **Co to jest stemming?** (wyjaśnij własnymi słowami)\n",
    "4. **W modelu Bag-of-Words, czy kolejność słów w dokumencie ma znaczenie?**\n",
    "5. **Załóżmy, że słowo występuje we wszystkich dokumentach w korpusie. Jaką mniej więcej wartość TF-IDF to słowo otrzyma?** (wysoką czy niską i dlaczego?)\n",
    "\n",
    "**Twoje odpowiedzi:**\n",
    "\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "4. ...\n",
    "5. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c721c67",
   "metadata": {},
   "source": [
    "## Ćwiczenia praktyczne\n",
    "\n",
    "* **Ćwiczenie 1:** Zaimportuj drugi zbiór danych (polskojęzyczny, np. recenzje z PolEmo 2.0) podobnie jak zrobiliśmy to dla IMDb. Wczytaj dane i sprawdź ich rozmiar oraz przykładowe wpisy. *(Wskazówka: użyj* *`load_dataset(\"clarin-pl/polemo2-official\")`* *analogicznie do IMDb. Zobacz* *`dataset['train']`* *i* *`dataset['test']`.)*\n",
    "* **Ćwiczenie 2:** Zastosuj funkcje przetwarzające tekst do kilku polskich recenzji. Zwróć uwagę na różnice: czy nasza funkcja `clean_text` z polskimi znakami działa poprawnie? Czy lista `stop_words_pl` powinna zostać rozszerzona o dodatkowe słowa, które widzisz w danych? Zanotuj kilka obserwacji.\n",
    "* **Ćwiczenie 3:** Wektoruj polskie recenzje za pomocą `CountVectorizer` (jeśli dane są duże, możesz ustawić `max_features` na kilka tysięcy). Porównaj najczęstsze słowa w polskim zbiorze z tymi z angielskiego IMDb – czy są to głównie stop words, czy pojawiają się konkretne słowa związane z oceną (np. \"dobry\", \"świetny\", \"nudny\")? Co to mówi o danych?\n",
    "\n",
    "*(Możesz wykorzystać poniższe komórki na powyższe ćwiczenia.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Ćwiczenie 1 – wczytaj polski zbiór danych i przeanalizuj podstawowe informacje (wielkość, przykłady).\n",
    "\n",
    "\n",
    "# TODO: Ćwiczenie 2 – zastosuj preprocess_text_pl do kilku polskich recenzji i przeanalizuj wyniki.\n",
    "\n",
    "\n",
    "# TODO: Ćwiczenie 3 – CountVectorizer na polskich danych, analiza najczęstszych słów.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6060b8d",
   "metadata": {},
   "source": [
    "\n",
    "## Notatki własne\n",
    "\n",
    "*(Zanotuj tutaj swoje własne podsumowanie, rzeczy do zapamiętania, pytania.)*\n",
    "\n",
    "* ...\n",
    "* ...\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b569a8a0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Trenowanie modelu klasyfikacji sentymentu – Naive Bayes i Logistic Regression\n",
    "\n",
    "## Cele edukacyjne\n",
    "\n",
    "* **Przygotowanie danych do modelowania** – przypomnisz sobie uzyskane wektory cech (BoW/TF-IDF) i etykiety, ewentualnie dokonasz drobnego przygotowania (podział na train/test, filtrowanie klas jeśli potrzebne).\n",
    "* **Poznasz algorytm Naiwnego Bayesa (Multinomial Naive Bayes)** – zrozumiesz w intuicyjny sposób, jak wykorzystuje prawdopodobieństwa słów do klasyfikacji tekstu.\n",
    "* **Poznasz algorytm Regresji Logistycznej** – dowiesz się, że to model liniowy potrafiący przewidywać prawdopodobieństwo klasy, i jest często używany w zadaniach NLP.\n",
    "* **Nauczysz się trenować modele ML w scikit-learn** – utworzysz model, dopasujesz go do danych treningowych, dokonasz predykcji na danych testowych.\n",
    "* **Ocena modelu** – dowiesz się, jak ocenić skuteczność modelu: miara dokładności (accuracy), macierz pomyłek; porównasz wyniki modeli.\n",
    "* **Ćwiczenia i eksperymenty** – samodzielnie sprawdzisz wpływ pewnych zmian (np. inny typ reprezentacji cech, tuning parametrów) na działanie modeli, przetestujesz model na nowych przykładach tekstu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda210b2",
   "metadata": {},
   "source": [
    "## Przygotowanie danych\n",
    "\n",
    "Mamy już zebrane cechy tekstowe ze zbioru recenzji IMDb (w języku angielskim). Przypomnijmy:\n",
    "\n",
    "* Zbiór **treningowy**: 25 000 recenzji, z których każda jest reprezentowana jako wektor cech (np. 5000-cechowy wektor TF-IDF lub BoW) i ma etykietę 0 lub 1 (negatywna/pozytywna).\n",
    "* Zbiór **testowy**: 25 000 recenzji, również przekonwertowanych do tej samej przestrzeni cech, posłuży nam do niezależnej oceny modeli.\n",
    "\n",
    "Przypiszmy `y_train` i `y_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['label'].to_numpy()\n",
    "y_test = df_test['label'].to_numpy()\n",
    "\n",
    "print(\"Przykładowe etykiety treningowe:\", y_train[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f0c59",
   "metadata": {},
   "source": [
    "Upewnijmy się, że rozmiary się zgadzają z macierzami cech:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d00d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rozmiar X_train, y_train:\", X_train_tfidf.shape, y_train.shape)\n",
    "print(\"Rozmiar X_test, y_test:\", X_test_tfidf.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1927214",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Jeśli wszystko jest w porządku, możemy przejść do trenowania modeli.\n",
    "\n",
    "> Uwaga (dotycząca drugiego zbioru danych): Jeśli chcesz równolegle przećwiczyć użycie polskiego zbioru recenzji, możesz powtórzyć proces przetwarzania i wektoryzacji również dla niego. Dla przejrzystości, w dalszej części notatnika skupimy się na danych IMDb (angielskich), ale w sekcji ćwiczeń zachęcam do sprawdzenia modeli także na polskich danych. Dodatkowo, jeśli polski zbiór ma więcej niż dwie kategorie (np. PolEmo 2.0 ma 4 kategorie: pozytywne, negatywne, neutralne, mieszane), można rozważyć filtrację do dwóch głównych (pozytywne vs negatywne) dla porównywalności – o czym jeszcze wspomnimy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d40e8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Multinomial Naive Bayes – intuicja działania\n",
    "\n",
    "\n",
    "\n",
    "**Naiwny klasyfikator Bayesowski** to rodzina prostych modeli probabilistycznych. \"Naiwny\" – bo zakłada niezależność cech od siebie, co w przypadku słów w zdaniu jest założeniem uproszczonym (słowa w zdaniu oczywiście nie są w pełni niezależne). Mimo tego założenia, model często sprawdza się zaskakująco dobrze.\n",
    "\n",
    "Wariant **Multinomial Naive Bayes** jest dostosowany do cech będących zliczeniami (jak słowa). Intuicja:\n",
    "\n",
    "* Dla każdej klasy (pozytywne/negatywne) obliczamy prawdopodobieństwa wystąpienia danego słowa w tej klasie na podstawie danych treningowych.\n",
    "* Aby obliczyć prawdopodobieństwo, że nowy dokument jest np. pozytywny, model mnoży (aproksymuje) prawdopodobieństwa wszystkich słów w dokumencie pod warunkiem klasy pozytywnej oraz *prior* klasy (np. częstość pozytywnych w treningu).\n",
    "* Mówiąc prościej: NB sprawdza, do której klasy dokument \"bardziej pasuje\" pod względem użytych słów.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a73e9",
   "metadata": {},
   "source": [
    "\n",
    "![Multinomial Naive Bayes Explained: Function, Advantages & Disadvantages,  Applications](https://ik.imagekit.io/upgrad1/abroad-images/imageCompo/images/Picture2SWG591.png?pr-true \"Multinomial Naive Bayes Explained: Function, Advantages & Disadvantages,  Applications\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69774dd7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Przykład uproszczony: Jeśli słowo \"great\" jest 10 razy częstsze w pozytywnych recenzjach niż w negatywnych, a słowo \"boring\" jest częstsze w negatywnych, to:\n",
    "\n",
    "* Dokument zawierający \"great\" będzie miał wyższe prawdopodobieństwo bycia pozytywnym (pomnoży się wysoki warunek dla \"great|positive\").\n",
    "* Dokument z \"boring\" – wyższe prawdopodobieństwo bycia negatywnym.\n",
    "\n",
    "Oczywiście model weźmie pod uwagę **wszystkie słowa** i zbalansuje te wskazówki.\n",
    "\n",
    "Nie zagłębiamy się tu w wzory Bayesa (choć są ładne): ważne że NB sprowadza się do bardzo szybkiego zliczania i porównywania częstości słów.\n",
    "\n",
    "Scikit-learn udostępnia implementację `MultinomialNB` w module `naive_bayes`. Będziemy go używać.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c78a4d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Trenowanie modelu Naive Bayes\n",
    "\n",
    "Załóżmy, że będziemy używać reprezentacji **TF-IDF** jako cech (można też spróbować BoW – NB często dobrze radzi sobie z czystymi zliczeniami).\n",
    "\n",
    "Stwórzmy i wytrenujmy model NB na zbiorze treningowym:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5004d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Inicjalizacja modelu NB\n",
    "nb_model = MultinomialNB()\n",
    "# Trenowanie modelu (dopasowanie do danych treningowych)\n",
    "nb_model.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92cf03a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Model się trenuje bardzo szybko (NB jest wydajny nawet na duże zbiory).\n",
    "\n",
    "Teraz dokonamy predykcji na zbiorze testowym i obliczymy **dokładność (accuracy)**, czyli odsetek poprawnie zaklasyfikowanych recenzji.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c1428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predykcja na danych testowych\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "# Obliczenie accuracy\n",
    "acc_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(f\"Dokładność (accuracy) Naive Bayes: {acc_nb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd09f2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Czy to dobrze? Biorąc pod uwagę, że losowe strzelanie dałoby 50%, a ludzie w sumie się z grubsza zgadzają w ocenach filmów może w \\~90% (są kontrowersyjne filmy), wynik \\~85% jest całkiem niezły dla tak prostego modelu. Oczywiście da się lepiej, spróbujemy to poprawić logisticzną.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa35b400",
   "metadata": {},
   "source": [
    "\n",
    "### Analiza wyników i macierz pomyłek\n",
    "\n",
    "Sam accuracy to nie wszystko. Warto zobaczyć **macierz pomyłek (confusion matrix)**, by wiedzieć, gdzie model się mylił najczęściej – czy częściej bierze pozytywne za negatywne czy odwrotnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83006f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Oblicz macierz pomyłek\n",
    "cm = confusion_matrix(y_test, y_pred_nb)\n",
    "print(\"Macierz pomyłek (NB):\\n\", cm)\n",
    "\n",
    "# Zakładamy, że klasy są oznaczone jako unikalne wartości z y_test\n",
    "classes = np.unique(y_test)\n",
    "\n",
    "# Wizualizacja macierzy pomyłek jako heatmap z użyciem seaborn\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.title(\"Macierz pomyłek (NB)\")\n",
    "plt.xlabel(\"Predykcja\")\n",
    "plt.ylabel(\"Rzeczywista wartość\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b197d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Interpretacja macierzy pomyłek:\n",
    "\n",
    "* Element \\[0,0]: liczba **negatywnych** recenzji poprawnie zaklasyfikowanych jako negatywne (True Negatives).\n",
    "* Element \\[1,1]: liczba **pozytywnych** recenzji poprawnie zaklasyfikowanych jako pozytywne (True Positives).\n",
    "* Element \\[0,1]: liczba negatywnych recenzji błędnie zaklasyfikowanych jako pozytywne (False Positives).\n",
    "* Element \\[1,0]: liczba pozytywnych recenzji błędnie zaklasyfikowanych jako negatywne (False Negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d6a85",
   "metadata": {},
   "source": [
    "\n",
    "Idealny model miałby tylko przekątną niezerową. U nas pewnie pomyłki będą w obu kategoriach. Można policzyć np. *precision* i *recall* każdej klasy, ale nie zagłębiajmy się – przy symetrycznych kosztach błędu i zbalansowanych klasach accuracy wystarczy.\n",
    "\n",
    "**Pytanie:** Jeśli zobaczysz, że np. \\[0,1] (FN) jest większe od \\[1,0] (FP), co to oznacza? (Np. model częściej myli się myśląc, że negatywna recenzja jest pozytywna, niż odwrotnie). Z czego to może wynikać? *(To pytanie otwarte, pomyśl - może model ma \"tendencję\" do przewidywania jednej klasy? Można by to powiązać np. z proporcjami klas w treningu lub specyfiką danych).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ae27c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Regresja logistyczna – intuicja\n",
    "\n",
    "![Linear Regression vs Logistic Regression - Tpoint Tech](https://d2jdgazzki9vjm.cloudfront.net/tutorial/machine-learning/images/linear-regression-vs-logistic-regression.png \"Linear Regression vs Logistic Regression - Tpoint Tech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb97bcf3",
   "metadata": {},
   "source": [
    "\n",
    "**Regresja logistyczna** to również model liniowy, ale zamiast zakładać rozkłady jak NB, bezpośrednio uczy się funkcji 𝑓(cechy) → prawdopodobieństwo klasy.\n",
    "\n",
    "Wersja binarna (dwie klasy) regresji logistycznej:\n",
    "\n",
    "* Model przypisuje każdemu słowu (cecha) pewną **waga** – liczba dodatnia lub ujemna.\n",
    "* Suma (ważona) wszystkich cech w dokumencie daje wynik, który przekształcamy funkcją sigmoid w zakres \\[0,1] – to jest przewidywane prawdopodobieństwo klasy pozytywnej.\n",
    "* Jeśli to prawdopodobieństwo > 0.5 (zazwyczaj), model klasyfikuje jako pozytywny, inaczej negatywny."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef19af19",
   "metadata": {},
   "source": [
    "\n",
    "Te wagi są optymalizowane na zbiorze treningowym tak, by maksymalizować poprawność. Np. model może nauczyć się wagi +2 dla słowa \"great\" (sprzyja pozytywnej) i wagi -3 dla \"boring\" (sprzyja negatywnej), itd., oraz pewnego progu bazowego.\n",
    "\n",
    "Regresja logistyczna nie zakłada niezależności cech – w zasadzie każda cecha ma własną wagę, więc może trochę lepiej wykorzystać sytuacje, gdy pewne słowa występują razem (chociaż to wciąż model liniowy, więc nie modeluje *interakcji* między słowami bezpośrednio, ale może łączyć wagi).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e3951",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Trenowanie regresji logist. polega na iteracyjnym dostrajaniu wag (z użyciem np. algorytmu gradientowego). To może zająć nieco więcej czasu niż NB, zwłaszcza na 25k dokumentach i 5000 cech. Warto więc:\n",
    "\n",
    "* Być może ograniczyć liczbę iteracji (parametr `max_iter`).\n",
    "* Ewentualnie użyć tylko części cech (ale my już ograniczyliśmy do 5000).\n",
    "* Monitorować, czy model się zbiega (czasami ostrzega o braku zbieżności – wtedy zwiększamy `max_iter`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37b71e0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Trenowanie modelu logistycznego\n",
    "\n",
    "Skorzystamy z `LogisticRegression` z scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_model = LogisticRegression(max_iter=200)\n",
    "log_model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220acaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c5133",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Jeśli pojawi się ostrzeżenie o konwergencji, można zwiększyć `max_iter` np. do 300 czy 500. 200 zazwyczaj wystarcza dla tego rozmiaru problemu, ale zależy od parametrów domyślnych.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f48c775",
   "metadata": {},
   "source": [
    "Po wytrenowaniu, wykonajmy predykcję i policzmy accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_log = log_model.predict(X_test_tfidf)\n",
    "acc_log = accuracy_score(y_test, y_pred_log)\n",
    "print(f\"Dokładność (accuracy) Logistic Regression: {acc_log:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d5d063",
   "metadata": {},
   "source": [
    "Sprawdźmy macierz pomyłek dla LR:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a468c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_log = confusion_matrix(y_test, y_pred_log)\n",
    "print(\"Macierz pomyłek (LR):\\n\", cm_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb16b7e5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Porównaj z macierzą NB – czy któryś model popełnia wyraźnie mniej błędów określonego typu?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c55a12",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Analiza i porównanie modeli\n",
    "\n",
    "Teraz mamy wyniki dwóch modeli:\n",
    "\n",
    "* **Naive Bayes**\n",
    "* **Logistic Regression**\n",
    "\n",
    "Możemy je bezpośrednio porównać – LR wyszedł nieco lepszy. Często tak bywa, bo LR może bardziej dostosować się do danych kosztem dłuższego treningu. NB jest jednak prosty i bywa lepszy przy mniejszych danych.\n",
    "\n",
    "Ciekawe może być też sprawdzenie kilku **przykładowych predykcji** – weźmy parę recenzji z testu i zobaczmy, co modele przewidziały, a jaka jest prawda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762aff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weźmy kilka losowych indeksów\n",
    "indices = [0, 1, 2, 100, 101, 20000]  # przykładowe indeksy (możesz zmienić lub losować)\n",
    "for i in indices:\n",
    "    text = df_test['text'].iloc[i]\n",
    "    true_label = y_test[i]\n",
    "    pred_nb = y_pred_nb[i]\n",
    "    pred_log = y_pred_log[i]\n",
    "    print(f\"Recenzja: {text[:200]}...\")  # skracamy dla czytelności\n",
    "    print(f\"   Prawdziwa etykieta: {true_label} | NB przewidział: {pred_nb} | LR przewidziała: {pred_log} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12cf16",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Przejrzyj te wyniki:\n",
    "\n",
    "* Zobacz, gdzie model się pomylił (true\\_label != przewidywanie). Przeczytaj fragment recenzji – czy zawiera jakieś słowa, które mogły zmylić model? Np. recenzja negatywna może zawierać zdanie \"It was a good attempt but overall boring\" – zawiera słowo \"good\", które NB mógł przecenić.\n",
    "* Czy są różnice między NB a LR? Może któryś poprawnie, a drugi źle w pewnych przypadkach.\n",
    "\n",
    "To daje pewną intuicję, co modele \"myślą\". Pamiętaj, że oba modele są dość *\"głupie\"* w tym sensie, że patrzą tylko na worek słów. Nie rozumieją ironii, negacji w kontekście (poza słowem \"not\" jeśli je uwzględnimy, NB i LR wiedzą że \"not\" często sygnalizuje negatyw, ale zdanie \"not bad\" to pozytywne znaczenie, a model może uznać \"not\" i \"bad\" oba za sygnały negatywu niezależnie)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54501287",
   "metadata": {},
   "source": [
    "## Zastosowanie modelu do nowych przykładów\n",
    "\n",
    "Na koniec zobaczmy, jak użyć wytrenowanych modeli do oceny nowych tekstów spoza naszego zbioru.\n",
    "\n",
    "Napiszmy funkcję pomocniczą, która weźmie tekst (np. recenzję), przetworzy go tak samo jak dane treningowe i wydrukuje przewidywany sentyment.\n",
    "\n",
    "Ponieważ nasz `tfidf_vectorizer` był trenowany na danych angielskich IMDb, najlepiej podawać mu teksty angielskie o podobnej tematyce (recenzje filmów). Dla testu możemy spróbować również z polskim tekstem – zobaczymy, że raczej nie zadziała sensownie, bo słów nie będzie w słowniku.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text: str):\n",
    "    # Przekształcenie tekstu za pomocą tego samego wektoryzatora TF-IDF\n",
    "    text_vector = tfidf_vectorizer.transform([text])\n",
    "    # Predykcja NB i LR\n",
    "    pred_nb = nb_model.predict(text_vector)[0]\n",
    "    pred_log = log_model.predict(text_vector)[0]\n",
    "    # Prawdopodobieństwo pozytywnej klasy wg LR (nb ma swoje, ale trudno je porównywać bezkontekstowo)\n",
    "    prob_log = log_model.predict_proba(text_vector)[0,1]\n",
    "    # Wyświetlenie wyniku\n",
    "    sentiment_nb = \"pozytywna\" if pred_nb == 1 else \"negatywna\"\n",
    "    sentiment_log = \"pozytywna\" if pred_log == 1 else \"negatywna\"\n",
    "    print(f\"Tekst: {text}\")\n",
    "    print(f\"   NB: {sentiment_nb}, LR: {sentiment_log} (P(positive)={prob_log:.2f})\")\n",
    "\n",
    "# Przetestujmy na kilku zdaniach:\n",
    "predict_sentiment(\"This movie was absolutely fantastic! I loved it.\")\n",
    "predict_sentiment(\"Boring film. Waste of time... I do not recommend.\")\n",
    "predict_sentiment(\"It had some good moments, but overall it was disappointing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00968aab",
   "metadata": {},
   "source": [
    "Spróbuj wymyślić własne krótkie recenzje i zobacz, co wyjdzie:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b895a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: zmień teksty poniżej na własne i sprawdź wyniki\n",
    "predict_sentiment(\"The story was not bad, but not great either.\")\n",
    "predict_sentiment(\"One of the best movies I've ever seen!!!\")\n",
    "predict_sentiment(\"It was a movie. The actors spoke and there was music.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d75b5",
   "metadata": {},
   "source": [
    "Jeśli spróbujesz z polskim tekstem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d48b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(\"Ten film był naprawdę świetny! Chętnie obejrzę go ponownie.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f428952a",
   "metadata": {},
   "source": [
    "\n",
    "prawdopodobnie model nie rozpozna słów (\"film\" może akurat jest też w angielskim, ale \"świetny\" na pewno nie) – większość cech wyjdzie 0, więc model może dać przewidywanie bazujące na biasie (prawdopodobnie uzna np. negatywny, bo nie znalazł pozytywnych słów znanych mu). To pokazuje, że model działa tylko w zakresie języka i słownictwa, na którym został nauczony.\n",
    "\n",
    "**Wniosek:** Nasz model angielski nie zrozumie polskiego zdania i vice versa. Dlatego budując systemy na różne rynki językowe, trzeba trenować osobne modele lub wielojęzyczne podejścia (to bardziej skomplikowane).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c9c58",
   "metadata": {},
   "source": [
    "\n",
    "## Ćwiczenia i pomysły do samodzielnego sprawdzenia\n",
    "\n",
    "Możesz teraz poeksperymentować, aby lepiej poznać temat:\n",
    "\n",
    "1. **Bag-of-Words vs TF-IDF:** Spróbuj wytrenować modele NB i LR na surowych cechach BoW (np. użyj `X_train_bow` zamiast `X_train_tfidf`). Porównaj wyniki. Czy TF-IDF poprawiło wynik? Który model bardziej zyskał na TF-IDF?\n",
    "2. **Zmiana liczby cech:** Wektor TF-IDF, który zbudowaliśmy, miał ograniczenie do 5000 cech. Spróbuj zwiększyć do 10 000 lub użyć pełnego słownika (usunąć `max_features` w `TfidfVectorizer`). Uwaga: więcej cech może wydłużyć trening. Sprawdź, czy accuracy rośnie, maleje czy pozostaje podobne.\n",
    "3. **Dodanie n-gramów:** Dotąd używaliśmy pojedynczych słów jako cechy. `TfidfVectorizer` pozwala uwzględnić n-gramy (sekwekcje słów). Ustaw `ngram_range=(1,2)` aby dodać bigramy (pary słów). Wytrenowanie modelu z bigramami może wychwycić np. frazy \"not good\" jako osobną cechę. Sprawdź, czy to poprawi wyniki.\n",
    "4. **Inny algorytm:** Spróbuj innego modelu klasyfikacji, np. SVM (`sklearn.svm.LinearSVC`) lub drzewo decyzyjne (`sklearn.tree.DecisionTreeClassifier`). Porównaj ich skuteczność i czas trenowania z NB i LR.\n",
    "5. **Klasy wielokrotne:** Jeżeli masz drugi zbiór danych z więcej niż dwiema klasami (np. **PolEmo 2.0**: plus, minus, zero, amb), spróbuj zbudować model 4-klasowy na nim. NB i LR potrafią obsłużyć multi-klasy (LR domyślnie używa strategii one-vs-rest). Sprawdź confusion matrix – czy model głównie myli neutralne z negatywnymi, itp.? (Więcej klas to trudniejsze zadanie!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3110a",
   "metadata": {},
   "source": [
    "\n",
    "**Wykonaj przynajmniej część z powyższych i zanotuj wnioski.** Pamiętaj, że eksperymenty mogą trochę potrwać (szczególnie SVM lub duża liczba cech).\n",
    "\n",
    "*(Możesz wykorzystać poniższe komórki do eksperymentów.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Eksperyment 1 - Porównanie NB/LR na BoW vs TF-IDF\n",
    "\n",
    "# TODO: Eksperyment 2 - Zwiększenie liczby cech\n",
    "\n",
    "# TODO: Eksperyment 3 - Dodanie bigramów do cech\n",
    "\n",
    "# TODO: Eksperyment 4 - Wypróbowanie innego algorytmu (np. SVM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160428f4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "Nauczyliśmy się trenować modele klasyczne do analizy sentymentu:\n",
    "\n",
    "* **Multinomial Naive Bayes** – szybki, oparty na prawdopodobieństwach słów, całkiem skuteczny przy niezbyt dużej liczbie danych.\n",
    "* **Regresja Logistyczna** – model liniowy uczący się wag cech, zazwyczaj dający wysoką skuteczność kosztem dłuższego treningu.\n",
    "\n",
    "Oba modele uzyskały dokładność w okolicach 85-90% na naszym zbiorze testowym IMDb, co jest przyzwoitym wynikiem. W praktyce można by spróbować poprawić go:\n",
    "\n",
    "* Dodając więcej danych treningowych (nasze 25k to już sporo, ale zawsze można mieć więcej).\n",
    "* Stosując bardziej złożone modele (np. ensemble modeli lub sieci neuronowe).\n",
    "* Udoskonalając cechy (np. uwzględniając bigramy, lepsze przetwarzanie negacji, itp.).\n",
    "\n",
    "Kluczowe wnioski:\n",
    "\n",
    "* **Pipeline NLP**: surowy tekst → przetwarzanie (clean, tokenizacja, stopwords, etc.) → wektor cech → model ML → przewidywanie.\n",
    "* Model trzeba dobrać do problemu i danych. Naiwny Bayes sprawdzi się, gdy cechy są dość niezależne, regresja logistyczna poradzi sobie z korelacjami lepiej.\n",
    "* Ocena modelu powinna być przeprowadzona na **oddzielnym zbiorze testowym**, by mieć pewność, że model generalizuje, a nie tylko nauczył się na pamięć danych treningowych.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c79b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872165da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e75c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d89915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fdf699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c47d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181df77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": ".VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
