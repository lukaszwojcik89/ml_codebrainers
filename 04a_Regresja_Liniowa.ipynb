{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3856b0",
   "metadata": {},
   "source": [
    "# **Regresja Liniowa w Uczeniu Maszynowym**\n",
    "\n",
    "![](https://datascientest.com/en/files/2021/01/Machine-learning-def-.png)\n",
    "\n",
    "_Bartek Bilski_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d592a",
   "metadata": {},
   "source": [
    "## **WstÄ™p**\n",
    "\n",
    "Witaj na zajÄ™ciach dotyczÄ…cych **regresji liniowej** â€“ jednego z fundamentalnych algorytmÃ³w uczenia maszynowego. W ciÄ…gu tych zajÄ™Ä‡ poznasz podstawy regresji liniowej, nauczysz siÄ™ implementowaÄ‡ modele przy uÅ¼yciu scikit-learn oraz zrozumiesz, jak regularyzacja moÅ¼e poprawiÄ‡ jakoÅ›Ä‡ modeli.\n",
    "\n",
    "Regresja liniowa to idealny punkt startowy do nauki machine learning, poniewaÅ¼ jest intuicyjna matematycznie, a jednoczeÅ›nie stanowi podstawÄ™ dla bardziej zaawansowanych technik."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d522841",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Struktura notebooka**\n",
    "\n",
    "Notebook zawiera objaÅ›nienia teoretyczne oraz praktyczne przykÅ‚ady w Pythonie. Po kaÅ¼dej sekcji znajdziesz **zadania do samodzielnego wykonania** oznaczone jako **Zadanie**. ZachÄ™cam do aktywnego wykonywania tych Ä‡wiczeÅ„.\n",
    "\n",
    "### **Podstawowe PojÄ™cia Statystyczne â€“ Wprowadzenie**\n",
    "\n",
    "- **Zmienna objaÅ›niajÄ…ca (feature)**: Atrybut opisujÄ…cy dane wejÅ›ciowe (np. metraÅ¼ mieszkania).\n",
    "- **Zmienna docelowa (target)**: WielkoÅ›Ä‡, ktÃ³rÄ… chcemy przewidzieÄ‡ (np. cena mieszkania).\n",
    "- **PrÃ³bka i populacja**: Na podstawie prÃ³by (posiadanych danych) wnioskujemy o caÅ‚ej populacji.\n",
    "- **Miary poÅ‚oÅ¼enia**: Å›rednia, mediana.\n",
    "- **Miary zmiennoÅ›ci**: wariancja, odchylenie standardowe.\n",
    "- **RozkÅ‚ady prawdopodobieÅ„stwa**: np. rozkÅ‚ad normalny.\n",
    "\n",
    "Statystyka pomaga nam lepiej zrozumieÄ‡ dane i ich struktury zanim zbudujemy model ML.\n",
    "\n",
    "![](https://i.ibb.co/KVCdqr3/Screenshot-2024-12-10-112852.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85044f5",
   "metadata": {},
   "source": [
    "### **Typy zmiennych w analizie danych**\n",
    "\n",
    "W procesie eksploracji i modelowania danych niezwykle istotne jest zrozumienie typÃ³w zmiennych w zbiorze danych.\n",
    "\n",
    "**GÅ‚Ã³wne rodzaje zmiennych:**\n",
    "\n",
    "1. **Numeryczne**\n",
    "   - **CiÄ…gÅ‚e:** MogÄ… przyjmowaÄ‡ dowolnÄ… wartoÅ›Ä‡ z pewnego przedziaÅ‚u (np. metraÅ¼, temperatura)\n",
    "   - **Dyskretne:** PrzyjmujÄ… konkretne, zliczalne wartoÅ›ci (np. liczba pokoi)\n",
    "\n",
    "2. **Kategoryczne**\n",
    "   - Nie majÄ… naturalnego porzÄ…dku (np. nazwy miast, typ nieruchomoÅ›ci)\n",
    "   - WymagajÄ… enkodowania do postaci numerycznej (One-Hot Encoding)\n",
    "\n",
    "3. **PorzÄ…dkowe (ordinal)**\n",
    "   - MajÄ… naturalny porzÄ…dek (np. ocena jakoÅ›ci 1-5, poziom edukacji)\n",
    "   - MoÅ¼na kodowaÄ‡ jako liczby zachowujÄ…c hierarchiÄ™\n",
    "\n",
    "\n",
    "![](https://statsandr.com/blog/variable-types_files/variable-types-and-examples.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59c4e1",
   "metadata": {},
   "source": [
    "## **Co to jest regresja?**\n",
    "\n",
    "**Regresja** to typ uczenia nadzorowanego, ktÃ³rego celem jest przewidywanie **wartoÅ›ci ciÄ…gÅ‚ych** (numerycznych) na podstawie innych zmiennych. W przeciwieÅ„stwie do klasyfikacji, ktÃ³ra przewiduje kategorie, regresja przewiduje konkretne liczby.\n",
    "\n",
    "**PrzykÅ‚ady zastosowaÅ„ regresji:**\n",
    "- Przewidywanie cen nieruchomoÅ›ci na podstawie powierzchni, lokalizacji, wieku\n",
    "- Prognozowanie sprzedaÅ¼y na podstawie nakÅ‚adÃ³w na reklamÄ™\n",
    "- Szacowanie temperatury na podstawie ciÅ›nienia atmosferycznego\n",
    "\n",
    "**Popularne algorytmy regresji:**\n",
    "- Regresja liniowa (Linear Regression)\n",
    "- Drzewa regresyjne (Regression Trees) \n",
    "- Las losowy (Random Forest)\n",
    "- Sieci neuronowe (Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d122e58",
   "metadata": {},
   "source": [
    "## **Teoria Regresji Liniowej**\n",
    "\n",
    "### **Czym jest regresja liniowa?**\n",
    "\n",
    "Regresja liniowa to metoda statystyczna uÅ¼ywana do modelowania zwiÄ…zku miÄ™dzy zmiennÄ… zaleÅ¼nÄ… (docelowÄ…) a jednÄ… lub wiÄ™cej zmiennymi niezaleÅ¼nymi (cechami). GÅ‚Ã³wne zaÅ‚oÅ¼enia:\n",
    "\n",
    "1. **LiniowoÅ›Ä‡**: ZwiÄ…zek miÄ™dzy zmiennymi jest liniowy\n",
    "2. **NiezaleÅ¼noÅ›Ä‡**: Obserwacje sÄ… niezaleÅ¼ne od siebie\n",
    "3. **HomoscedastycznoÅ›Ä‡**: Wariancja reszt jest staÅ‚a\n",
    "4. **NormalnoÅ›Ä‡**: Reszty majÄ… rozkÅ‚ad normalny\n",
    "\n",
    "### **RÃ³wnanie regresji liniowej:**\n",
    "\n",
    "**Regresja prosta (jedna cecha):**\n",
    "$$ y = \\beta_0 + \\beta_1 x + \\epsilon $$\n",
    "\n",
    "**Regresja wielokrotna (wiele cech):**\n",
    "$$y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™ + Îµ$$\n",
    "Gdzie:\n",
    "- $y$ - zmienna docelowa (np. cena)\n",
    "- $x_1,x_2,x_n$ - cechy (zmienne objaÅ›niajÄ…ce)\n",
    "- $\\beta_0$ - wyraz wolny (przeciÄ™cie z osiÄ… Y, intercept)\n",
    "- $\\beta_1, \\beta_2, \\beta_n$ - wspÃ³Å‚czynniki regresji (slopes)\n",
    "- $\\epsilon$ - skÅ‚adnik bÅ‚Ä™du\n",
    "\n",
    "**Interpretacja parametrÃ³w:**\n",
    "- $\\beta_0$: wartoÅ›Ä‡ $y$ gdy $x = 0$\n",
    "- $\\beta_1$: o ile zmienia siÄ™ $y$ przy wzroÅ›cie $x$ o 1 jednostkÄ™\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d0f8b",
   "metadata": {},
   "source": [
    "\n",
    "### **Jak dziaÅ‚a algorytm?**\n",
    "#### **Metoda Najmniejszych KwadratÃ³w (Ordinary Least Squares - OLS)**\n",
    "\n",
    "Aby znaleÅºÄ‡ najlepszÄ… liniÄ™ regresji, minimalizujemy **sumÄ™ kwadratÃ³w bÅ‚Ä™dÃ³w** (SSE):\n",
    "\n",
    "$$ \\text{SSE} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "gdzie $\\hat{y}_i = \\beta_0 + \\beta_1 x_i$ to przewidywana wartoÅ›Ä‡.\n",
    "\n",
    "**RozwiÄ…zania analityczne dla regresji jednowymiarowej:**\n",
    "\n",
    "$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} $$\n",
    "\n",
    "$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\n",
    "\n",
    "Metoda OLS gwarantuje znalezienie globalnego optimum dla regresji liniowej.\n",
    "\n",
    "![](https://vitalflux.com/wp-content/uploads/2022/02/ordinary-least-squares-method.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6591c36",
   "metadata": {},
   "source": [
    "## **Problem Overfittingu i Regularyzacja**\n",
    "\n",
    "### **Overfitting w regresji liniowej**\n",
    "\n",
    "Kiedy model ma zbyt wiele cech w stosunku do liczby obserwacji, moÅ¼e wystÄ…piÄ‡ **overfitting**:\n",
    "- Model \"zapamiÄ™tuje\" dane treningowe\n",
    "- SÅ‚abo generalizuje na nowe dane\n",
    "- WspÃ³Å‚czynniki mogÄ… byÄ‡ bardzo duÅ¼e i niestabilne\n",
    "\n",
    "### **Regularyzacja jako rozwiÄ…zanie**\n",
    "\n",
    "Regularyzacja dodaje \"karÄ™\" za zbyt duÅ¼e wspÃ³Å‚czynniki, wymuszajÄ…c prostsze modele:\n",
    "\n",
    "**Funkcja kosztu z regularyzacjÄ…:**\n",
    "$$ \\text{Koszt} = \\text{SSE} + \\lambda \\cdot \\text{Kara} $$\n",
    "\n",
    "gdzie $\\lambda$ (lambda) kontroluje siÅ‚Ä™ regularyzacji.\n",
    "\n",
    "### **Lasso (L1 Regularization)**\n",
    "\n",
    "**Idea:** Dodaje karÄ™ $$ \\lambda \\sum |\\beta_j| $$ do funkcji kosztu.\n",
    "\n",
    "**Efekty Lasso:**\n",
    "- **Selekcja cech:** MoÅ¼e wyzerowaÄ‡ wspÃ³Å‚czynniki nieistotnych cech\n",
    "- **Modele rzadkie:** PozostajÄ… tylko najwaÅ¼niejsze cechy\n",
    "- **Åatwiejsza interpretacja:** Model uÅ¼ywa mniej zmiennych\n",
    "\n",
    "**Kiedy uÅ¼ywaÄ‡ Lasso:**\n",
    "\n",
    "âœ… Gdy masz tysiÄ…ce cech i chcesz przeprowadziÄ‡ selekcjÄ™  \n",
    "âœ… Gdy zaleÅ¼y Ci na interpretowalnoÅ›ci modelu  \n",
    "âœ… W wysokowymiarowych problemach (wiÄ™cej cech niÅ¼ obserwacji).\n",
    "### **Ridge (L2 Regularization)**\n",
    "\n",
    "**Idea:** Dodaje karÄ™ $\\lambda \\sum \\beta_j^2$ do funkcji kosztu.\n",
    "\n",
    "**Efekty Ridge:**\n",
    "- **Stabilizacja wspÃ³Å‚czynnikÃ³w:** Wszystkie wspÃ³Å‚czynniki stajÄ… siÄ™ mniejsze\n",
    "- **ObsÅ‚uga multikolinearnoÅ›ci:** Lepiej radzi sobie ze skorelowanymi cechami\n",
    "- **Zachowanie wszystkich cech:** Nie zeruje wspÃ³Å‚czynnikÃ³w\n",
    "\n",
    "**Kiedy uÅ¼ywaÄ‡ Ridge:**\n",
    "\n",
    "âœ… Gdy masz wiele podobnych/skorelowanych cech  \n",
    "âœ… Gdy nie chcesz eliminowaÄ‡ Å¼adnych zmiennych  \n",
    "âœ… Gdy chcesz stabilizowaÄ‡ model bez utraty informacji\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Elastic Net**\n",
    "**Idea:** Dodaje karÄ™ $\\lambda \\sum \\beta_j^2$ + $ \\lambda \\sum |\\beta_j| $ do funkcji kosztu.\n",
    "\n",
    "**Efekty Elastic Net**\n",
    "- Kombinacja Ridge i Lasso\n",
    "- Balansuje miÄ™dzy stabilnoÅ›ciÄ… a selekcjÄ… cech\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dd53a3",
   "metadata": {},
   "source": [
    "## **Wczytanie i Eksploracja Danych - California Housing**\n",
    "\n",
    "BÄ™dziemy pracowaÄ‡ ze zbiorem danych **California Housing**, ktÃ³ry zawiera informacje o cenach domÃ³w w Kalifornii z 1990 roku. To klasyczny dataset do nauki regresji liniowej.\n",
    "\n",
    "### **Opis zbioru danych:**\n",
    "- **20,640 obserwacji** (blokÃ³w mieszkaniowych)\n",
    "- **8 cech numerycznych**\n",
    "- **Zmienna docelowa**: Mediana wartoÅ›ci domÃ³w (w $100k)\n",
    "\n",
    "### **Cechy w zbiorze:**\n",
    "1. **MedInc**: Mediana dochodu w bloku (w $10k)\n",
    "2. **HouseAge**: Mediana wieku domÃ³w w bloku\n",
    "3. **AveRooms**: Åšrednia liczba pokoi na dom\n",
    "4. **AveBedrms**: Åšrednia liczba sypialni na dom\n",
    "5. **Population**: Populacja bloku\n",
    "6. **AveOccup**: Åšrednia liczba mieszkaÅ„cÃ³w na dom\n",
    "7. **Latitude**: SzerokoÅ›Ä‡ geograficzna\n",
    "8. **Longitude**: DÅ‚ugoÅ›Ä‡ geograficzna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### =====================================\n",
    "### IMPORT BIBLIOTEK I KONFIGURACJA\n",
    "### =====================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Ustawienia wizualizacji\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"âœ… Wszystkie biblioteki zostaÅ‚y zaimportowane pomyÅ›lnie!\")\n",
    "print(\"ğŸš€ JesteÅ›my gotowi do pracy z regresjÄ… liniowÄ…!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# WCZYTANIE DANYCH CALIFORNIA HOUSING\n",
    "# =====================================\n",
    "\n",
    "print(\"ğŸ“Š WCZYTYWANIE DANYCH CALIFORNIA HOUSING:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Wczytanie danych\n",
    "data = fetch_california_housing()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['MedHouseVal'] = data.target\n",
    "\n",
    "print(f\"âœ… Dane wczytane pomyÅ›lnie!\")\n",
    "print(f\"ğŸ“ Rozmiar zbioru: {df.shape[0]} obserwacji, {df.shape[1]} kolumn\")\n",
    "print(f\"ğŸ¯ Zmienna docelowa: MedHouseVal (mediana wartoÅ›ci domÃ³w)\")\n",
    "\n",
    "# Podstawowe informacje\n",
    "print(\"\\nğŸ“‹ PODSTAWOWE INFORMACJE O DANYCH:\")\n",
    "print(\"=\" * 50)\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nğŸ‘€ PODGLÄ„D PIERWSZYCH 5 WIERSZY:\")\n",
    "print(\"=\" * 50)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Statystyki opisowe\n",
    "print(\"\\nğŸ“ˆ STATYSTYKI OPISOWE:\")\n",
    "print(\"=\" * 50)\n",
    "display(df.describe().round(2))\n",
    "\n",
    "print(\"\\nğŸ’¡ KLUCZOWE OBSERWACJE:\")\n",
    "print(\"âœ… Wszystkie zmienne sÄ… numeryczne\")\n",
    "print(\"âœ… Brak wartoÅ›ci NaN - dane sÄ… kompletne\")\n",
    "print(\"âš ï¸ RÃ³Å¼ne skale danych (MedInc: 0-15, Population: 3-35k)\")\n",
    "print(\"âš ï¸ MoÅ¼liwe outliers (max AveRooms: 141!)\")\n",
    "print(\"ğŸ¯ Zmienna docelowa: MedHouseVal (0.15 - 5.0 * $100k)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3de43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Sprawdzenie brakujÄ…cych wartoÅ›ci\n",
    "print(\"ğŸ” SPRAWDZENIE KOMPLETNOÅšCI DANYCH:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_data = df.isnull().sum()\n",
    "print(\"BrakujÄ…ce wartoÅ›ci na kolumnÄ™:\")\n",
    "print(missing_data)\n",
    "\n",
    "if missing_data.sum() == 0:\n",
    "    print(\"\\nâœ… Å»adnych brakujÄ…cych wartoÅ›ci - dane sÄ… kompletne!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Znaleziono {missing_data.sum()} brakujÄ…cych wartoÅ›ci\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Wizualizacja rozkÅ‚adu zmiennej docelowej\n",
    "print(\"ğŸ“Š ANALIZA ZMIENNEJ DOCELOWEJ (CENY DOMÃ“W):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['MedHouseVal'], kde=True, bins=50)\n",
    "plt.title(\"RozkÅ‚ad cen domÃ³w\")\n",
    "plt.xlabel(\"Mediana wartoÅ›ci domÃ³w ($100k)\")\n",
    "plt.ylabel(\"Liczba obserwacji\")\n",
    "\n",
    "# Boxplot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df['MedHouseVal'])\n",
    "plt.title(\"RozkÅ‚ad cen - wykres pudeÅ‚kowy\")\n",
    "plt.ylabel(\"Mediana wartoÅ›ci domÃ³w ($100k)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sprawdzenie wartoÅ›ci maksymalnych\n",
    "max_value = df['MedHouseVal'].max()\n",
    "max_count = (df['MedHouseVal'] == max_value).sum()\n",
    "print(f\"\\nğŸ” Analiza wartoÅ›ci maksymalnych:\")\n",
    "print(f\"Maksymalna wartoÅ›Ä‡: ${max_value} * 100k = ${max_value * 100000}\")\n",
    "print(f\"Liczba obserwacji z maksymalnÄ… wartoÅ›ciÄ…: {max_count}\")\n",
    "if max_count > 100:\n",
    "    print(\"âš ï¸ MoÅ¼liwe, Å¼e dane sÄ… 'ucapowane' (ograniczone od gÃ³ry)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c80dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ˆ Analiza zaleÅ¼noÅ›ci miÄ™dzy kluczowymi zmiennymi\n",
    "print(\"ğŸ“ˆ ZALEÅ»NOÅšCI MIÄ˜DZY ZMIENNYMI:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Wybieramy najciekawsze zmienne do analizy\n",
    "key_vars = ['MedHouseVal', 'MedInc', 'AveRooms', 'HouseAge', 'Population']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.pairplot(df[key_vars], diag_kind='hist', plot_kws={'alpha': 0.6})\n",
    "plt.suptitle('ZaleÅ¼noÅ›ci miÄ™dzy kluczowymi zmiennymi', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ Na co zwrÃ³ciÄ‡ uwagÄ™:\")\n",
    "print(\"- Czy widaÄ‡ liniowe zaleÅ¼noÅ›ci?\")\n",
    "print(\"- KtÃ³re zmienne wydajÄ… siÄ™ najsilniej skorelowane z cenÄ…?\")\n",
    "print(\"- Czy sÄ… wartoÅ›ci odstajÄ…ce?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¥ Mapa korelacji\n",
    "print(\"ğŸ”¥ ANALIZA KORELACJI:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, \n",
    "           annot=True, \n",
    "           cmap='RdBu_r', \n",
    "           center=0,\n",
    "           square=True,\n",
    "           linewidths=0.5,\n",
    "           cbar_kws={\"shrink\": .8})\n",
    "plt.title('Mapa korelacji - California Housing Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analiza korelacji z zmiennÄ… docelowÄ…\n",
    "print(\"\\nğŸ“ˆ KORELACJE Z CENÄ„ DOMÃ“W:\")\n",
    "target_corr = correlation_matrix['MedHouseVal'].drop('MedHouseVal').abs().sort_values(ascending=False)\n",
    "for var, corr in target_corr.items():\n",
    "    strength = \"Silna\" if corr > 0.6 else \"Umiarkowana\" if corr > 0.3 else \"SÅ‚aba\"\n",
    "    print(f\"{var:12s}: {corr:+.3f} ({strength})\")\n",
    "\n",
    "print(f\"\\nğŸ† Najsilniejszy predyktor: {target_corr.index[0]} (r={target_corr.iloc[0]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§¹ Czyszczenie danych (opcjonalnie)\n",
    "print(\"ğŸ§¹ CZYSZCZENIE DANYCH:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "max_value = df['MedHouseVal'].max()\n",
    "capped_count = (df['MedHouseVal'] == max_value).sum()\n",
    "\n",
    "print(f\"Obserwacje z maksymalnÄ… wartoÅ›ciÄ… ({max_value}): {capped_count}\")\n",
    "print(f\"Procent obserwacji: {(capped_count/len(df)*100):.1f}%\")\n",
    "\n",
    "if capped_count > 100:  # JeÅ›li wiÄ™cej niÅ¼ 100 obserwacji ma maksymalnÄ… wartoÅ›Ä‡\n",
    "    print(f\"\\nğŸ“Š RozwaÅ¼amy usuniÄ™cie {capped_count} obserwacji z 'ucapowanymi' wartoÅ›ciami\")\n",
    "    print(\"(Te wartoÅ›ci mogÄ… byÄ‡ artefaktem sposobu zbierania danych)\")\n",
    "    \n",
    "    # Dla tego tutoriala zachowujemy wszystkie dane\n",
    "    print(\"ğŸ”§ Dla celÃ³w edukacyjnych zachowujemy wszystkie dane\")\n",
    "else:\n",
    "    print(\"\\nâœ… Brak potrzeby usuwania danych\")\n",
    "\n",
    "# Wizualizacja gÅ‚Ã³wnej zaleÅ¼noÅ›ci\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['MedInc'], df['MedHouseVal'], alpha=0.5, s=20)\n",
    "plt.xlabel(\"Median Income (in $10k)\")\n",
    "plt.ylabel(\"Median House Value (in $100k)\")\n",
    "plt.title(\"ZaleÅ¼noÅ›Ä‡ miÄ™dzy dochodem a cenÄ… domÃ³w\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Obserwacja: WyraÅºna pozytywna korelacja miÄ™dzy dochodem a cenÄ…!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a902e088",
   "metadata": {},
   "source": [
    "## **Przygotowanie Danych do Modelowania**\n",
    "\n",
    "Przed rozpoczÄ™ciem modelowania musimy:\n",
    "\n",
    "1. **PodzieliÄ‡ dane** na cechy (X) i zmiennÄ… docelowÄ… (y)\n",
    "2. **RozdzieliÄ‡ zbiÃ³r** na treningowy i testowy\n",
    "3. **PrzeanalizowaÄ‡ skale** cech (waÅ¼ne dla regularyzacji)\n",
    "4. **PrzygotowaÄ‡ pipeline** z preprocessingiem\n",
    "\n",
    "### **Dlaczego podziaÅ‚ train/test jest waÅ¼ny?**\n",
    "\n",
    "- **Zapobiega overfitting**: Model uczy siÄ™ tylko na danych treningowych\n",
    "- **Ocena generalizacji**: Testujemy na \"niewidzianych\" danych\n",
    "- **Uczciwa ewaluacja**: Symuluje rzeczywiste warunki uÅ¼ycia modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e17c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PRZYGOTOWANIE DANYCH DO MODELOWANIA\n",
    "# =====================================\n",
    "\n",
    "print(\"ğŸ”§ PRZYGOTOWANIE DANYCH DO MODELOWANIA:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# PodziaÅ‚ na cechy (X) i zmiennÄ… docelowÄ… (y)\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "print(f\"KsztaÅ‚t macierzy cech (X): {X.shape}\")\n",
    "print(f\"KsztaÅ‚t wektora etykiet (y): {y.shape}\")\n",
    "print(f\"\\nCechy w modelu: {list(X.columns)}\")\n",
    "\n",
    "# PodziaÅ‚ na zbiory treningowy i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,    # 20% danych na test\n",
    "    random_state=42   # Dla powtarzalnoÅ›ci wynikÃ³w\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š PODZIAÅ DANYCH:\")\n",
    "print(f\"ZbiÃ³r treningowy: {X_train.shape[0]} obserwacji ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"ZbiÃ³r testowy:    {X_test.shape[0]} obserwacji ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(\"âœ… PodziaÅ‚ zachowuje proporcje!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c17bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Analiza skal cech\n",
    "print(\"ğŸ” ANALIZA SKAL CECH:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Statystyki skal\n",
    "scaling_analysis = pd.DataFrame({\n",
    "    'Cecha': X_train.columns,\n",
    "    'Åšrednia': X_train.mean(),\n",
    "    'Odch. std.': X_train.std(),\n",
    "    'Minimum': X_train.min(),\n",
    "    'Maksimum': X_train.max(),\n",
    "    'Zakres': X_train.max() - X_train.min()\n",
    "}).round(2)\n",
    "\n",
    "display(scaling_analysis)\n",
    "\n",
    "print(\"\\nğŸ’¡ PROBLEM:\")\n",
    "print(\"- MedInc: skala 0-15 (dochody w dziesiÄ…tkach tysiÄ™cy)\")\n",
    "print(\"- Population: skala 3-35k (liczba mieszkaÅ„cÃ³w)\")\n",
    "print(\"- AveRooms: skala 0-140 (pokoje na dom)\")\n",
    "print(\"\\nâš ï¸ RÃ³Å¼ne skale mogÄ… zdominowaÄ‡ model!\")\n",
    "print(\"ğŸ”§ RozwiÄ…zanie: Skalowanie (StandardScaler)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2cf0b5",
   "metadata": {},
   "source": [
    "## **Pierwszy Model - Regresja Liniowa bez Preprocessingu**\n",
    "\n",
    "Zacznijmy od prostego modelu bez skalowania, Å¼eby zobaczyÄ‡ bazowe wyniki, a nastÄ™pnie porÃ³wnamy je z modelem ze skalowaniem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e83ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PIERWSZY MODEL - BASIC LINEAR REGRESSION\n",
    "# =====================================\n",
    "\n",
    "print(\"ğŸš€ PIERWSZY MODEL - REGRESJA LINIOWA (BEZ PREPROCESSINGU):\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¯ Cel: Uzyskanie wynikÃ³w bazowych do porÃ³wnania\")\n",
    "\n",
    "# Tworzenie i trenowanie modelu\n",
    "lin_reg_basic = LinearRegression()\n",
    "lin_reg_basic.fit(X_train, y_train)\n",
    "\n",
    "# Predykcje\n",
    "y_pred_basic = lin_reg_basic.predict(X_test)\n",
    "\n",
    "# Obliczenie metryk\n",
    "mse_basic = mean_squared_error(y_test, y_pred_basic)\n",
    "rmse_basic = np.sqrt(mse_basic)\n",
    "r2_basic = r2_score(y_test, y_pred_basic)\n",
    "mae_basic = mean_absolute_error(y_test, y_pred_basic)\n",
    "\n",
    "print(\"ğŸ“Š WYNIKI MODELU BAZOWEGO:\")\n",
    "print(f\"MSE:  {mse_basic:.4f}\")\n",
    "print(f\"RMSE: ${rmse_basic*100:.0f}k (Å›redni bÅ‚Ä…d predykcji)\")\n",
    "print(f\"RÂ²:   {r2_basic:.4f} ({r2_basic*100:.1f}% wyjaÅ›nionej zmiennoÅ›ci)\")\n",
    "print(f\"MAE:  ${mae_basic*100:.0f}k (Å›redni bÅ‚Ä…d bezwzglÄ™dny)\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ INTERPRETACJA:\")\n",
    "print(f\"Model wyjaÅ›nia {r2_basic*100:.1f}% zmiennoÅ›ci cen domÃ³w\")\n",
    "print(f\"Åšredni bÅ‚Ä…d predykcji: Â±${rmse_basic*100:.0f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31629770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Wizualizacja wynikÃ³w\n",
    "print(\"ğŸ“Š WIZUALIZACJA WYNIKÃ“W:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Predykcje vs rzeczywiste wartoÅ›ci\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, y_pred_basic, alpha=0.6, s=30)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Rzeczywiste ceny')\n",
    "plt.ylabel('Przewidywane ceny')\n",
    "plt.title('Predykcje vs Rzeczywiste wartoÅ›ci')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram reszt\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = y_test - y_pred_basic\n",
    "plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Reszty (bÅ‚Ä™dy predykcji)')\n",
    "plt.ylabel('Liczba obserwacji')\n",
    "plt.title('RozkÅ‚ad reszt')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ Obserwacje z wykresÃ³w:\")\n",
    "print(\"- Punkty blisko linii y=x oznaczajÄ… dobre predykcje\")\n",
    "print(\"- Reszty powinny byÄ‡ normalnie rozÅ‚oÅ¼one wokÃ³Å‚ zera\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Analiza wspÃ³Å‚czynnikÃ³w\n",
    "print(\"ğŸ” ANALIZA WSPÃ“ÅCZYNNIKÃ“W REGRESJI:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Tworzenie DataFrame z wspÃ³Å‚czynnikami\n",
    "coef_df = pd.DataFrame({\n",
    "    'Cecha': X.columns,\n",
    "    'WspÃ³Å‚czynnik': lin_reg_basic.coef_,\n",
    "    'WartoÅ›Ä‡_bezwzglÄ™dna': np.abs(lin_reg_basic.coef_)\n",
    "})\n",
    "\n",
    "# Sortowanie wedÅ‚ug wartoÅ›ci bezwzglÄ™dnej\n",
    "coef_df = coef_df.sort_values('WartoÅ›Ä‡_bezwzglÄ™dna', ascending=False)\n",
    "\n",
    "print(\"ğŸ“Š RANKING WAÅ»NOÅšCI CECH:\")\n",
    "display(coef_df.round(4))\n",
    "\n",
    "print(f\"\\nğŸ  WYRAZ WOLNY (Intercept): {lin_reg_basic.intercept_:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ INTERPRETACJA WSPÃ“ÅCZYNNIKÃ“W:\")\n",
    "print(\"- Dodatni wspÃ³Å‚czynnik = wzrost cechy zwiÄ™ksza cenÄ™\")\n",
    "print(\"- Ujemny wspÃ³Å‚czynnik = wzrost cechy zmniejsza cenÄ™\")\n",
    "print(\"- WiÄ™ksza wartoÅ›Ä‡ bezwzglÄ™dna = silniejszy wpÅ‚yw\")\n",
    "print(\"\\nâš ï¸ UWAGA: WspÃ³Å‚czynniki zaleÅ¼Ä… od skali cech!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9044ed3",
   "metadata": {},
   "source": [
    "## **Model z Preprocessingiem - Skalowanie Danych**\n",
    "\n",
    "Teraz stworzymy model z odpowiednim skalowaniem danych. Wykorzystamy **Pipeline** - potÄ™Å¼ne narzÄ™dzie scikit-learn, ktÃ³re:\n",
    "\n",
    "1. **Automatyzuje** preprocessing i modelowanie\n",
    "2. **Zapobiega data leakage** (preprocessor uczony tylko na train)\n",
    "3. **Upraszcza kod** i zwiÄ™ksza czytelnoÅ›Ä‡\n",
    "4. **UÅ‚atwia wdroÅ¼enie** modelu w produkcji\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d2c9d",
   "metadata": {},
   "source": [
    "**Popularne metody skalowania:**\n",
    "\n",
    "1. **StandardScaler (standaryzacja)**  \n",
    "   - PrzeksztaÅ‚ca dane tak, by miaÅ‚y Å›redniÄ… 0 i odchylenie standardowe 1.  \n",
    "   - WzÃ³r dla kaÅ¼dej cechy:  \n",
    "     $$ x_{scaled} = \\frac{x - \\bar{x}}{\\sigma} $$\n",
    "   \n",
    "2. **MinMaxScaler**  \n",
    "   - PrzeksztaÅ‚ca dane do przedziaÅ‚u [0,1].  \n",
    "   - WzÃ³r:  \n",
    "     $$ x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}} $$\n",
    "\n",
    "### **WpÅ‚yw skalowania na modele**\n",
    "\n",
    "**Algorytmy wraÅ¼liwe na skalÄ™:**\n",
    "\n",
    "âœ… Regresja z regularyzacjÄ… (Ridge, Lasso)  \n",
    "âœ… SVM, KNN, K-means  \n",
    "âœ… Sieci neuronowe  \n",
    "âœ… PCA, LDA  \n",
    "\n",
    "**Algorytmy odporne na skalÄ™:**\n",
    "\n",
    "âŒ Drzewa decyzyjne  \n",
    "âŒ Random Forest  \n",
    "âŒ Naive Bayes  \n",
    "\n",
    "### **Zadanie **\n",
    "\n",
    "ZastanÃ³w siÄ™: dlaczego regularyzacja (Ridge/Lasso) jest wraÅ¼liwa na skalÄ™ cech? \n",
    "WskazÃ³wka: przypomnij sobie wzory na kary L1 i L2!\n",
    "\n",
    "![](https://kharshit.github.io/img/scaling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c4d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# MODEL Z PREPROCESSINGIEM - PIPELINE\n",
    "# =====================================\n",
    "\n",
    "print(\"ğŸ”§ TWORZENIE MODELU Z PREPROCESSINGIEM:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ¯ Pipeline: Skalowanie + Regresja Liniowa\")\n",
    "\n",
    "# Tworzenie pipeline\n",
    "lin_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),      # Krok 1: Skalowanie\n",
    "    ('linear_reg', LinearRegression()) # Krok 2: Regresja\n",
    "])\n",
    "\n",
    "print(\"âœ… Pipeline utworzony!\")\n",
    "print(\"\\nğŸ“Š Kroki w pipeline:\")\n",
    "print(\"1. 'scaler': StandardScaler (mean=0, std=1)\")\n",
    "print(\"2. 'linear_reg': LinearRegression\")\n",
    "\n",
    "# Trenowanie pipeline\n",
    "print(\"\\nğŸƒâ€â™‚ï¸ Trenowanie modelu...\")\n",
    "lin_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predykcje\n",
    "y_pred_scaled = lin_pipeline.predict(X_test)\n",
    "\n",
    "# Metryki\n",
    "mse_scaled = mean_squared_error(y_test, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "r2_scaled = r2_score(y_test, y_pred_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test, y_pred_scaled)\n",
    "\n",
    "print(\"ğŸ“Š WYNIKI MODELU Z SKALOWANIEM:\")\n",
    "print(f\"MSE:  {mse_scaled:.4f}\")\n",
    "print(f\"RMSE: ${rmse_scaled*100:.0f}k\")\n",
    "print(f\"RÂ²:   {r2_scaled:.4f} ({r2_scaled*100:.1f}%)\")\n",
    "print(f\"MAE:  ${mae_scaled*100:.0f}k\")\n",
    "\n",
    "print(f\"\\nğŸ” PORÃ“WNANIE Z MODELEM BAZOWYM:\")\n",
    "print(f\"RÂ² improvement: {r2_scaled - r2_basic:+.4f}\")\n",
    "print(f\"RMSE change: ${(rmse_scaled - rmse_basic)*100:+.0f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950339b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š PorÃ³wnanie modeli\n",
    "print(\"ğŸ“Š PORÃ“WNANIE MODELI:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['RÂ²', 'RMSE ($k)', 'MAE ($k)'],\n",
    "    'Bez skalowania': [r2_basic, rmse_basic*100, mae_basic*100],\n",
    "    'Ze skalowaniem': [r2_scaled, rmse_scaled*100, mae_scaled*100]\n",
    "}).round(3)\n",
    "\n",
    "display(comparison_df)\n",
    "\n",
    "# Wizualizacja porÃ³wnania\n",
    "plt.figure(figsize=(10, 6))\n",
    "metrics = ['RÂ²', 'RMSE', 'MAE']\n",
    "basic_values = [r2_basic, rmse_basic, mae_basic]\n",
    "scaled_values = [r2_scaled, rmse_scaled, mae_scaled]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, basic_values, width, label='Bez skalowania', alpha=0.8)\n",
    "plt.bar(x + width/2, scaled_values, width, label='Ze skalowaniem', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Metryki')\n",
    "plt.ylabel('WartoÅ›Ä‡')\n",
    "plt.title('PorÃ³wnanie modeli: z skalowaniem vs bez skalowania')\n",
    "plt.xticks(x, metrics)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ WNIOSEK:\")\n",
    "if abs(r2_scaled - r2_basic) < 0.001:\n",
    "    print(\"Skalowanie nie wpÅ‚ynÄ™Å‚o znaczÄ…co na Linear Regression\")\n",
    "    print(\"(To normalne - Linear Regression jest maÅ‚o wraÅ¼liwa na skalÄ™)\")\n",
    "    print(\"ğŸ”§ Skalowanie bÄ™dzie waÅ¼ne dla regularyzacji!\")\n",
    "else:\n",
    "    print(f\"Skalowanie {'poprawiÅ‚o' if r2_scaled > r2_basic else 'pogorszyÅ‚o'} wyniki\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52087d",
   "metadata": {},
   "source": [
    "## **Regularyzacja - Ridge i Lasso Regression**\n",
    "\n",
    "Teraz przetestujemy modele z regularyzacjÄ…. Regularyzacja jest szczegÃ³lnie przydatna gdy:\n",
    "\n",
    "1. **Mamy wiele cech** (zapobiega overfitting)\n",
    "2. **Cechy sÄ… skorelowane** (Ridge stabilizuje wspÃ³Å‚czynniki)\n",
    "3. **Podejrzewamy nieistotne cechy** (Lasso moÅ¼e je wyeliminowaÄ‡)\n",
    "4. **Chcemy prostszy model** (Lasso redukuje liczbÄ™ cech)\n",
    "\n",
    "### **Parametr Î± (alpha):**\n",
    "- **Î± = 0**: ZwykÅ‚a regresja liniowa\n",
    "- **Î± â†’ âˆ**: Wszystkie wspÃ³Å‚czynniki â†’ 0\n",
    "- **Optymalne Î±**: Znajdziemy przez cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae109191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# REGULARYZACJA - RIDGE I LASSO\n",
    "# =====================================\n",
    "\n",
    "print(\"âš–ï¸ REGULARYZACJA - RIDGE I LASSO REGRESSION:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¯ Cel: PorÃ³wnanie rÃ³Å¼nych wartoÅ›ci alpha\")\n",
    "\n",
    "# RÃ³Å¼ne wartoÅ›ci alpha do przetestowania\n",
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "print(f\"ğŸ”§ Testowane wartoÅ›ci alpha: {alphas}\")\n",
    "print(\"ğŸ“Š Dla kaÅ¼dego alpha trenujemy Ridge i Lasso\")\n",
    "\n",
    "# Przechowywanie wynikÃ³w\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(f\"\\nğŸ”„ Testowanie alpha = {alpha}...\")\n",
    "    \n",
    "    # Ridge Pipeline\n",
    "    ridge_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ridge', Ridge(alpha=alpha))\n",
    "    ])\n",
    "    ridge_pipeline.fit(X_train, y_train)\n",
    "    y_pred_ridge = ridge_pipeline.predict(X_test)\n",
    "    \n",
    "    # Lasso Pipeline\n",
    "    lasso_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lasso', Lasso(alpha=alpha, max_iter=10000))\n",
    "    ])\n",
    "    lasso_pipeline.fit(X_train, y_train)\n",
    "    y_pred_lasso = lasso_pipeline.predict(X_test)\n",
    "    \n",
    "    # Metryki\n",
    "    ridge_r2 = r2_score(y_test, y_pred_ridge)\n",
    "    lasso_r2 = r2_score(y_test, y_pred_lasso)\n",
    "    ridge_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "    lasso_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "    \n",
    "    results.append({\n",
    "        'alpha': alpha,\n",
    "        'Ridge_R2': ridge_r2,\n",
    "        'Lasso_R2': lasso_r2,\n",
    "        'Ridge_RMSE': ridge_rmse,\n",
    "        'Lasso_RMSE': lasso_rmse\n",
    "    })\n",
    "\n",
    "print(\"âœ… Testowanie zakoÅ„czone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Analiza wynikÃ³w regularyzacji\n",
    "print(\"ğŸ“Š WYNIKI REGULARYZACJI:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df.round(4))\n",
    "\n",
    "# ZnajdÅº najlepsze alpha\n",
    "best_ridge_alpha = results_df.loc[results_df['Ridge_R2'].idxmax(), 'alpha']\n",
    "best_lasso_alpha = results_df.loc[results_df['Lasso_R2'].idxmax(), 'alpha']\n",
    "\n",
    "print(f\"\\nğŸ† NAJLEPSZE WYNIKI:\")\n",
    "print(f\"Ridge - najlepsze Î±: {best_ridge_alpha} (RÂ² = {results_df['Ridge_R2'].max():.4f})\")\n",
    "print(f\"Lasso - najlepsze Î±: {best_lasso_alpha} (RÂ² = {results_df['Lasso_R2'].max():.4f})\")\n",
    "print(f\"Linear - RÂ²: {r2_scaled:.4f}\")\n",
    "\n",
    "# PorÃ³wnanie z modelem liniowym\n",
    "best_ridge_r2 = results_df['Ridge_R2'].max()\n",
    "best_lasso_r2 = results_df['Lasso_R2'].max()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ PORÃ“WNANIE Z LINEAR REGRESSION:\")\n",
    "print(f\"Ridge improvement: {best_ridge_r2 - r2_scaled:+.4f}\")\n",
    "print(f\"Lasso improvement: {best_lasso_r2 - r2_scaled:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fee5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ˆ Wizualizacja wpÅ‚ywu alpha\n",
    "print(\"ğŸ“ˆ WIZUALIZACJA WPÅYWU ALPHA:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# RÂ² vs alpha\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.semilogx(alphas, results_df['Ridge_R2'], 'o-', label='Ridge', linewidth=2, markersize=8)\n",
    "plt.semilogx(alphas, results_df['Lasso_R2'], 's-', label='Lasso', linewidth=2, markersize=8)\n",
    "plt.axhline(y=r2_scaled, color='red', linestyle='--', alpha=0.7, label='Linear Reg')\n",
    "plt.xlabel('Alpha (log scale)')\n",
    "plt.ylabel('RÂ² Score')\n",
    "plt.title('RÂ² vs Alpha')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE vs alpha\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogx(alphas, results_df['Ridge_RMSE'], 'o-', label='Ridge', linewidth=2, markersize=8)\n",
    "plt.semilogx(alphas, results_df['Lasso_RMSE'], 's-', label='Lasso', linewidth=2, markersize=8)\n",
    "plt.axhline(y=rmse_scaled, color='red', linestyle='--', alpha=0.7, label='Linear Reg')\n",
    "plt.xlabel('Alpha (log scale)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs Alpha')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ INTERPRETACJA WYKRESÃ“W:\")\n",
    "print(\"1. MaÅ‚e alpha â†’ bliskie regresji liniowej\")\n",
    "print(\"2. DuÅ¼e alpha â†’ wiÄ™cej regularyzacji\")\n",
    "print(\"3. Optimum to kompromis miÄ™dzy underfitting a overfitting\")\n",
    "print(\"4. Ridge i Lasso mogÄ… mieÄ‡ rÃ³Å¼ne optymalne alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4682425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Analiza wspÃ³Å‚czynnikÃ³w z regularyzacjÄ…\n",
    "print(\"ğŸ” ANALIZA WSPÃ“ÅCZYNNIKÃ“W Z REGULARYZACJÄ„:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¯ PorÃ³wnanie jak regularyzacja wpÅ‚ywa na wspÃ³Å‚czynniki\")\n",
    "\n",
    "# Trenujemy modele z najlepszymi alpha\n",
    "ridge_final = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge(alpha=best_ridge_alpha))\n",
    "])\n",
    "\n",
    "lasso_final = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lasso', Lasso(alpha=best_lasso_alpha, max_iter=10000))\n",
    "])\n",
    "\n",
    "ridge_final.fit(X_train, y_train)\n",
    "lasso_final.fit(X_train, y_train)\n",
    "\n",
    "# PorÃ³wnanie wspÃ³Å‚czynnikÃ³w\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Linear': lin_pipeline.named_steps['linear_reg'].coef_,\n",
    "    'Ridge': ridge_final.named_steps['ridge'].coef_,\n",
    "    'Lasso': lasso_final.named_steps['lasso'].coef_,\n",
    "    'Lasso_Zero': np.abs(lasso_final.named_steps['lasso'].coef_) < 1e-6\n",
    "})\n",
    "\n",
    "print(\"ğŸ“Š WSPÃ“ÅCZYNNIKI WSZYSTKICH MODELI:\")\n",
    "display(coef_comparison.round(4))\n",
    "\n",
    "# Analiza selekcji cech przez Lasso\n",
    "zero_coefs = coef_comparison['Lasso_Zero'].sum()\n",
    "total_coefs = len(coef_comparison)\n",
    "\n",
    "print(f\"\\nğŸ—‘ï¸ SELEKCJA CECH (LASSO Î±={best_lasso_alpha}):\")\n",
    "print(f\"WspÃ³Å‚czynniki wyzerowane: {zero_coefs}/{total_coefs} ({zero_coefs/total_coefs*100:.0f}%)\")\n",
    "\n",
    "if zero_coefs > 0:\n",
    "    removed_features = coef_comparison[coef_comparison['Lasso_Zero']]['Feature'].tolist()\n",
    "    print(\"\\nğŸ“‹ CECHY USUNIÄ˜TE PRZEZ LASSO:\")\n",
    "    for feature in removed_features:\n",
    "        print(f\"  - {feature}\")\n",
    "    \n",
    "    remaining_features = coef_comparison[~coef_comparison['Lasso_Zero']]['Feature'].tolist()\n",
    "    print(\"\\nâœ… CECHY ZACHOWANE PRZEZ LASSO:\")\n",
    "    for feature in remaining_features:\n",
    "        coef_val = coef_comparison[coef_comparison['Feature'] == feature]['Lasso'].iloc[0]\n",
    "        print(f\"  - {feature}: {coef_val:.4f}\")\n",
    "else:\n",
    "    print(\"âœ… LASSO zachowaÅ‚ wszystkie cechy (alpha moÅ¼e byÄ‡ za maÅ‚e)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e9736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Wizualizacja wspÃ³Å‚czynnikÃ³w\n",
    "print(\"ğŸ“Š WIZUALIZACJA WSPÃ“ÅCZYNNIKÃ“W:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. PorÃ³wnanie wspÃ³Å‚czynnikÃ³w\n",
    "ax1 = axes[0, 0]\n",
    "x_pos = np.arange(len(X.columns))\n",
    "width = 0.25\n",
    "\n",
    "ax1.bar(x_pos - width, coef_comparison['Linear'], width, label='Linear', alpha=0.8)\n",
    "ax1.bar(x_pos, coef_comparison['Ridge'], width, label='Ridge', alpha=0.8)\n",
    "ax1.bar(x_pos + width, coef_comparison['Lasso'], width, label='Lasso', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Cechy')\n",
    "ax1.set_ylabel('WartoÅ›Ä‡ wspÃ³Å‚czynnika')\n",
    "ax1.set_title('PorÃ³wnanie wspÃ³Å‚czynnikÃ³w')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(X.columns, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. WartoÅ›ci bezwzglÄ™dne\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(x_pos - width, np.abs(coef_comparison['Linear']), width, label='Linear', alpha=0.8)\n",
    "ax2.bar(x_pos, np.abs(coef_comparison['Ridge']), width, label='Ridge', alpha=0.8)\n",
    "ax2.bar(x_pos + width, np.abs(coef_comparison['Lasso']), width, label='Lasso', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Cechy')\n",
    "ax2.set_ylabel('|WspÃ³Å‚czynnik|')\n",
    "ax2.set_title('WartoÅ›ci bezwzglÄ™dne wspÃ³Å‚czynnikÃ³w')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(X.columns, rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Ridge vs Lasso scatter\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(coef_comparison['Ridge'], coef_comparison['Lasso'], alpha=0.7, s=80)\n",
    "for i, feature in enumerate(X.columns):\n",
    "    ax3.annotate(feature, \n",
    "                (coef_comparison['Ridge'].iloc[i], coef_comparison['Lasso'].iloc[i]),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# Linia x=y\n",
    "min_val = min(coef_comparison['Ridge'].min(), coef_comparison['Lasso'].min())\n",
    "max_val = max(coef_comparison['Ridge'].max(), coef_comparison['Lasso'].max())\n",
    "ax3.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5)\n",
    "\n",
    "ax3.set_xlabel('Ridge wspÃ³Å‚czynniki')\n",
    "ax3.set_ylabel('Lasso wspÃ³Å‚czynniki')\n",
    "ax3.set_title('Ridge vs Lasso')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. PorÃ³wnanie metryk\n",
    "ax4 = axes[1, 1]\n",
    "models = ['Linear', 'Ridge\\n(Î±=' + str(best_ridge_alpha) + ')', 'Lasso\\n(Î±=' + str(best_lasso_alpha) + ')']\n",
    "r2_scores = [r2_scaled, best_ridge_r2, best_lasso_r2]\n",
    "\n",
    "bars = ax4.bar(models, r2_scores, color=['skyblue', 'lightgreen', 'lightcoral'], alpha=0.8)\n",
    "ax4.set_ylabel('RÂ² Score')\n",
    "ax4.set_title('PorÃ³wnanie jakoÅ›ci modeli')\n",
    "ax4.set_ylim(0, 1)\n",
    "\n",
    "# Dodanie wartoÅ›ci na sÅ‚upkach\n",
    "for bar, score in zip(bars, r2_scores):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ WNIOSKI Z WIZUALIZACJI:\")\n",
    "print(\"1. Ridge zmniejsza wszystkie wspÃ³Å‚czynniki proporcjonalnie\")\n",
    "print(\"2. Lasso moÅ¼e eliminowaÄ‡ cechy (zerowe wspÃ³Å‚czynniki)\")\n",
    "print(\"3. Regularyzacja stabilizuje model i zapobiega overfittingu\")\n",
    "print(\"4. WybÃ³r miÄ™dzy Ridge/Lasso zaleÅ¼y od problemu i danych\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0658e0b",
   "metadata": {},
   "source": [
    "## **Zadania Praktyczne - Samodzielna Praca**\n",
    "\n",
    "### **Zadanie 1: Eksperyment z cechami geograficznymi**\n",
    "California Housing zawiera wspÃ³Å‚rzÄ™dne geograficzne (Latitude, Longitude). SprawdÅº:\n",
    "1. Jak usuniÄ™cie lub dodanie tych cech wpÅ‚ywa na model?\n",
    "2. Czy moÅ¼na stworzyÄ‡ nowÄ… cechÄ™ \"distance from center\" (np. odlegÅ‚oÅ›Ä‡ od San Francisco)?\n",
    "3. PorÃ³wnaj wyniki z obecnym modelem\n",
    "\n",
    "### **Zadanie 2: Analiza reszt (residuals)**\n",
    "Dla najlepszego modelu:\n",
    "1. Oblicz reszty: `residuals = y_true - y_pred`\n",
    "2. SprawdÅº czy reszty majÄ… rozkÅ‚ad normalny (histogram + QQ-plot)\n",
    "3. Narysuj reszty vs przewidywane wartoÅ›ci\n",
    "4. Zidentyfikuj outliers - ktÃ³re domy sÄ… najgorzej przewidziane?\n",
    "\n",
    "### **Zadanie 3: Feature Engineering**\n",
    "StwÃ³rz nowe cechy:\n",
    "1. **Rooms per household**: `AveRooms / AveOccup`\n",
    "2. **Income per room**: `MedInc / AveRooms`  \n",
    "3. **Population density**: `Population / (HouseAge + 1)`\n",
    "4. Przetestuj czy nowe cechy poprawiajÄ… RÂ²\n",
    "\n",
    "### **Zadanie 4: Cross-Validation i Grid Search**\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# ZnajdÅº optymalne alpha uÅ¼ywajÄ…c Grid Search\n",
    "param_grid = {'lasso__alpha': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid_search = GridSearchCV(lasso_pipeline, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### **Zadanie 5: Model Interpretability**\n",
    "1. Ranking cech - ktÃ³re sÄ… najwaÅ¼niejsze dla przewidywania ceny?\n",
    "2. Analiza business impact - co oznaczajÄ… wspÃ³Å‚czynniki w praktyce?\n",
    "3. Jak zmiana kaÅ¼dej cechy o 1 jednostkÄ™ wpÅ‚ywa na cenÄ™ domu?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ffbee",
   "metadata": {},
   "source": [
    "## **Podsumowanie i Najlepsze Praktyki**\n",
    "\n",
    "### **ğŸ† Co osiÄ…gnÄ™liÅ›my:**\n",
    "\n",
    "1. **ğŸ“Š Eksploracja danych**: PoznaliÅ›my California Housing dataset\n",
    "2. **ğŸ”§ Preprocessing**: NauczyliÅ›my siÄ™ skalowania i pipeline'Ã³w\n",
    "3. **ğŸ“ˆ Modelowanie**: ZaimplementowaliÅ›my Linear, Ridge, i Lasso regression\n",
    "4. **âš–ï¸ Regularyzacja**: ZrozumieliÅ›my wpÅ‚yw parametru alpha\n",
    "5. **ğŸ” Interpretacja**: PrzeanalizowaliÅ›my wspÃ³Å‚czynniki i ich znaczenie\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ’¡ Kluczowe wnioski:**\n",
    "\n",
    "**O California Housing:**\n",
    "- **MedInc** (dochÃ³d) to najsilniejszy predyktor ceny\n",
    "- Model wyjaÅ›nia ~60% zmiennoÅ›ci cen domÃ³w\n",
    "- Regularyzacja nie poprawiÅ‚a znaczÄ…co wynikÃ³w (dane sÄ… juÅ¼ dobrze przygotowane)\n",
    "\n",
    "**O regresji liniowej:**\n",
    "- âœ… Prosta i interpretowalana\n",
    "- âœ… Dobra jako model bazowy  \n",
    "- âš ï¸ ZakÅ‚ada liniowe zaleÅ¼noÅ›ci\n",
    "- âš ï¸ Wymaga odpowiedniego preprocessingu\n",
    "\n",
    "**O regularyzacji:**\n",
    "- **Ridge**: Stabilizuje wspÃ³Å‚czynniki, dobry dla skorelowanych cech\n",
    "- **Lasso**: Automatyczna selekcja cech, tworzy rzadkie modele\n",
    "- **Alpha**: Kontroluje siÅ‚Ä™ regularyzacji (wiÄ™ksze = wiÄ™cej ograniczeÅ„)\n",
    "\n",
    "### **ğŸ”§ Najlepsze praktyki:**\n",
    "\n",
    "1. **Zawsze skaluj dane** przy regularyzacji\n",
    "2. **UÅ¼ywaj Pipeline** dla czystego kodu\n",
    "3. **Dziel dane** na train/test przed preprocessingiem\n",
    "4. **Waliduj cross-validation** dla lepszej oceny\n",
    "5. **Interpretuj wspÃ³Å‚czynniki** w kontekÅ›cie biznesowym\n",
    "6. **Sprawdzaj zaÅ‚oÅ¼enia** regresji liniowej\n",
    "\n",
    "### **ğŸš€ Co dalej:**\n",
    "\n",
    "1. **Polynomial Features** - nieliniowe zaleÅ¼noÅ›ci\n",
    "2. **Ensemble Methods** - Random Forest, Gradient Boosting\n",
    "3. **Feature Selection** - automatyczna selekcja cech\n",
    "4. **Hyperparameter Tuning** - Grid Search, Random Search\n",
    "5. **Advanced Validation** - nested CV, time series splits\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ PamiÄ™taj**: \n",
    "> \"All models are wrong, but some are useful\" - George Box\n",
    "\n",
    "Regresja liniowa moÅ¼e nie byÄ‡ idealna, ale jest **uÅ¼yteczna**, **interpretowalana** i stanowi **solidnÄ… podstawÄ™** do nauki bardziej zaawansowanych technik ML.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c8291",
   "metadata": {},
   "source": [
    "## **ğŸ“š Dalsze MateriaÅ‚y i Zasoby**\n",
    "\n",
    "### **ğŸ”— Przydatne linki:**\n",
    "\n",
    "**Dokumentacja:**\n",
    "- [Scikit-learn Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html)\n",
    "\n",
    "**Kursy online:**\n",
    "- [Andrew Ng - Machine Learning Course](https://www.coursera.org/learn/machine-learning)\n",
    "- [Fast.ai - Practical Deep Learning](https://www.fast.ai/)\n",
    "- [Elements of Statistical Learning (ksiÄ…Å¼ka)](https://hastie.su.domains/ElemStatLearn/)\n",
    "\n",
    "**Praktyczne zasoby:**\n",
    "- [Kaggle Learn](https://www.kaggle.com/learn) - darmowe kursy ML\n",
    "- [Google Colab](https://colab.research.google.com/) - darmowe GPU do eksperymentÃ³w\n",
    "- [Papers with Code](https://paperswithcode.com/) - najnowsze badania\n",
    "\n",
    "### **ğŸ“ˆ Datasety do Ä‡wiczeÅ„:**\n",
    "\n",
    "1. **Boston Housing** - klasyczny dataset regresji\n",
    "2. **Wine Quality** - regression + classification  \n",
    "3. **Bike Sharing** - analiza szeregÃ³w czasowych\n",
    "4. **Student Performance** - edukacyjny dataset\n",
    "5. **Real Estate** - ceny nieruchomoÅ›ci\n",
    "\n",
    "### **ğŸ”§ NarzÄ™dzia do zaawansowanej analizy:**\n",
    "\n",
    "- **Seaborn** - zaawansowane wizualizacje\n",
    "- **Plotly** - interaktywne wykresy\n",
    "- **SHAP** - interpretowalny ML\n",
    "- **MLflow** - tracking eksperymentÃ³w  \n",
    "- **Streamlit** - szybkie web apps\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸš€ Powodzenia w dalszej nauce Machine Learning!**\n",
    "\n",
    "PamiÄ™taj: najlepszy sposÃ³b nauki to praktyka. Eksperymentuj z rÃ³Å¼nymi algorytmami, dataset'ami i podejÅ›ciami. KaÅ¼dy projekt uczy czegoÅ› nowego!\n",
    "\n",
    "**Happy Coding! ğŸğŸ’»**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
