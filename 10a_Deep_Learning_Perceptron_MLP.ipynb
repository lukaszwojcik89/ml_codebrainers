{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1154e1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Wprowadzenie do g≈Çƒôbokiego uczenia (Deep Learning), perceptronu, MLP\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ôªø![](https://datascientest.com/en/files/2021/01/Machine-learning-def-.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1e9d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "W tych zajƒôciach wprowadzimy pojƒôcia zwiƒÖzane z g≈Çƒôbokim uczeniem, poczƒÖwszy od pojedynczego perceptronu, a≈º po wielowarstwowe sieci neuronowe typu MLP (Multi-Layer Perceptron). Poznamy podstawy sieci neuronowych, rolƒô funkcji aktywacji, ideƒô wielowarstwowych architektur, a tak≈ºe koncepcjƒô gradientu i wstecznej propagacji b≈Çƒôd√≥w (backpropagation) ‚Äì oczywi≈õcie bez wchodzenia w skomplikowane r√≥wnania matematyczne.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc86da4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Czym jest Deep Learning?**\n",
    "\n",
    "- Deep Learning to poddziedzina uczenia maszynowego, kt√≥ra osiƒÖgnƒô≈Ça w ostatnich latach spektakularne wyniki w wielu dziedzinach, takich jak rozpoznawanie obraz√≥w, t≈Çumaczenie maszynowe, analiza tekstu czy sterowanie grami.\n",
    "- G≈Çƒôbokie sieci neuronowe sk≈ÇadajƒÖ siƒô z wielu warstw przetwarzajƒÖcych dane, co pozwala im hierarchicznie wyodrƒôbniaƒá wzorce o rosnƒÖcej z≈Ço≈ºono≈õci.\n",
    "- W przeciwie≈Ñstwie do tradycyjnego uczenia maszynowego, deep learning czƒôsto eliminuje konieczno≈õƒá rƒôcznego tworzenia cech (feature engineering), pozwalajƒÖc modelom \"nauczyƒá siƒô\" optymalnych reprezentacji bezpo≈õrednio z danych.\n",
    "- Postƒôpy w deep learningu doprowadzi≈Çy do powstania modeli, kt√≥re dor√≥wnujƒÖ lub przewy≈ºszajƒÖ mo≈ºliwo≈õci ludzkich ekspert√≥w w wielu z≈Ço≈ºonych zadaniach.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e4233",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Perceptron i jednostka liniowa (The Linear Unit)**\n",
    "\n",
    "### Podstawy perceptronu\n",
    "Perceptron to najprostszy model neuronu w sztucznej sieci neuronowej. Przyjmuje pewne wej≈õcia, mno≈ºy je przez odpowiednie wagi, dodaje obciƒÖ≈ºenie (bias) i wylicza sumƒô liniowƒÖ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb211a",
   "metadata": {},
   "source": [
    "\n",
    "**Jednostka liniowa: ùë¶ = ùë§ùë• + ùëè**\n",
    "\n",
    "- Wej≈õcie: x  \n",
    "- Waga: w  \n",
    "- ObciƒÖ≈ºenie (bias): b  \n",
    "- Wyj≈õcie: y = w*x + b\n",
    "\n",
    "W ten spos√≥b pojedynczy perceptron jest modelem liniowym. Mo≈ºe reprezentowaƒá proste zale≈ºno≈õci, takie jak r√≥wnanie prostej na wykresie.\n",
    "\n",
    "![Untitled%207f31d7e7baa34ad182834df4a6f789de/Untitled.png](https://i.ibb.co/jGQrQDX/Untitled.png)\n",
    "\n",
    "W tym przyk≈Çadzie widzimy pojedynczy neuron z jednym wej≈õciem x. Waga w oraz bias b pozwalajƒÖ regulowaƒá nachylenie i po≈Ço≈ºenie prostej.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5c407",
   "metadata": {},
   "source": [
    "\n",
    "## **Przyk≈Çad: jednostka liniowa jako model**\n",
    "\n",
    "Rozwa≈ºmy zbi√≥r danych dotyczƒÖcy p≈Çatk√≥w ≈õniadaniowych (80 Cereals). Je≈õli nasz model mia≈Çby przewidywaƒá kaloryczno≈õƒá na podstawie zawarto≈õci cukru, uzyskamy r√≥wnanie w stylu:\n",
    "\n",
    "kalorie = w*(cukier) + b\n",
    "\n",
    "Je≈ºeli w = 2,5 a b = 90, to dla 5 gram√≥w cukru model da kalorie = 2,5*5 + 90 = 102,5. To nadal model liniowy. DodajƒÖc inne cechy, np. b≈Çonnik czy bia≈Çko, neuron bƒôdzie mia≈Ç wiƒôcej wag, ale nadal pozostanie liniowy.\n",
    "\n",
    "![Untitled](https://i.ibb.co/xhC1X7k/Untitled-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60790ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mfr</th>\n",
       "      <th>type</th>\n",
       "      <th>calories</th>\n",
       "      <th>protein</th>\n",
       "      <th>fat</th>\n",
       "      <th>sodium</th>\n",
       "      <th>fiber</th>\n",
       "      <th>carbo</th>\n",
       "      <th>sugars</th>\n",
       "      <th>potass</th>\n",
       "      <th>vitamins</th>\n",
       "      <th>shelf</th>\n",
       "      <th>weight</th>\n",
       "      <th>cups</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100% Bran</td>\n",
       "      <td>N</td>\n",
       "      <td>C</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>280</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>68.402973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100% Natural Bran</td>\n",
       "      <td>Q</td>\n",
       "      <td>C</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>33.983679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All-Bran</td>\n",
       "      <td>K</td>\n",
       "      <td>C</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>320</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>59.425505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All-Bran with Extra Fiber</td>\n",
       "      <td>K</td>\n",
       "      <td>C</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>330</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>93.704912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Almond Delight</td>\n",
       "      <td>R</td>\n",
       "      <td>C</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>34.384843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name mfr type  calories  protein  fat  sodium  fiber  \\\n",
       "0                  100% Bran   N    C        70        4    1     130   10.0   \n",
       "1          100% Natural Bran   Q    C       120        3    5      15    2.0   \n",
       "2                   All-Bran   K    C        70        4    1     260    9.0   \n",
       "3  All-Bran with Extra Fiber   K    C        50        4    0     140   14.0   \n",
       "4             Almond Delight   R    C       110        2    2     200    1.0   \n",
       "\n",
       "   carbo  sugars  potass  vitamins  shelf  weight  cups     rating  \n",
       "0    5.0       6     280        25      3     1.0  0.33  68.402973  \n",
       "1    8.0       8     135         0      3     1.0  1.00  33.983679  \n",
       "2    7.0       5     320        25      3     1.0  0.33  59.425505  \n",
       "3    8.0       0     330        25      3     1.0  0.50  93.704912  \n",
       "4   14.0       8      -1        25      3     1.0  0.75  34.384843  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sugars</th>\n",
       "      <th>calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sugars  calories\n",
       "0       6        70\n",
       "1       8       120\n",
       "2       5        70\n",
       "3       0        50\n",
       "4       8       110"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cereal = pd.read_csv('data/cereal.csv')\n",
    "\n",
    "\n",
    "display(cereal.head(),cereal[['sugars','calories' ]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38abf09",
   "metadata": {},
   "source": [
    "### **Jednostki liniowe w Kerasie**\n",
    "\n",
    "Najprostszym sposobem na stworzenie modelu w Kerasie jest u≈ºycie funkcji **`keras.Sequential`**, kt√≥ra tworzy sieƒá neuronowƒÖ jako stos warstw. Mo≈ºemy tworzyƒá modele podobne do tych pokazanych powy≈ºej, u≈ºywajƒÖc gƒôstej warstwy (o kt√≥rej dowiemy siƒô wiƒôcej w nastƒôpnej lekcji).\n",
    "\n",
    "Mo≈ºemy zdefiniowaƒá liniowy model, kt√≥ry przyjmuje trzy cechy wej≈õciowe (\"cukry\", \"b≈Çonnik\" i \"bia≈Çko\") i generuje pojedyncze wyj≈õcie (\"kalorie\"), w nastƒôpujƒÖcy spos√≥b:\n",
    "\n",
    "```python\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Utw√≥rz sieƒá z 1 jednostkƒÖ liniowƒÖ\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units=1, input_shape=[3])\n",
    "])\n",
    "```\n",
    "\n",
    "Przy pomocy pierwszego argumentu, **`units`**, okre≈õlamy, jakie wyj≈õcie chcemy otrzymaƒá. W tym przypadku wybieramy tylko \"kalorie\" i u≈ºywamy **`units=1`**.\n",
    "\n",
    "W drugim argumencie, **`input_shape`**, informujemy Kerasa o wymiarach wej≈õƒá. Ustawienie **`input_shape=[3]`** oznacza, ≈ºe model bƒôdzie przyjmowa≈Ç trzy cechy jako wej≈õcie (\"cukry\", \"b≈Çonnik\" i \"bia≈Çko\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434163cd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Dlaczego **`input_shape`** jest listƒÖ w Pythonie?\n",
    "\n",
    "Powy≈ºsze cechy sƒÖ u≈Ço≈ºone w kolumnach, wiƒôc zawsze bƒôdziemy mieli **`input_shape=[liczba_kolumn]`**. Keras u≈ºywa listy tutaj, aby umo≈ºliwiƒá u≈ºycie bardziej z≈Ço≈ºonych zestaw√≥w danych. Dane obrazowe, na przyk≈Çad, mogƒÖ wymagaƒá trzech wymiar√≥w: **`[wysoko≈õƒá, szeroko≈õƒá, kana≈Çy]`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24cddc",
   "metadata": {},
   "source": [
    "Zestaw danych 80 Cereals charakteryzuje siƒô znacznie wiƒôkszƒÖ liczbƒÖ cech ni≈º tylko \"cukry\". Je≈õli zdecydujemy siƒô na rozwiniƒôcie modelu o dodatkowe cechy, takie jak zawarto≈õƒá b≈Çonnika lub bia≈Çka, bƒôdzie to proste do zrealizowania. Wystarczy dodaƒá wiƒôcej po≈ÇƒÖcze≈Ñ wej≈õciowych do neuronu, jeden dla ka≈ºdej dodatkowej cechy. Aby uzyskaƒá wynik, pomno≈ºymy ka≈ºde wej≈õcie przez wagƒô jego po≈ÇƒÖczenia, a nastƒôpnie dodamy wszystkie wyniki razem, co jest bardzo proste.\n",
    "\n",
    "Mamy trzy po≈ÇƒÖczenia wej≈õciowe: x0, x1 i x2, wraz z obciƒÖ≈ºeniem.\n",
    "\n",
    "![Untitled](https://i.ibb.co/YT5ZywK/Untitled-2.png)\n",
    "\n",
    "R√≥wnanie dla tego neuronu to ùë¶ = ùë§0ùë•0 + ùë§1ùë•1 + ùë§2ùë•2 + ùëè. Jednostka liniowa z dwoma wej≈õciami pasuje do p≈Çaszczyzny, a jednostka z wiƒôcej wej≈õciami ni≈º to bƒôdzie pasowaƒá do hiperp≈Çaszczyzny."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81eb3d",
   "metadata": {},
   "source": [
    "\n",
    "### Wiele wej≈õƒá\n",
    "\n",
    "Je≈õli chcemy wykorzystaƒá wiƒôcej cech wej≈õciowych (x0, x1, x2 ...), dodajemy wiƒôcej wag i sumujemy ich iloczyny z odpowiednimi wej≈õciami. Dla dw√≥ch wej≈õƒá mieliby≈õmy: y = w0*x0 + w1*x1 + b, co reprezentuje r√≥wnanie p≈Çaszczyzny.\n",
    "\n",
    "Przy wiƒôkszej liczbie wej≈õƒá powstaje hiperp≈Çaszczyzna w wielowymiarowej przestrzeni cech.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d30dc",
   "metadata": {},
   "source": [
    "\n",
    "## **Przeskok do wielu warstw ‚Äì MLP**\n",
    "\n",
    "### Idea MLP\n",
    "\n",
    "MLP (Multi-Layer Perceptron) to sieƒá neuronowa z≈Ço≈ºona z wielu warstw:\n",
    "\n",
    "- Warstwa wej≈õciowa (input)\n",
    "- Jedna lub wiƒôcej warstw ukrytych (hidden layers)\n",
    "- Warstwa wyj≈õciowa (output)\n",
    "Ka≈ºda warstwa sk≈Çada siƒô z wielu neuron√≥w (jednostek liniowych), a pomiƒôdzy warstwami wprowadzamy nieliniowo≈õƒá poprzez funkcje aktywacji.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d36d2e",
   "metadata": {},
   "source": [
    "Sieci neuronowe zazwyczaj organizujƒÖ swoje neurony w warstwy. Kiedy zbieramy razem jednostki liniowe, kt√≥re majƒÖ wsp√≥lny zbi√≥r wej≈õƒá, otrzymujemy gƒôstƒÖ warstwƒô.\n",
    "\n",
    "![Untitled](https://i.ibb.co/3TJjL14/Untitled-3.png)\n",
    "\n",
    "Mo≈ºna rozwa≈ºaƒá ka≈ºdƒÖ warstwƒô w sieci neuronowej jako przeprowadzajƒÖcƒÖ stosunkowo prostƒÖ transformacjƒô. Dziƒôki stosowaniu wielu warstw, sieƒá neuronowa mo≈ºe przetwarzaƒá swoje wej≈õcie w coraz bardziej z≈Ço≈ºonych sposobach. W dobrze wytrenowanej sieci neuronowej ka≈ºda warstwa to transformacja, kt√≥ra przybli≈ºa nas trochƒô bardziej do rozwiƒÖzania."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a64803",
   "metadata": {},
   "source": [
    "### Warstwy ukryte i reprezentacje wewnƒôtrzne\n",
    "\n",
    "Gdy ≈ÇƒÖczymy wiele warstw, pierwsze warstwy wyodrƒôbniajƒÖ proste wzorce, a kolejne, bardziej ukryte warstwy ≈ÇƒÖczƒÖ je w coraz bardziej z≈Ço≈ºone reprezentacje. W efekcie MLP potrafi modelowaƒá nieliniowe i skomplikowane zale≈ºno≈õci.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e1e4f",
   "metadata": {},
   "source": [
    "## **Funkcje aktywacji**\n",
    "\n",
    "Bez funkcji aktywacji dwie warstwy liniowe sk≈ÇadajƒÖ siƒô nadal na model liniowy. Funkcja aktywacji wprowadza nieliniowo≈õƒá, np. ReLU (Rectified Linear Unit):\n",
    "\n",
    "ReLU: max(0, x)\n",
    "\n",
    "Funkcja ReLU \"prostuje\" ujemne warto≈õci do zera, a dla warto≈õci dodatnich ro≈õnie liniowo. Dziƒôki temu sieƒá mo≈ºe dopasowywaƒá skomplikowane, krzywoliniowe zale≈ºno≈õci.\n",
    "\n",
    "![Untitled](https://i.ibb.co/mF55Qcd/Untitled-5.png)\n",
    "\n",
    "W praktyce stosuje siƒô te≈º inne funkcje aktywacji, np. sigmoid, tanh czy softmax (dla zada≈Ñ klasyfikacji).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a83c24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Po≈ÇƒÖczenie jednostki liniowej z prostownikiem daje jednostkƒô liniowƒÖ prostowanƒÖ (ReLU). Z tego powodu funkcjƒô prostownikowƒÖ czƒôsto nazywa siƒô funkcjƒÖ ReLU. Zastosowanie aktywacji ReLU do jednostki liniowej oznacza, ≈ºe wynik jest r√≥wny max(0, w * x + b)\n",
    "\n",
    "![Untitled](https://i.ibb.co/DM5x0q9/Untitled-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9801ff",
   "metadata": {},
   "source": [
    "## **Architektura MLP**\n",
    "\n",
    "MLP to sieƒá w pe≈Çni po≈ÇƒÖczonych warstw (dense layers). Ka≈ºda warstwa to zbi√≥r neuron√≥w, a ka≈ºdy neuron w warstwie jest po≈ÇƒÖczony ze wszystkimi neuronami warstwy poprzedniej.\n",
    "\n",
    "![Untitled](https://i.ibb.co/C0MFmx6/Untitled-7.png)\n",
    "\n",
    "Warstwy ukryte zawierajƒÖ funkcje aktywacji, warstwa wyj≈õciowa mo≈ºe byƒá liniowa (regresja) lub mieƒá innƒÖ aktywacjƒô (np. softmax dla klasyfikacji wieloklasowej).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffcdc0c",
   "metadata": {},
   "source": [
    "\n",
    "## Budowanie modeli sekwencyjnych\n",
    "\n",
    "Model sekwencyjny, kt√≥ry u≈ºywali≈õmy, ≈ÇƒÖczy ze sobƒÖ listƒô warstw w kolejno≈õci od pierwszej do ostatniej. Pierwsza warstwa otrzymuje wej≈õcie, a ostatnia warstwa produkuje wyj≈õcie. Powstaje w ten spos√≥b model przedstawiony na powy≈ºszym diagramie.\n",
    "\n",
    "```\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
    "    layers.Dense(units=3, activation='relu'),\n",
    "    # the linear output layer\n",
    "    layers.Dense(units=1),\n",
    "])\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3398b",
   "metadata": {},
   "source": [
    "## **Koncepcja gradientu i backpropagation**\n",
    "\n",
    "### Gradient i minimalizacja straty\n",
    "\n",
    "- Sieƒá neuronowa jest trenowana poprzez minimalizacjƒô funkcji straty, kt√≥ra mierzy r√≥≈ºnicƒô miƒôdzy predykcjami a warto≈õciami docelowymi.\n",
    "- Gradient funkcji straty wzglƒôdem wag m√≥wi nam, w jakim kierunku powinni≈õmy zmieniaƒá wagi, aby zmniejszyƒá b≈ÇƒÖd.\n",
    "- Spadek gradientu (Gradient Descent) polega na iteracyjnym dostosowywaniu wag w kierunku przeciwnym do gradientu, co prowadzi do minimalizacji straty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf5591",
   "metadata": {},
   "source": [
    "## **Optymalizator - Stochastyczny spadek gradientu**\n",
    "\n",
    "W uczeniu maszynowym do optymalizacji wag sieci u≈ºywa siƒô algorytm√≥w nazywanych optymalizatorami. Najczƒô≈õciej stosowanƒÖ rodzinƒÖ algorytm√≥w optymalizacji jest Stochastyczny Spadek Gradientu (SGD).\n",
    "\n",
    "SGD to iteracyjny algorytm, kt√≥ry trenuje sieƒá w krokach. W ka≈ºdym kroku:\n",
    "\n",
    "- Wybierana jest pr√≥bka danych treningowych, przez kt√≥rƒÖ sieƒá przetwarza predykcjƒô.\n",
    "- Obliczana jest r√≥≈ºnica miƒôdzy predykcjƒÖ a prawdziwymi warto≈õciami.\n",
    "- Wagi sieci sƒÖ dostosowywane, aby zminimalizowaƒá stratƒô.\n",
    "\n",
    "Proces ten jest powtarzany, a≈º warto≈õƒá straty bƒôdzie wystarczajƒÖco ma≈Ça (lub do momentu, gdy przestanie siƒô dalej zmniejszaƒá).\n",
    "\n",
    "[https://i.imgur.com/rFI1tIk.mp4](https://i.imgur.com/rFI1tIk.mp4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc0457d",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "Backpropagation to algorytm, kt√≥ry oblicza gradienty dla wszystkich wag w sieci poprzez propagacjƒô b≈Çƒôdu od warstwy wyj≈õciowej do warstw wej≈õciowych.\n",
    "\n",
    "Choƒá matematycznie z≈Ço≈ºony, koncepcyjnie dzia≈Ça tak:\n",
    "- Oblicz stratƒô na wyj≈õciu sieci (r√≥≈ºnicƒô miƒôdzy przewidywanƒÖ warto≈õciƒÖ a warto≈õciƒÖ rzeczywistƒÖ).\n",
    "- Roz≈Ç√≥≈º ten b≈ÇƒÖd warstwa po warstwie, obliczajƒÖc wp≈Çyw ka≈ºdej wagi na ko≈ÑcowƒÖ stratƒô.\n",
    "- Na podstawie tego wp≈Çywu (gradientu) dostosuj wagi, aby w kolejnej iteracji wynik by≈Ç bli≈ºszy oczekiwanemu.\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3586254a",
   "metadata": {},
   "source": [
    "## **Funkcje Straty i Optymalizacja**\n",
    "\n",
    "Aby trenowaƒá sieƒá, potrzebujemy:\n",
    "- Funkcji straty (loss function), np. MSE (Mean Squared Error) dla regresji, lub Cross-Entropy dla klasyfikacji.\n",
    "- Optymalizatora, np. Adam, RMSProp czy klasyczny Stochastic Gradient Descent, kt√≥ry na podstawie obliczonych gradient√≥w aktualizuje wagi w kierunku minimalizacji straty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb8244",
   "metadata": {},
   "source": [
    "\n",
    "PowszechnƒÖ funkcjƒÖ straty dla problem√≥w regresji jest ≈õredni b≈ÇƒÖd bezwzglƒôdny lub MAE. Dla ka≈ºdej predykcji **`y_pred`**, MAE mierzy r√≥≈ºnicƒô miƒôdzy prawdziwƒÖ warto≈õciƒÖ docelowƒÖ **`y_true`** a przewidywanƒÖ przez model warto≈õciƒÖ **`abs(y_true - y_pred)`**.\n",
    "\n",
    "≈ÅƒÖczna funkcja straty MAE na zbiorze danych to ≈õrednia z tych bezwzglƒôdnych r√≥≈ºnic.\n",
    "\n",
    "Opr√≥cz MAE, inne funkcje straty, kt√≥re mo≈ºesz spotkaƒá dla problem√≥w regresji, to ≈õredni b≈ÇƒÖd kwadratowy (MSE) lub straty Hubera (obie dostƒôpne w Keras).\n",
    "\n",
    "![VDcvkZN.png](https://i.ibb.co/9WCzNXk/VDcvkZN.png)\n",
    "\n",
    "Podczas treningu modelu funkcja straty jest wykorzystywana jako wskaz√≥wka do znalezienia poprawnych warto≈õci wag (ni≈ºsza warto≈õƒá straty jest lepsza). Innymi s≈Çowy, funkcja straty m√≥wi sieci, jaki jest jej cel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d13cc7",
   "metadata": {},
   "source": [
    "\n",
    "## **Dodawanie funkcji straty i optymalizatora**\n",
    "\n",
    "Po zdefiniowaniu modelu mo≈ºna dodaƒá funkcjƒô straty i optymalizatora za pomocƒÖ metody kompilacji modelu:\n",
    "\n",
    "```\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mae\",\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "Mo≈ºemy okre≈õliƒá funkcjƒô straty i optymalizatora za pomocƒÖ tylko jednego ≈Ça≈Ñcucha znak√≥w. Mo≈ºna te≈º uzyskaƒá do nich bezpo≈õredni dostƒôp przez API Keras - je≈õli chcieliby≈õmy np. dostosowaƒá parametry - ale dla nas domy≈õlne warto≈õci bƒôdƒÖ dzia≈Çaƒá dobrze..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e31507",
   "metadata": {},
   "source": [
    "![Untitled](https://i.ibb.co/7r9PLt4/Untitled-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a489ec",
   "metadata": {},
   "source": [
    "## **Podsumowanie**\n",
    "\n",
    "- Deep learning opiera siƒô na g≈Çƒôbokich sieciach neuronowych, zdolnych do modelowania nieliniowych i z≈Ço≈ºonych zale≈ºno≈õci.\n",
    "- PodstawƒÖ jest perceptron (jednostka liniowa): y = w*x + b.\n",
    "- Aby wyj≈õƒá poza liniowo≈õƒá, dodajemy warstwy i funkcje aktywacji, tworzƒÖc MLP.\n",
    "- Backpropagation i optymalizacja (spadek gradientu) pozwalajƒÖ sieci uczyƒá siƒô wag, kt√≥re minimalizujƒÖ funkcjƒô straty.\n",
    "- MLP jest podstawowym przyk≈Çadem sieci neuronowej, stanowiƒÖc fundament do poznania bardziej zaawansowanych architektur g≈Çƒôbokiego uczenia.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": ".VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
