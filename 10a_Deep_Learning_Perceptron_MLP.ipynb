{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1154e1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Wprowadzenie do gÅ‚Ä™bokiego uczenia (Deep Learning), perceptronu, MLP\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ï»¿![](https://datascientest.com/en/files/2021/01/Machine-learning-def-.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1e9d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "W tych zajÄ™ciach wprowadzimy pojÄ™cia zwiÄ…zane z gÅ‚Ä™bokim uczeniem, poczÄ…wszy od pojedynczego perceptronu, aÅ¼ po wielowarstwowe sieci neuronowe typu MLP (Multi-Layer Perceptron). Poznamy podstawy sieci neuronowych, rolÄ™ funkcji aktywacji, ideÄ™ wielowarstwowych architektur, a takÅ¼e koncepcjÄ™ gradientu i wstecznej propagacji bÅ‚Ä™dÃ³w (backpropagation) â€“ oczywiÅ›cie bez wchodzenia w skomplikowane rÃ³wnania matematyczne.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc86da4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Czym jest Deep Learning?**\n",
    "\n",
    "- Deep Learning to poddziedzina uczenia maszynowego, ktÃ³ra osiÄ…gnÄ™Å‚a w ostatnich latach spektakularne wyniki w wielu dziedzinach, takich jak rozpoznawanie obrazÃ³w, tÅ‚umaczenie maszynowe, analiza tekstu czy sterowanie grami.\n",
    "- GÅ‚Ä™bokie sieci neuronowe skÅ‚adajÄ… siÄ™ z wielu warstw przetwarzajÄ…cych dane, co pozwala im hierarchicznie wyodrÄ™bniaÄ‡ wzorce o rosnÄ…cej zÅ‚oÅ¼onoÅ›ci.\n",
    "- W przeciwieÅ„stwie do tradycyjnego uczenia maszynowego, deep learning czÄ™sto eliminuje koniecznoÅ›Ä‡ rÄ™cznego tworzenia cech (feature engineering), pozwalajÄ…c modelom \"nauczyÄ‡ siÄ™\" optymalnych reprezentacji bezpoÅ›rednio z danych.\n",
    "- PostÄ™py w deep learningu doprowadziÅ‚y do powstania modeli, ktÃ³re dorÃ³wnujÄ… lub przewyÅ¼szajÄ… moÅ¼liwoÅ›ci ludzkich ekspertÃ³w w wielu zÅ‚oÅ¼onych zadaniach.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e4233",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Perceptron i jednostka liniowa (The Linear Unit)**\n",
    "\n",
    "### Podstawy perceptronu\n",
    "Perceptron to najprostszy model neuronu w sztucznej sieci neuronowej. Przyjmuje pewne wejÅ›cia, mnoÅ¼y je przez odpowiednie wagi, dodaje obciÄ…Å¼enie (bias) i wylicza sumÄ™ liniowÄ….\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb211a",
   "metadata": {},
   "source": [
    "\n",
    "**Jednostka liniowa: ğ‘¦ = ğ‘¤ğ‘¥ + ğ‘**\n",
    "\n",
    "- WejÅ›cie: x  \n",
    "- Waga: w  \n",
    "- ObciÄ…Å¼enie (bias): b  \n",
    "- WyjÅ›cie: y = w*x + b\n",
    "\n",
    "W ten sposÃ³b pojedynczy perceptron jest modelem liniowym. MoÅ¼e reprezentowaÄ‡ proste zaleÅ¼noÅ›ci, takie jak rÃ³wnanie prostej na wykresie.\n",
    "\n",
    "![Untitled%207f31d7e7baa34ad182834df4a6f789de/Untitled.png](https://i.ibb.co/jGQrQDX/Untitled.png)\n",
    "\n",
    "W tym przykÅ‚adzie widzimy pojedynczy neuron z jednym wejÅ›ciem x. Waga w oraz bias b pozwalajÄ… regulowaÄ‡ nachylenie i poÅ‚oÅ¼enie prostej.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5c407",
   "metadata": {},
   "source": [
    "\n",
    "## **PrzykÅ‚ad: jednostka liniowa jako model**\n",
    "\n",
    "RozwaÅ¼my zbiÃ³r danych dotyczÄ…cy pÅ‚atkÃ³w Å›niadaniowych (80 Cereals). JeÅ›li nasz model miaÅ‚by przewidywaÄ‡ kalorycznoÅ›Ä‡ na podstawie zawartoÅ›ci cukru, uzyskamy rÃ³wnanie w stylu:\n",
    "\n",
    "kalorie = w*(cukier) + b\n",
    "\n",
    "JeÅ¼eli w = 2,5 a b = 90, to dla 5 gramÃ³w cukru model da kalorie = 2,5*5 + 90 = 102,5. To nadal model liniowy. DodajÄ…c inne cechy, np. bÅ‚onnik czy biaÅ‚ko, neuron bÄ™dzie miaÅ‚ wiÄ™cej wag, ale nadal pozostanie liniowy.\n",
    "\n",
    "![Untitled](https://i.ibb.co/xhC1X7k/Untitled-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60790ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mfr</th>\n",
       "      <th>type</th>\n",
       "      <th>calories</th>\n",
       "      <th>protein</th>\n",
       "      <th>fat</th>\n",
       "      <th>sodium</th>\n",
       "      <th>fiber</th>\n",
       "      <th>carbo</th>\n",
       "      <th>sugars</th>\n",
       "      <th>potass</th>\n",
       "      <th>vitamins</th>\n",
       "      <th>shelf</th>\n",
       "      <th>weight</th>\n",
       "      <th>cups</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100% Bran</td>\n",
       "      <td>N</td>\n",
       "      <td>C</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>280</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>68.402973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100% Natural Bran</td>\n",
       "      <td>Q</td>\n",
       "      <td>C</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>33.983679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All-Bran</td>\n",
       "      <td>K</td>\n",
       "      <td>C</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>320</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>59.425505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All-Bran with Extra Fiber</td>\n",
       "      <td>K</td>\n",
       "      <td>C</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>330</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>93.704912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Almond Delight</td>\n",
       "      <td>R</td>\n",
       "      <td>C</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>34.384843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name mfr type  calories  protein  fat  sodium  fiber  \\\n",
       "0                  100% Bran   N    C        70        4    1     130   10.0   \n",
       "1          100% Natural Bran   Q    C       120        3    5      15    2.0   \n",
       "2                   All-Bran   K    C        70        4    1     260    9.0   \n",
       "3  All-Bran with Extra Fiber   K    C        50        4    0     140   14.0   \n",
       "4             Almond Delight   R    C       110        2    2     200    1.0   \n",
       "\n",
       "   carbo  sugars  potass  vitamins  shelf  weight  cups     rating  \n",
       "0    5.0       6     280        25      3     1.0  0.33  68.402973  \n",
       "1    8.0       8     135         0      3     1.0  1.00  33.983679  \n",
       "2    7.0       5     320        25      3     1.0  0.33  59.425505  \n",
       "3    8.0       0     330        25      3     1.0  0.50  93.704912  \n",
       "4   14.0       8      -1        25      3     1.0  0.75  34.384843  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sugars</th>\n",
       "      <th>calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sugars  calories\n",
       "0       6        70\n",
       "1       8       120\n",
       "2       5        70\n",
       "3       0        50\n",
       "4       8       110"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cereal = pd.read_csv('data/cereal.csv')\n",
    "\n",
    "\n",
    "display(cereal.head(),cereal[['sugars','calories' ]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38abf09",
   "metadata": {},
   "source": [
    "### **Jednostki liniowe w Kerasie**\n",
    "\n",
    "Najprostszym sposobem na stworzenie modelu w Kerasie jest uÅ¼ycie funkcji **`keras.Sequential`**, ktÃ³ra tworzy sieÄ‡ neuronowÄ… jako stos warstw. MoÅ¼emy tworzyÄ‡ modele podobne do tych pokazanych powyÅ¼ej, uÅ¼ywajÄ…c gÄ™stej warstwy (o ktÃ³rej dowiemy siÄ™ wiÄ™cej w nastÄ™pnej lekcji).\n",
    "\n",
    "MoÅ¼emy zdefiniowaÄ‡ liniowy model, ktÃ³ry przyjmuje trzy cechy wejÅ›ciowe (\"cukry\", \"bÅ‚onnik\" i \"biaÅ‚ko\") i generuje pojedyncze wyjÅ›cie (\"kalorie\"), w nastÄ™pujÄ…cy sposÃ³b:\n",
    "\n",
    "```python\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# UtwÃ³rz sieÄ‡ z 1 jednostkÄ… liniowÄ…\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units=1, input_shape=[3])\n",
    "])\n",
    "```\n",
    "\n",
    "Przy pomocy pierwszego argumentu, **`units`**, okreÅ›lamy, jakie wyjÅ›cie chcemy otrzymaÄ‡. W tym przypadku wybieramy tylko \"kalorie\" i uÅ¼ywamy **`units=1`**.\n",
    "\n",
    "W drugim argumencie, **`input_shape`**, informujemy Kerasa o wymiarach wejÅ›Ä‡. Ustawienie **`input_shape=[3]`** oznacza, Å¼e model bÄ™dzie przyjmowaÅ‚ trzy cechy jako wejÅ›cie (\"cukry\", \"bÅ‚onnik\" i \"biaÅ‚ko\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434163cd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Dlaczego **`input_shape`** jest listÄ… w Pythonie?\n",
    "\n",
    "PowyÅ¼sze cechy sÄ… uÅ‚oÅ¼one w kolumnach, wiÄ™c zawsze bÄ™dziemy mieli **`input_shape=[liczba_kolumn]`**. Keras uÅ¼ywa listy tutaj, aby umoÅ¼liwiÄ‡ uÅ¼ycie bardziej zÅ‚oÅ¼onych zestawÃ³w danych. Dane obrazowe, na przykÅ‚ad, mogÄ… wymagaÄ‡ trzech wymiarÃ³w: **`[wysokoÅ›Ä‡, szerokoÅ›Ä‡, kanaÅ‚y]`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24cddc",
   "metadata": {},
   "source": [
    "Zestaw danych 80 Cereals charakteryzuje siÄ™ znacznie wiÄ™kszÄ… liczbÄ… cech niÅ¼ tylko \"cukry\". JeÅ›li zdecydujemy siÄ™ na rozwiniÄ™cie modelu o dodatkowe cechy, takie jak zawartoÅ›Ä‡ bÅ‚onnika lub biaÅ‚ka, bÄ™dzie to proste do zrealizowania. Wystarczy dodaÄ‡ wiÄ™cej poÅ‚Ä…czeÅ„ wejÅ›ciowych do neuronu, jeden dla kaÅ¼dej dodatkowej cechy. Aby uzyskaÄ‡ wynik, pomnoÅ¼ymy kaÅ¼de wejÅ›cie przez wagÄ™ jego poÅ‚Ä…czenia, a nastÄ™pnie dodamy wszystkie wyniki razem, co jest bardzo proste.\n",
    "\n",
    "Mamy trzy poÅ‚Ä…czenia wejÅ›ciowe: x0, x1 i x2, wraz z obciÄ…Å¼eniem.\n",
    "\n",
    "![Untitled](https://i.ibb.co/YT5ZywK/Untitled-2.png)\n",
    "\n",
    "RÃ³wnanie dla tego neuronu to ğ‘¦ = ğ‘¤0ğ‘¥0 + ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 + ğ‘. Jednostka liniowa z dwoma wejÅ›ciami pasuje do pÅ‚aszczyzny, a jednostka z wiÄ™cej wejÅ›ciami niÅ¼ to bÄ™dzie pasowaÄ‡ do hiperpÅ‚aszczyzny."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81eb3d",
   "metadata": {},
   "source": [
    "\n",
    "### Wiele wejÅ›Ä‡\n",
    "\n",
    "JeÅ›li chcemy wykorzystaÄ‡ wiÄ™cej cech wejÅ›ciowych (x0, x1, x2 ...), dodajemy wiÄ™cej wag i sumujemy ich iloczyny z odpowiednimi wejÅ›ciami. Dla dwÃ³ch wejÅ›Ä‡ mielibyÅ›my: y = w0*x0 + w1*x1 + b, co reprezentuje rÃ³wnanie pÅ‚aszczyzny.\n",
    "\n",
    "Przy wiÄ™kszej liczbie wejÅ›Ä‡ powstaje hiperpÅ‚aszczyzna w wielowymiarowej przestrzeni cech.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d30dc",
   "metadata": {},
   "source": [
    "\n",
    "## **Przeskok do wielu warstw â€“ MLP**\n",
    "\n",
    "### Idea MLP\n",
    "\n",
    "MLP (Multi-Layer Perceptron) to sieÄ‡ neuronowa zÅ‚oÅ¼ona z wielu warstw:\n",
    "\n",
    "- Warstwa wejÅ›ciowa (input)\n",
    "- Jedna lub wiÄ™cej warstw ukrytych (hidden layers)\n",
    "- Warstwa wyjÅ›ciowa (output)\n",
    "KaÅ¼da warstwa skÅ‚ada siÄ™ z wielu neuronÃ³w (jednostek liniowych), a pomiÄ™dzy warstwami wprowadzamy nieliniowoÅ›Ä‡ poprzez funkcje aktywacji.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d36d2e",
   "metadata": {},
   "source": [
    "Sieci neuronowe zazwyczaj organizujÄ… swoje neurony w warstwy. Kiedy zbieramy razem jednostki liniowe, ktÃ³re majÄ… wspÃ³lny zbiÃ³r wejÅ›Ä‡, otrzymujemy gÄ™stÄ… warstwÄ™.\n",
    "\n",
    "![Untitled](https://i.ibb.co/3TJjL14/Untitled-3.png)\n",
    "\n",
    "MoÅ¼na rozwaÅ¼aÄ‡ kaÅ¼dÄ… warstwÄ™ w sieci neuronowej jako przeprowadzajÄ…cÄ… stosunkowo prostÄ… transformacjÄ™. DziÄ™ki stosowaniu wielu warstw, sieÄ‡ neuronowa moÅ¼e przetwarzaÄ‡ swoje wejÅ›cie w coraz bardziej zÅ‚oÅ¼onych sposobach. W dobrze wytrenowanej sieci neuronowej kaÅ¼da warstwa to transformacja, ktÃ³ra przybliÅ¼a nas trochÄ™ bardziej do rozwiÄ…zania."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a64803",
   "metadata": {},
   "source": [
    "### Warstwy ukryte i reprezentacje wewnÄ™trzne\n",
    "\n",
    "Gdy Å‚Ä…czymy wiele warstw, pierwsze warstwy wyodrÄ™bniajÄ… proste wzorce, a kolejne, bardziej ukryte warstwy Å‚Ä…czÄ… je w coraz bardziej zÅ‚oÅ¼one reprezentacje. W efekcie MLP potrafi modelowaÄ‡ nieliniowe i skomplikowane zaleÅ¼noÅ›ci.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e1e4f",
   "metadata": {},
   "source": [
    "## **Funkcje aktywacji**\n",
    "\n",
    "Bez funkcji aktywacji dwie warstwy liniowe skÅ‚adajÄ… siÄ™ nadal na model liniowy. Funkcja aktywacji wprowadza nieliniowoÅ›Ä‡, np. ReLU (Rectified Linear Unit):\n",
    "\n",
    "ReLU: max(0, x)\n",
    "\n",
    "Funkcja ReLU \"prostuje\" ujemne wartoÅ›ci do zera, a dla wartoÅ›ci dodatnich roÅ›nie liniowo. DziÄ™ki temu sieÄ‡ moÅ¼e dopasowywaÄ‡ skomplikowane, krzywoliniowe zaleÅ¼noÅ›ci.\n",
    "\n",
    "![Untitled](https://i.ibb.co/mF55Qcd/Untitled-5.png)\n",
    "\n",
    "W praktyce stosuje siÄ™ teÅ¼ inne funkcje aktywacji, np. sigmoid, tanh czy softmax (dla zadaÅ„ klasyfikacji).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a83c24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "PoÅ‚Ä…czenie jednostki liniowej z prostownikiem daje jednostkÄ™ liniowÄ… prostowanÄ… (ReLU). Z tego powodu funkcjÄ™ prostownikowÄ… czÄ™sto nazywa siÄ™ funkcjÄ… ReLU. Zastosowanie aktywacji ReLU do jednostki liniowej oznacza, Å¼e wynik jest rÃ³wny max(0, w * x + b)\n",
    "\n",
    "![Untitled](https://i.ibb.co/DM5x0q9/Untitled-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9801ff",
   "metadata": {},
   "source": [
    "## **Architektura MLP**\n",
    "\n",
    "MLP to sieÄ‡ w peÅ‚ni poÅ‚Ä…czonych warstw (dense layers). KaÅ¼da warstwa to zbiÃ³r neuronÃ³w, a kaÅ¼dy neuron w warstwie jest poÅ‚Ä…czony ze wszystkimi neuronami warstwy poprzedniej.\n",
    "\n",
    "![Untitled](https://i.ibb.co/C0MFmx6/Untitled-7.png)\n",
    "\n",
    "Warstwy ukryte zawierajÄ… funkcje aktywacji, warstwa wyjÅ›ciowa moÅ¼e byÄ‡ liniowa (regresja) lub mieÄ‡ innÄ… aktywacjÄ™ (np. softmax dla klasyfikacji wieloklasowej).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffcdc0c",
   "metadata": {},
   "source": [
    "\n",
    "## Budowanie modeli sekwencyjnych\n",
    "\n",
    "Model sekwencyjny, ktÃ³ry uÅ¼ywaliÅ›my, Å‚Ä…czy ze sobÄ… listÄ™ warstw w kolejnoÅ›ci od pierwszej do ostatniej. Pierwsza warstwa otrzymuje wejÅ›cie, a ostatnia warstwa produkuje wyjÅ›cie. Powstaje w ten sposÃ³b model przedstawiony na powyÅ¼szym diagramie.\n",
    "\n",
    "```\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
    "    layers.Dense(units=3, activation='relu'),\n",
    "    # the linear output layer\n",
    "    layers.Dense(units=1),\n",
    "])\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3398b",
   "metadata": {},
   "source": [
    "## **Koncepcja gradientu i backpropagation**\n",
    "\n",
    "### Gradient i minimalizacja straty\n",
    "\n",
    "- SieÄ‡ neuronowa jest trenowana poprzez minimalizacjÄ™ funkcji straty, ktÃ³ra mierzy rÃ³Å¼nicÄ™ miÄ™dzy predykcjami a wartoÅ›ciami docelowymi.\n",
    "- Gradient funkcji straty wzglÄ™dem wag mÃ³wi nam, w jakim kierunku powinniÅ›my zmieniaÄ‡ wagi, aby zmniejszyÄ‡ bÅ‚Ä…d.\n",
    "- Spadek gradientu (Gradient Descent) polega na iteracyjnym dostosowywaniu wag w kierunku przeciwnym do gradientu, co prowadzi do minimalizacji straty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf5591",
   "metadata": {},
   "source": [
    "## **Optymalizator - Stochastyczny spadek gradientu**\n",
    "\n",
    "W uczeniu maszynowym do optymalizacji wag sieci uÅ¼ywa siÄ™ algorytmÃ³w nazywanych optymalizatorami. NajczÄ™Å›ciej stosowanÄ… rodzinÄ… algorytmÃ³w optymalizacji jest Stochastyczny Spadek Gradientu (SGD).\n",
    "\n",
    "SGD to iteracyjny algorytm, ktÃ³ry trenuje sieÄ‡ w krokach. W kaÅ¼dym kroku:\n",
    "\n",
    "- Wybierana jest prÃ³bka danych treningowych, przez ktÃ³rÄ… sieÄ‡ przetwarza predykcjÄ™.\n",
    "- Obliczana jest rÃ³Å¼nica miÄ™dzy predykcjÄ… a prawdziwymi wartoÅ›ciami.\n",
    "- Wagi sieci sÄ… dostosowywane, aby zminimalizowaÄ‡ stratÄ™.\n",
    "\n",
    "Proces ten jest powtarzany, aÅ¼ wartoÅ›Ä‡ straty bÄ™dzie wystarczajÄ…co maÅ‚a (lub do momentu, gdy przestanie siÄ™ dalej zmniejszaÄ‡).\n",
    "\n",
    "[https://i.imgur.com/rFI1tIk.mp4](https://i.imgur.com/rFI1tIk.mp4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc0457d",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "Backpropagation to algorytm, ktÃ³ry oblicza gradienty dla wszystkich wag w sieci poprzez propagacjÄ™ bÅ‚Ä™du od warstwy wyjÅ›ciowej do warstw wejÅ›ciowych.\n",
    "\n",
    "ChoÄ‡ matematycznie zÅ‚oÅ¼ony, koncepcyjnie dziaÅ‚a tak:\n",
    "- Oblicz stratÄ™ na wyjÅ›ciu sieci (rÃ³Å¼nicÄ™ miÄ™dzy przewidywanÄ… wartoÅ›ciÄ… a wartoÅ›ciÄ… rzeczywistÄ…).\n",
    "- RozÅ‚Ã³Å¼ ten bÅ‚Ä…d warstwa po warstwie, obliczajÄ…c wpÅ‚yw kaÅ¼dej wagi na koÅ„cowÄ… stratÄ™.\n",
    "- Na podstawie tego wpÅ‚ywu (gradientu) dostosuj wagi, aby w kolejnej iteracji wynik byÅ‚ bliÅ¼szy oczekiwanemu.\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3586254a",
   "metadata": {},
   "source": [
    "## **Funkcje Straty i Optymalizacja**\n",
    "\n",
    "Aby trenowaÄ‡ sieÄ‡, potrzebujemy:\n",
    "- Funkcji straty (loss function), np. MSE (Mean Squared Error) dla regresji, lub Cross-Entropy dla klasyfikacji.\n",
    "- Optymalizatora, np. Adam, RMSProp czy klasyczny Stochastic Gradient Descent, ktÃ³ry na podstawie obliczonych gradientÃ³w aktualizuje wagi w kierunku minimalizacji straty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb8244",
   "metadata": {},
   "source": [
    "\n",
    "PowszechnÄ… funkcjÄ… straty dla problemÃ³w regresji jest Å›redni bÅ‚Ä…d bezwzglÄ™dny lub MAE. Dla kaÅ¼dej predykcji **`y_pred`**, MAE mierzy rÃ³Å¼nicÄ™ miÄ™dzy prawdziwÄ… wartoÅ›ciÄ… docelowÄ… **`y_true`** a przewidywanÄ… przez model wartoÅ›ciÄ… **`abs(y_true - y_pred)`**.\n",
    "\n",
    "ÅÄ…czna funkcja straty MAE na zbiorze danych to Å›rednia z tych bezwzglÄ™dnych rÃ³Å¼nic.\n",
    "\n",
    "OprÃ³cz MAE, inne funkcje straty, ktÃ³re moÅ¼esz spotkaÄ‡ dla problemÃ³w regresji, to Å›redni bÅ‚Ä…d kwadratowy (MSE) lub straty Hubera (obie dostÄ™pne w Keras).\n",
    "\n",
    "![VDcvkZN.png](https://i.ibb.co/9WCzNXk/VDcvkZN.png)\n",
    "\n",
    "Podczas treningu modelu funkcja straty jest wykorzystywana jako wskazÃ³wka do znalezienia poprawnych wartoÅ›ci wag (niÅ¼sza wartoÅ›Ä‡ straty jest lepsza). Innymi sÅ‚owy, funkcja straty mÃ³wi sieci, jaki jest jej cel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d13cc7",
   "metadata": {},
   "source": [
    "\n",
    "## **Dodawanie funkcji straty i optymalizatora**\n",
    "\n",
    "Po zdefiniowaniu modelu moÅ¼na dodaÄ‡ funkcjÄ™ straty i optymalizatora za pomocÄ… metody kompilacji modelu:\n",
    "\n",
    "```\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mae\",\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "MoÅ¼emy okreÅ›liÄ‡ funkcjÄ™ straty i optymalizatora za pomocÄ… tylko jednego Å‚aÅ„cucha znakÃ³w. MoÅ¼na teÅ¼ uzyskaÄ‡ do nich bezpoÅ›redni dostÄ™p przez API Keras - jeÅ›li chcielibyÅ›my np. dostosowaÄ‡ parametry - ale dla nas domyÅ›lne wartoÅ›ci bÄ™dÄ… dziaÅ‚aÄ‡ dobrze..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e31507",
   "metadata": {},
   "source": [
    "![Untitled](https://i.ibb.co/7r9PLt4/Untitled-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a489ec",
   "metadata": {},
   "source": [
    "## **Podsumowanie**\n",
    "\n",
    "- Deep learning opiera siÄ™ na gÅ‚Ä™bokich sieciach neuronowych, zdolnych do modelowania nieliniowych i zÅ‚oÅ¼onych zaleÅ¼noÅ›ci.\n",
    "- PodstawÄ… jest perceptron (jednostka liniowa): y = w*x + b.\n",
    "- Aby wyjÅ›Ä‡ poza liniowoÅ›Ä‡, dodajemy warstwy i funkcje aktywacji, tworzÄ…c MLP.\n",
    "- Backpropagation i optymalizacja (spadek gradientu) pozwalajÄ… sieci uczyÄ‡ siÄ™ wag, ktÃ³re minimalizujÄ… funkcjÄ™ straty.\n",
    "- MLP jest podstawowym przykÅ‚adem sieci neuronowej, stanowiÄ…c fundament do poznania bardziej zaawansowanych architektur gÅ‚Ä™bokiego uczenia.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": ".VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
