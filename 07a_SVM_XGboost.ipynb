{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8c915e",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM) i XGBoost \n",
    "\n",
    "\n",
    "\n",
    "## Wprowadzenie\n",
    "\n",
    "W tym notebooku poznamy dwa potÄ™Å¼ne algorytmy uczenia maszynowego:\n",
    "\n",
    "### Support Vector Machines (SVM)\n",
    "![SVM Concept](https://scikit-learn.org/stable/_images/sphx_glr_plot_svm_margin_001.png)\n",
    "\n",
    "**SVM** to algorytm znajdowania optymalnej granicy decyzyjnej, ktÃ³ry maksymalizuje margines miÄ™dzy klasami.\n",
    "\n",
    "### XGBoost\n",
    "![XGBoost Concept](https://aiml.com/wp-content/uploads/2024/06/evolution-of-boosting.png)\n",
    "\n",
    "**XGBoost** to zaawansowana implementacja gradient boosting, ktÃ³ra buduje model z wielu sÅ‚abych uczniÃ³w.\n",
    "\n",
    "---\n",
    "\n",
    "Oba algorytmy majÄ… rÃ³Å¼ne podejÅ›cia do rozwiÄ…zywania problemÃ³w klasyfikacji i regresji, co czyni je komplementarnymi narzÄ™dziami w arsenale data scientist'a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b113dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import niezbÄ™dnych bibliotek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_circles, make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ustawienia wykresÃ³w\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49d94f",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "## Teoria i podstawy\n",
    "\n",
    "![SVM Visualization](https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/512px-SVM_margin.png)\n",
    "\n",
    "**Support Vector Machine (SVM)** to algorytm nadzorowanego uczenia maszynowego, ktÃ³ry:\n",
    "- Szuka **hiperplanu** (granicy decyzyjnej) najlepiej oddzielajÄ…cego klasy\n",
    "- **Maksymalizuje margines** - odlegÅ‚oÅ›Ä‡ od najbliÅ¼szych punktÃ³w kaÅ¼dej klasy\n",
    "- MoÅ¼e tworzyÄ‡ **nieliniowe granice** dziÄ™ki funkcjom jÄ…dra (kernelom)\n",
    "\n",
    "### Kluczowe koncepcje:\n",
    "- **Wektory noÅ›ne (Support Vectors)** - punkty najbliÅ¼sze granicy decyzyjnej\n",
    "- **Margines** - \"Å¡szerokoÅ›Ä‡\" strefy separujÄ…cej klasy\n",
    "- **Parametr C** - kontroluje trade-off miÄ™dzy maksymalizacjÄ… marginesu a minimalizacjÄ… bÅ‚Ä™dÃ³w\n",
    "- **Kernele** - umoÅ¼liwiajÄ… modelowanie nieliniowych zaleÅ¼noÅ›ci\n",
    "\n",
    "### Typy kerneli:\n",
    "![Kernel Types](https://scikit-learn.org/stable/_images/sphx_glr_plot_svm_kernels_001.png)\n",
    "\n",
    "1. **Linear Kernel** - dla danych liniowo separowalnych\n",
    "2. **RBF (Radial Basis Function)** - najczÄ™Å›ciej uÅ¼ywany dla danych nieliniowych\n",
    "3. **Polynomial Kernel** - dla zÅ‚oÅ¼onych zaleÅ¼noÅ›ci wielomianowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003faaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do wizualizacji granic decyzyjnych\n",
    "def plot_decision_boundary(model, X, y, title=\"\", ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Tworzenie siatki punktÃ³w\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predykcja dla kaÅ¼dego punktu siatki\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Rysowanie granicy decyzyjnej\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
    "    \n",
    "    # Rysowanie punktÃ³w danych\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b101e",
   "metadata": {},
   "source": [
    "## PrzykÅ‚ad 1: SVM na danych liniowo separowalnych\n",
    "\n",
    "Zaczniemy od prostego przykÅ‚adu z danymi, ktÃ³re moÅ¼na rozdzieliÄ‡ liniÄ… prostÄ…."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197db4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generowanie danych liniowo separowalnych\n",
    "np.random.seed(42)\n",
    "X_linear, y_linear = make_classification(n_samples=100, n_features=2, n_redundant=0, \n",
    "                                       n_informative=2, n_clusters_per_class=1, \n",
    "                                       random_state=42)\n",
    "\n",
    "# PodziaÅ‚ na zbiory treningowy i testowy\n",
    "X_train_lin, X_test_lin, y_train_lin, y_test_lin = train_test_split(\n",
    "    X_linear, y_linear, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardyzacja danych\n",
    "scaler = StandardScaler()\n",
    "X_train_lin_scaled = scaler.fit_transform(X_train_lin)\n",
    "X_test_lin_scaled = scaler.transform(X_test_lin)\n",
    "\n",
    "print(f\"Rozmiar zbioru treningowego: {X_train_lin_scaled.shape}\")\n",
    "print(f\"Rozmiar zbioru testowego: {X_test_lin_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec8c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trenowanie SVM z kernelem liniowym\n",
    "svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_linear.fit(X_train_lin_scaled, y_train_lin)\n",
    "\n",
    "# Predykcja i ocena\n",
    "y_pred_lin = svm_linear.predict(X_test_lin_scaled)\n",
    "accuracy_lin = accuracy_score(y_test_lin, y_pred_lin)\n",
    "\n",
    "print(f\"DokÅ‚adnoÅ›Ä‡ SVM liniowego: {accuracy_lin:.3f}\")\n",
    "\n",
    "# Wizualizacja\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Dane oryginalne\n",
    "ax1.scatter(X_linear[:, 0], X_linear[:, 1], c=y_linear, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "ax1.set_title('Dane oryginalne')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "\n",
    "# Granica decyzyjna\n",
    "plot_decision_boundary(svm_linear, X_train_lin_scaled, y_train_lin, \n",
    "                      'SVM Linear - Granica decyzyjna', ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a4757",
   "metadata": {},
   "source": [
    "## PrzykÅ‚ad 2: SVM na danych nieliniowo separowalnych\n",
    "\n",
    "Teraz sprawdzimy, jak SVM radzi sobie z danymi, ktÃ³re nie mogÄ… byÄ‡ rozdzielone liniÄ… prostÄ…. UÅ¼yjemy danych w ksztaÅ‚cie okrÄ™gÃ³w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80943ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generowanie danych w ksztaÅ‚cie okrÄ™gÃ³w (nieliniowo separowalne)\n",
    "X_circles, y_circles = make_circles(n_samples=300, factor=0.5, noise=0.1, random_state=42)\n",
    "\n",
    "# PodziaÅ‚ na zbiory\n",
    "X_train_circ, X_test_circ, y_train_circ, y_test_circ = train_test_split(\n",
    "    X_circles, y_circles, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Rozmiar zbioru treningowego: {X_train_circ.shape}\")\n",
    "print(f\"Rozmiar zbioru testowego: {X_test_circ.shape}\")\n",
    "\n",
    "# Wizualizacja danych\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "plt.title('Dane w ksztaÅ‚cie okrÄ™gÃ³w')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca93c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorÃ³wnanie rÃ³Å¼nych kerneli SVM\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_value = 1.0\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, kernel in enumerate(kernels):\n",
    "    # Trenowanie modelu\n",
    "    if kernel == 'poly':\n",
    "        svm = SVC(kernel=kernel, C=C_value, degree=3, random_state=42)\n",
    "    else:\n",
    "        svm = SVC(kernel=kernel, C=C_value, random_state=42)\n",
    "    \n",
    "    svm.fit(X_train_circ, y_train_circ)\n",
    "    \n",
    "    # Predykcja i ocena\n",
    "    y_pred = svm.predict(X_test_circ)\n",
    "    accuracy = accuracy_score(y_test_circ, y_pred)\n",
    "    \n",
    "    # Wizualizacja\n",
    "    plot_decision_boundary(svm, X_circles, y_circles, \n",
    "                          f'SVM {kernel.capitalize()} (Acc: {accuracy:.3f})', axes[i])\n",
    "    \n",
    "    print(f\"DokÅ‚adnoÅ›Ä‡ SVM {kernel}: {accuracy:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d1decc",
   "metadata": {},
   "source": [
    "## WpÅ‚yw hiperparametrÃ³w SVM\n",
    "\n",
    "![SVM Parameters](https://scikit-learn.org/stable/_images/sphx_glr_plot_rbf_parameters_001.png)\n",
    "\n",
    "### Parametr C (Regularyzacja)\n",
    "- **Wysokie C**: Model stara siÄ™ klasyfikowaÄ‡ wszystkie punkty poprawnie â†’ moÅ¼e prowadziÄ‡ do overfitting\n",
    "- **Niskie C**: Model akceptuje wiÄ™cej bÅ‚Ä™dÃ³w, ale ma szerszy margines â†’ moÅ¼e prowadziÄ‡ do underfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04fdade",
   "metadata": {},
   "source": [
    "\n",
    "### Parametr gamma (dla kernela RBF)\n",
    "- **Wysokie gamma**: WpÅ‚yw pojedynczego punktu jest bardzo lokalny â†’ skomplikowane granice\n",
    "- **Niskie gamma**: WpÅ‚yw punktÃ³w jest szerszy â†’ gÅ‚adsze granice\n",
    "\n",
    "![Gamma Effect](https://miro.medium.com/v2/resize:fit:1400/1*vmLmExiS9n5pY4Xy9Z516g.png)\n",
    "\n",
    "### Zasady doboru parametrÃ³w:\n",
    "1. **MaÅ‚e zbiory danych** â†’ niÅ¼sze C, wyÅ¼sze gamma\n",
    "2. **DuÅ¼e zbiory danych** â†’ wyÅ¼sze C, niÅ¼sze gamma\n",
    "3. **Szumy w danych** â†’ niÅ¼sze C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a956fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Badanie wpÅ‚ywu parametru C\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, C in enumerate(C_values):\n",
    "    svm = SVC(kernel='rbf', C=C, random_state=42)\n",
    "    svm.fit(X_train_circ, y_train_circ)\n",
    "    \n",
    "    y_pred = svm.predict(X_test_circ)\n",
    "    accuracy = accuracy_score(y_test_circ, y_pred)\n",
    "    \n",
    "    plot_decision_boundary(svm, X_circles, y_circles, \n",
    "                          f'SVM RBF (C={C}, Acc: {accuracy:.3f})', axes[i])\n",
    "    \n",
    "    print(f\"C={C}: DokÅ‚adnoÅ›Ä‡ = {accuracy:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b725c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Badanie wpÅ‚ywu parametru gamma\n",
    "gamma_values = [0.1, 1, 10, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, gamma in enumerate(gamma_values):\n",
    "    svm = SVC(kernel='rbf', C=1, gamma=gamma, random_state=42)\n",
    "    svm.fit(X_train_circ, y_train_circ)\n",
    "    \n",
    "    y_pred = svm.predict(X_test_circ)\n",
    "    accuracy = accuracy_score(y_test_circ, y_pred)\n",
    "    \n",
    "    plot_decision_boundary(svm, X_circles, y_circles, \n",
    "                          f'SVM RBF (Î³={gamma}, Acc: {accuracy:.3f})', axes[i])\n",
    "    \n",
    "    print(f\"gamma={gamma}: DokÅ‚adnoÅ›Ä‡ = {accuracy:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0756eec",
   "metadata": {},
   "source": [
    "## Optymalizacja hiperparametrÃ³w SVM\n",
    "\n",
    "UÅ¼yjemy Grid Search do znalezienia optymalnych parametrÃ³w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c88cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search dla SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "svm_grid = SVC(random_state=42)\n",
    "grid_search = GridSearchCV(svm_grid, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_circ, y_train_circ)\n",
    "\n",
    "print(\"Najlepsze parametry:\", grid_search.best_params_)\n",
    "print(\"Najlepszy wynik CV:\", grid_search.best_score_)\n",
    "\n",
    "# Ocena na zbiorze testowym\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred_best = best_svm.predict(X_test_circ)\n",
    "test_accuracy = accuracy_score(y_test_circ, y_pred_best)\n",
    "print(f\"DokÅ‚adnoÅ›Ä‡ na zbiorze testowym: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wizualizacja wynikÃ³w Grid Search\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Tworzenie macierzy wynikÃ³w\n",
    "C_vals = param_grid['C']\n",
    "gamma_vals = param_grid['gamma']\n",
    "scores = np.zeros((len(gamma_vals), len(C_vals)))\n",
    "\n",
    "for i, gamma in enumerate(gamma_vals):\n",
    "    for j, C in enumerate(C_vals):\n",
    "        # ZnajdÅº wynik dla tej kombinacji\n",
    "        for result in grid_search.cv_results_['params']:\n",
    "            if result['C'] == C and result['gamma'] == gamma:\n",
    "                idx = grid_search.cv_results_['params'].index(result)\n",
    "                scores[i, j] = grid_search.cv_results_['mean_test_score'][idx]\n",
    "                break\n",
    "\n",
    "# Heatmapa\n",
    "sns.heatmap(scores, annot=True, fmt='.3f', \n",
    "            xticklabels=[f'C={c}' for c in C_vals],\n",
    "            yticklabels=[f'Î³={g}' for g in gamma_vals],\n",
    "            cmap='viridis')\n",
    "plt.title('Grid Search Results - Åšrednia dokÅ‚adnoÅ›Ä‡ CV')\n",
    "plt.xlabel('Parametr C')\n",
    "plt.ylabel('Parametr gamma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8fb1f",
   "metadata": {},
   "source": [
    "![Boosting Process](https://cdn.corporatefinanceinstitute.com/assets/boosting1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c99bcca",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### XGBoost - ulepszenia:\n",
    "\n",
    "\n",
    "- **WydajnoÅ›Ä‡**: Optymalizacje algorytmiczne i moÅ¼liwoÅ›Ä‡ przetwarzania rÃ³wnolegÅ‚ego\n",
    "- **Regularyzacja**: Wbudowane mechanizmy przeciwdziaÅ‚ajÄ…ce overfitting\n",
    "- **ElastycznoÅ›Ä‡**: ObsÅ‚uga rÃ³Å¼nych typÃ³w problemÃ³w i metryk\n",
    "- **Braki danych**: Automatyczne radzenie sobie z missing values\n",
    "\n",
    "### Algorytm krok po kroku:\n",
    "1. **Inicjalizacja**: Zacznij z prostym modelem (np. Å›rednia)\n",
    "2. **Oblicz residua**: ZnajdÅº bÅ‚Ä™dy obecnego modelu\n",
    "3. **Trenuj nowe drzewo**: Na residuach poprzedniego kroku\n",
    "4. **Dodaj do ensemble**: Z odpowiedniÄ… wagÄ… (learning_rate)\n",
    "5. **PowtÃ³rz**: AÅ¼ do osiÄ…gniÄ™cia zaÅ‚oÅ¼onej liczby drzew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cabeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrzykÅ‚ad z danymi \"moons\" dla XGBoost\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "\n",
    "# PodziaÅ‚ na zbiory\n",
    "X_train_moons, X_test_moons, y_train_moons, y_test_moons = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.3, random_state=42)\n",
    "\n",
    "# Wizualizacja danych\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "plt.title('Dane \"moons\"')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Rozmiar zbioru treningowego: {X_train_moons.shape}\")\n",
    "print(f\"Rozmiar zbioru testowego: {X_test_moons.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005be441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trenowanie XGBoost z domyÅ›lnymi parametrami\n",
    "xgb_default = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "xgb_default.fit(X_train_moons, y_train_moons)\n",
    "\n",
    "# Predykcja i ocena\n",
    "y_pred_xgb = xgb_default.predict(X_test_moons)\n",
    "accuracy_xgb = accuracy_score(y_test_moons, y_pred_xgb)\n",
    "\n",
    "print(f\"DokÅ‚adnoÅ›Ä‡ XGBoost (domyÅ›lne): {accuracy_xgb:.3f}\")\n",
    "\n",
    "# Wizualizacja\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_boundary(xgb_default, X_moons, y_moons, \n",
    "                      f'XGBoost - Granica decyzyjna (Acc: {accuracy_xgb:.3f})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27111ca",
   "metadata": {},
   "source": [
    "## Kluczowe hiperparametry XGBoost\n",
    "\n",
    "\n",
    "### Parametry sterujÄ…ce zÅ‚oÅ¼onoÅ›ciÄ… modelu:\n",
    "- **n_estimators**: Liczba drzew w ensemble\n",
    "- **max_depth**: Maksymalna gÅ‚Ä™bokoÅ›Ä‡ pojedynczego drzewa\n",
    "- **learning_rate (eta)**: Tempo uczenia - skaluje wkÅ‚ad kaÅ¼dego drzewa\n",
    "\n",
    "### WpÅ‚yw max_depth:\n",
    "\n",
    "- **GÅ‚Ä™bokie drzewa (depth > 6)**: MogÄ… prowadziÄ‡ do overfitting\n",
    "- **PÅ‚ytkie drzewa (depth < 3)**: MogÄ… prowadziÄ‡ do underfitting\n",
    "- **Optymalne (depth 3-6)**: Dobry balans bias-variance\n",
    "\n",
    "### Parametry regularyzacji:\n",
    "- **subsample**: Frakcja prÃ³bek uÅ¼ywana do trenowania kaÅ¼dego drzewa\n",
    "- **colsample_bytree**: Frakcja cech uÅ¼ywana do trenowania kaÅ¼dego drzewa\n",
    "- **reg_alpha**: Regularyzacja L1\n",
    "- **reg_lambda**: Regularyzacja L2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a8b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Badanie wpÅ‚ywu max_depth\n",
    "max_depths = [1, 3, 6, 10]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, max_depth in enumerate(max_depths):\n",
    "    xgb_model = XGBClassifier(max_depth=max_depth, n_estimators=100, \n",
    "                             learning_rate=0.1, random_state=42, eval_metric='logloss')\n",
    "    xgb_model.fit(X_train_moons, y_train_moons)\n",
    "    \n",
    "    # Ocena na zbiorach treningowym i testowym\n",
    "    train_acc = xgb_model.score(X_train_moons, y_train_moons)\n",
    "    test_acc = xgb_model.score(X_test_moons, y_test_moons)\n",
    "    \n",
    "    plot_decision_boundary(xgb_model, X_moons, y_moons, \n",
    "                          f'XGBoost (depth={max_depth})\\nTrain: {train_acc:.3f}, Test: {test_acc:.3f}', \n",
    "                          axes[i])\n",
    "    \n",
    "    print(f\"max_depth={max_depth}: Train={train_acc:.3f}, Test={test_acc:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb508d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Badanie wpÅ‚ywu learning_rate\n",
    "learning_rates = [0.01, 0.1, 0.3, 1.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    xgb_model = XGBClassifier(learning_rate=lr, n_estimators=100, \n",
    "                             max_depth=3, random_state=42, eval_metric='logloss')\n",
    "    xgb_model.fit(X_train_moons, y_train_moons)\n",
    "    \n",
    "    train_acc = xgb_model.score(X_train_moons, y_train_moons)\n",
    "    test_acc = xgb_model.score(X_test_moons, y_test_moons)\n",
    "    \n",
    "    plot_decision_boundary(xgb_model, X_moons, y_moons, \n",
    "                          f'XGBoost (lr={lr})\\nTrain: {train_acc:.3f}, Test: {test_acc:.3f}', \n",
    "                          axes[i])\n",
    "    \n",
    "    print(f\"learning_rate={lr}: Train={train_acc:.3f}, Test={test_acc:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457b023",
   "metadata": {},
   "source": [
    "## Optymalizacja hiperparametrÃ³w XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f63ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search dla XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "grid_search_xgb = GridSearchCV(xgb_grid, param_grid_xgb, cv=5, \n",
    "                              scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Rozpoczynam Grid Search dla XGBoost...\")\n",
    "grid_search_xgb.fit(X_train_moons, y_train_moons)\n",
    "\n",
    "print(\"Najlepsze parametry XGBoost:\", grid_search_xgb.best_params_)\n",
    "print(\"Najlepszy wynik CV:\", grid_search_xgb.best_score_)\n",
    "\n",
    "# Ocena na zbiorze testowym\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "y_pred_best_xgb = best_xgb.predict(X_test_moons)\n",
    "test_accuracy_xgb = accuracy_score(y_test_moons, y_pred_best_xgb)\n",
    "print(f\"DokÅ‚adnoÅ›Ä‡ na zbiorze testowym: {test_accuracy_xgb:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059adbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorÃ³wnanie wynikÃ³w przed i po optymalizacji\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Model domyÅ›lny\n",
    "plot_decision_boundary(xgb_default, X_moons, y_moons, \n",
    "                      f'XGBoost - DomyÅ›lne parametry\\n(Acc: {accuracy_xgb:.3f})', ax1)\n",
    "\n",
    "# Model zoptymalizowany\n",
    "plot_decision_boundary(best_xgb, X_moons, y_moons, \n",
    "                      f'XGBoost - Zoptymalizowane parametry\\n(Acc: {test_accuracy_xgb:.3f})', ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67858312",
   "metadata": {},
   "source": [
    "## Analiza waÅ¼noÅ›ci cech w XGBoost\n",
    "\n",
    "XGBoost automatycznie oblicza waÅ¼noÅ›Ä‡ cech, co pomaga w interpretacji modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25fb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrzykÅ‚ad z wiÄ™kszÄ… liczbÄ… cech\n",
    "X_complex, y_complex = make_classification(n_samples=1000, n_features=10, \n",
    "                                         n_informative=5, n_redundant=2,\n",
    "                                         n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "feature_names = [f'Feature_{i+1}' for i in range(X_complex.shape[1])]\n",
    "\n",
    "# PodziaÅ‚ danych\n",
    "X_train_comp, X_test_comp, y_train_comp, y_test_comp = train_test_split(\n",
    "    X_complex, y_complex, test_size=0.3, random_state=42)\n",
    "\n",
    "# Trenowanie modelu\n",
    "xgb_complex = XGBClassifier(n_estimators=100, max_depth=4, \n",
    "                           learning_rate=0.1, random_state=42, eval_metric='logloss')\n",
    "xgb_complex.fit(X_train_comp, y_train_comp)\n",
    "\n",
    "# WaÅ¼noÅ›Ä‡ cech\n",
    "feature_importance = xgb_complex.feature_importances_\n",
    "\n",
    "# Wizualizacja waÅ¼noÅ›ci cech\n",
    "plt.figure(figsize=(10, 6))\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "plt.bar(range(len(feature_importance)), feature_importance[indices])\n",
    "plt.xlabel('Cechy')\n",
    "plt.ylabel('WaÅ¼noÅ›Ä‡')\n",
    "plt.title('WaÅ¼noÅ›Ä‡ cech w XGBoost')\n",
    "plt.xticks(range(len(feature_importance)), [feature_names[i] for i in indices], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Wydrukowanie waÅ¼noÅ›ci\n",
    "print(\"WaÅ¼noÅ›Ä‡ cech (posortowane):\")\n",
    "for i in indices:\n",
    "    print(f\"{feature_names[i]}: {feature_importance[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeb41ab",
   "metadata": {},
   "source": [
    "# PorÃ³wnanie SVM vs XGBoost\n",
    "\n",
    "\n",
    "## Kiedy uÅ¼ywaÄ‡ SVM:\n",
    "\n",
    "\n",
    "- **Dane o wysokiej wymiarowoÅ›ci** (np. text mining, image recognition)\n",
    "- **MaÅ‚e zbiory danych** (SVM dziaÅ‚a dobrze z mniejszÄ… iloÅ›ciÄ… danych)\n",
    "- **Potrzeba teoretycznego uzasadnienia** (silne fundamenty matematyczne)\n",
    "- **Problemy z wyraÅºnÄ… separacjÄ… klas**\n",
    "- **StabilnoÅ›Ä‡ wynikÃ³w** (deterministyczny)\n",
    "\n",
    "## Kiedy uÅ¼ywaÄ‡ XGBoost:\n",
    "\n",
    "\n",
    "- **DuÅ¼e zbiory danych** (skaluje siÄ™ lepiej)\n",
    "- **Dane tabelaryczne** (strukturalne, mixed features)\n",
    "- **Potrzeba interpretacji** (feature importance)\n",
    "- **Konkursy ML** (czÄ™sto wygrywa kaggle competitions)\n",
    "- **Automatic feature engineering** (moÅ¼e znajdowaÄ‡ interakcje)\n",
    "- **ObsÅ‚uga brakÃ³w danych** (native missing value handling)\n",
    "\n",
    "## Zestawienie wÅ‚aÅ›ciwoÅ›ci:\n",
    "\n",
    "| Cecha | SVM | XGBoost |\n",
    "|-------|-----|----------|\n",
    "| **Typ modelu** | Kernel-based | Tree ensemble |\n",
    "| **InterpretowalnoÅ›Ä‡** | Niska | Wysoka (feature importance) |\n",
    "| **SzybkoÅ›Ä‡ trenowania** | Wolna na duÅ¼ych danych | Szybka |\n",
    "| **OdpornoÅ›Ä‡ na overfitting** | Wysoka (z regularyzacjÄ…) | Åšrednia (wymaga tuningu) |\n",
    "| **ObsÅ‚uga brakÃ³w danych** | Nie | Tak |\n",
    "| **Hiperparametry** | MaÅ‚o, ale kluczowe | DuÅ¼o opcji do tuningu |\n",
    "| **PamiÄ™Ä‡** | Efektywna | MoÅ¼e byÄ‡ intensywna |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11972f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorÃ³wnanie czasÃ³w trenowania i dokÅ‚adnoÅ›ci\n",
    "import time\n",
    "\n",
    "# Przygotowanie danych rÃ³Å¼nej wielkoÅ›ci\n",
    "sizes = [100, 500, 1000, 2000]\n",
    "svm_times = []\n",
    "xgb_times = []\n",
    "svm_accuracies = []\n",
    "xgb_accuracies = []\n",
    "\n",
    "for size in sizes:\n",
    "    print(f\"\\nTestowanie na {size} prÃ³bkach...\")\n",
    "    \n",
    "    # Generowanie danych\n",
    "    X_comp, y_comp = make_classification(n_samples=size, n_features=10, \n",
    "                                       n_informative=7, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_comp, y_comp, \n",
    "                                                       test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Standardyzacja dla SVM\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Test SVM\n",
    "    start_time = time.time()\n",
    "    svm = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    svm_time = time.time() - start_time\n",
    "    svm_acc = svm.score(X_test_scaled, y_test)\n",
    "    \n",
    "    # Test XGBoost\n",
    "    start_time = time.time()\n",
    "    xgb_model = XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_time = time.time() - start_time\n",
    "    xgb_acc = xgb_model.score(X_test, y_test)\n",
    "    \n",
    "    svm_times.append(svm_time)\n",
    "    xgb_times.append(xgb_time)\n",
    "    svm_accuracies.append(svm_acc)\n",
    "    xgb_accuracies.append(xgb_acc)\n",
    "    \n",
    "    print(f\"SVM: {svm_time:.3f}s, Acc: {svm_acc:.3f}\")\n",
    "    print(f\"XGB: {xgb_time:.3f}s, Acc: {xgb_acc:.3f}\")\n",
    "\n",
    "# Wizualizacja porÃ³wnania\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Czasy trenowania\n",
    "ax1.plot(sizes, svm_times, 'o-', label='SVM', linewidth=2, markersize=8)\n",
    "ax1.plot(sizes, xgb_times, 's-', label='XGBoost', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Rozmiar zbioru danych')\n",
    "ax1.set_ylabel('Czas trenowania (s)')\n",
    "ax1.set_title('PorÃ³wnanie czasÃ³w trenowania')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# DokÅ‚adnoÅ›ci\n",
    "ax2.plot(sizes, svm_accuracies, 'o-', label='SVM', linewidth=2, markersize=8)\n",
    "ax2.plot(sizes, xgb_accuracies, 's-', label='XGBoost', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Rozmiar zbioru danych')\n",
    "ax2.set_ylabel('DokÅ‚adnoÅ›Ä‡')\n",
    "ax2.set_title('PorÃ³wnanie dokÅ‚adnoÅ›ci')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae7676",
   "metadata": {},
   "source": [
    "# Zadania praktyczne\n",
    "\n",
    "\n",
    "## Zadanie 1: SVM na rÃ³Å¼nych typach danych ðŸŽ¯\n",
    "\n",
    "\n",
    "1. **Wygeneruj 3 rÃ³Å¼ne typy syntetycznych danych**:\n",
    "   - Liniowo separowalne (`make_classification`)\n",
    "   - OkrÄ™gi (`make_circles`)\n",
    "   - PÃ³Å‚ksiÄ™Å¼yce (`make_moons`)\n",
    "\n",
    "2. **Dla kaÅ¼dego typu danych**:\n",
    "   - Przetestuj rÃ³Å¼ne kernele SVM (linear, rbf, poly)\n",
    "   - ZnajdÅº optymalne parametry uÅ¼ywajÄ…c GridSearchCV\n",
    "   - PorÃ³wnaj wyniki i zastanowi siÄ™, dlaczego niektÃ³re kernele dziaÅ‚ajÄ… lepiej\n",
    "\n",
    "**WskazÃ³wka**: UÅ¼yj funkcji `plot_decision_boundary` z notebooka!\n",
    "\n",
    "---\n",
    "\n",
    "## Zadanie 2: XGBoost - feature engineering ðŸ”§\n",
    "\n",
    "\n",
    "1. **ZaÅ‚aduj prawdziwy zbiÃ³r danych** (np. z sklearn.datasets)\n",
    "2. **StwÃ³rz nowe cechy przez**:\n",
    "   - Kombinacje liniowe istniejÄ…cych cech (`feature1 + feature2`)\n",
    "   - Interakcje miÄ™dzy cechami (`feature1 * feature2`)\n",
    "   - Transformacje nieliniowe (`log`, `sqrt`, `square`, itp.)\n",
    "3. **PorÃ³wnaj wyniki** przed i po feature engineering\n",
    "4. **Przeanalizuj feature importance** - ktÃ³re nowe cechy sÄ… najwaÅ¼niejsze?\n",
    "\n",
    "---\n",
    "\n",
    "## Zadanie 3: PorÃ³wnanie algorytmÃ³w âš”ï¸\n",
    "\n",
    "![Algorithm Comparison](https://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png)\n",
    "\n",
    "1. **Wybierz prawdziwy problem klasyfikacyjny**\n",
    "2. **Zaimplementuj pipeline dla obu algorytmÃ³w**:\n",
    "   ```python\n",
    "   # PrzykÅ‚ad pipeline\n",
    "   svm_pipeline = Pipeline([\n",
    "       ('scaler', StandardScaler()),\n",
    "       ('svm', SVC())\n",
    "   ])\n",
    "   \n",
    "   xgb_pipeline = Pipeline([\n",
    "       ('xgb', XGBClassifier())\n",
    "   ])\n",
    "   ```\n",
    "3. **PorÃ³wnaj**:\n",
    "   - DokÅ‚adnoÅ›Ä‡ (accuracy, precision, recall, F1)\n",
    "   - Czas trenowania\n",
    "   - InterpretacjÄ™ wynikÃ³w\n",
    "   - StabilnoÅ›Ä‡ wynikÃ³w (uruchom kilka razy)\n",
    "\n",
    "---\n",
    "\n",
    "## Zadanie 4: Analiza bÅ‚Ä™dÃ³w ðŸ”\n",
    "\n",
    "\n",
    "1. **Dla wybranego modelu przeanalizuj bÅ‚Ä™dy klasyfikacji**:\n",
    "   - StwÃ³rz confusion matrix\n",
    "   - ZnajdÅº false positives i false negatives\n",
    "   - Wizualizuj bÅ‚Ä™dnie sklasyfikowane prÃ³bki\n",
    "\n",
    "2. **SprawdÅº, czy istnieje wzÃ³r w bÅ‚Ä™dnie klasyfikowanych prÃ³bkach**:\n",
    "   - Czy majÄ… podobne wartoÅ›ci cech?\n",
    "   - Czy sÄ… outlierami?\n",
    "   - Czy znajdujÄ… siÄ™ blisko granicy decyzyjnej?\n",
    "\n",
    "3. **Zaproponuj sposoby poprawy modelu**:\n",
    "   - Feature engineering\n",
    "   - Zmiana hiperparametrÃ³w\n",
    "   - UsuniÄ™cie outlierÃ³w\n",
    "   - Zbieranie wiÄ™cej danych\n",
    "\n",
    "---\n",
    "\n",
    "## Zadanie bonusowe: Ensemble obu algorytmÃ³w ðŸŽ†\n",
    "\n",
    "\n",
    "**SprÃ³buj poÅ‚Ä…czyÄ‡ SVM i XGBoost** w jedno rozwiÄ…zanie:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('svm', svm_pipeline), ('xgb', xgb_pipeline)],\n",
    "    voting='soft'  # lub 'hard'\n",
    ")\n",
    "```\n",
    "\n",
    "**SprawdÅº, czy ensemble daje lepsze wyniki niÅ¼ pojedyncze modele!**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ WskazÃ³wki do wszystkich zadaÅ„:\n",
    "\n",
    "1. **Dokumentuj swoje eksperymenty** - zapisuj parametry i wyniki\n",
    "2. **Wizualizuj wyniki** - uÅ¼yj matplotlib i seaborn\n",
    "3. **Interpretuj wyniki** - nie tylko patrzeÄ‡ na accuracy\n",
    "4. **Testuj na rÃ³Å¼nych zbiorach danych** - kaÅ¼dy algoritm ma swoje mocne strony\n",
    "5. **Mierz czas wykonania** - w praktyce to czÄ™sto kluczowe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c2a1f",
   "metadata": {},
   "source": [
    "# Podsumowanie\n",
    "\n",
    "![ML Algorithms Comparison](https://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png)\n",
    "\n",
    "## SVM - Kluczowe punkty:\n",
    "\n",
    "![SVM Summary](https://scikit-learn.org/stable/_images/sphx_glr_plot_svm_margin_001.png)\n",
    "\n",
    "- **Znajduje optymalnÄ… granicÄ™** maksymalizujÄ…c margines\n",
    "- **Kernele** umoÅ¼liwiajÄ… modelowanie nieliniowych zaleÅ¼noÅ›ci\n",
    "- **Parametr C** kontroluje trade-off miÄ™dzy marginesem a bÅ‚Ä™dami\n",
    "- **Parametr gamma (RBF)** kontroluje zÅ‚oÅ¼onoÅ›Ä‡ granicy decyzyjnej\n",
    "- **DziaÅ‚a dobrze** na wysokowymiarowych danych\n",
    "- **Deterministyczny** - zawsze te same wyniki dla tych samych danych\n",
    "\n",
    "## XGBoost - Kluczowe punkty:\n",
    "![XGBoost Summary ](https://miro.medium.com/v2/resize:fit:1400/1*Sc1OVfgV_jYTIDltlmirIw.png)\n",
    "\n",
    "\n",
    "- **Buduje model sekwencyjnie**, poprawiajÄ…c bÅ‚Ä™dy poprzednikÃ³w\n",
    "- **Wiele mechanizmÃ³w regularyzacji** przeciwdziaÅ‚ajÄ…cych overfitting\n",
    "- **Automatycznie obsÅ‚uguje braki danych**\n",
    "- **Zapewnia waÅ¼noÅ›Ä‡ cech** pomagajÄ…cÄ… w interpretacji\n",
    "- **Skalowalny i efektywny** na duÅ¼ych zbiorach danych\n",
    "- **Elastyczny** - wiele opcji tuningu i customizacji\n",
    "\n",
    "## WybÃ³r algorytmu - Praktyczne wskazÃ³wki:\n",
    "\n",
    "### UÅ¼yj **SVM** gdy:\n",
    "âœ… MaÅ‚e/Å›rednie dane (< 10k prÃ³bek)  \n",
    "âœ… Wysokie wymiary (> 1000 cech)  \n",
    "âœ… Potrzebujesz stabilnych, powtarzalnych wynikÃ³w  \n",
    "âœ… Dane sÄ… czyste (bez brakÃ³w)  \n",
    "âœ… Teoretyczne uzasadnienie jest waÅ¼ne  \n",
    "\n",
    "### UÅ¼yj **XGBoost** gdy:\n",
    "âœ… DuÅ¼e dane tabelaryczne (> 10k prÃ³bek)  \n",
    "âœ… Potrzebujesz interpretacji (feature importance)  \n",
    "âœ… Dane majÄ… braki lub sÄ… zaszumione  \n",
    "âœ… Chcesz szybkie prototypowanie  \n",
    "âœ… Uczestniczysz w konkursach ML  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**PamiÄ™taj**: Nie ma jednego \"najlepszego\" algorytmu. SkutecznoÅ›Ä‡ zaleÅ¼y od konkretnego problemu, danych i kontekstu biznesowego. Najlepsze podejÅ›cie to przetestowanie kilku algorytmÃ³w i wybÃ³r tego, ktÃ³ry najlepiej speÅ‚nia Twoje wymagania pod wzglÄ™dem dokÅ‚adnoÅ›ci, interpretowalnoÅ›ci i efektywnoÅ›ci obliczeniowej."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
