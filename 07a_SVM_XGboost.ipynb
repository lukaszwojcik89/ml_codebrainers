{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8c915e",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM) i XGBoost \n",
    "\n",
    "\n",
    "\n",
    "## Wprowadzenie\n",
    "\n",
    "W tym notebooku poznamy dwa potężne algorytmy uczenia maszynowego:\n",
    "\n",
    "### Support Vector Machines (SVM)\n",
    "![SVM Concept](https://scikit-learn.org/stable/_images/sphx_glr_plot_svm_margin_001.png)\n",
    "\n",
    "**SVM** to algorytm znajdowania optymalnej granicy decyzyjnej, który maksymalizuje margines między klasami.\n",
    "\n",
    "### XGBoost\n",
    "![XGBoost Concept](https://aiml.com/wp-content/uploads/2024/06/evolution-of-boosting.png)\n",
    "\n",
    "**XGBoost** to zaawansowana implementacja gradient boosting, która buduje model z wielu słabych uczniów.\n",
    "\n",
    "---\n",
    "\n",
    "Oba algorytmy mają różne podejścia do rozwiązywania problemów klasyfikacji i regresji, co czyni je komplementarnymi narzędziami w arsenale data scientist'a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b113dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import niezbędnych bibliotek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_circles, make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ustawienia wykresów\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49d94f",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "## Teoria i podstawy\n",
    "\n",
    "![SVM Visualization](https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/512px-SVM_margin.png)\n",
    "\n",
    "**Support Vector Machine (SVM)** to algorytm nadzorowanego uczenia maszynowego, który:\n",
    "- Szuka **hiperplanu** (granicy decyzyjnej) najlepiej oddzielającego klasy\n",
    "- **Maksymalizuje margines** - odległość od najbliższych punktów każdej klasy\n",
    "- Może tworzyć **nieliniowe granice** dzięki funkcjom jądra (kernelom)\n",
    "\n",
    "### Kluczowe koncepcje:\n",
    "- **Wektory nośne (Support Vectors)** - punkty najbliższe granicy decyzyjnej\n",
    "- **Margines** - \"šszerokość\" strefy separującej klasy\n",
    "- **Parametr C** - kontroluje trade-off między maksymalizacją marginesu a minimalizacją błędów\n",
    "- **Kernele** - umożliwiają modelowanie nieliniowych zależności\n",
    "\n",
    "### Typy kerneli:\n",
    "![Kernel Types](https://scikit-learn.org/stable/_images/sphx_glr_plot_svm_kernels_001.png)\n",
    "\n",
    "1. **Linear Kernel** - dla danych liniowo separowalnych\n",
    "2. **RBF (Radial Basis Function)** - najczęściej używany dla danych nieliniowych\n",
    "3. **Polynomial Kernel** - dla złożonych zależności wielomianowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003faaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do wizualizacji granic decyzyjnych\n",
    "def plot_decision_boundary(model, X, y, title=\"\", ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Tworzenie siatki punktów\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predykcja dla każdego punktu siatki\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Rysowanie granicy decyzyjnej\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
    "    \n",
    "    # Rysowanie punktów danych\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b101e",
   "metadata": {},
   "source": [
    "## Przykład 1: SVM na danych liniowo separowalnych\n",
    "\n",
    "Zaczniemy od prostego przykładu z danymi, które można rozdzielić linią prostą."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197db4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generowanie danych liniowo separowalnych\n",
    "np.random.seed(42)\n",
    "X_linear, y_linear = make_classification(n_samples=100, n_features=2, n_redundant=0, \n",
    "                                       n_informative=2, n_clusters_per_class=1, \n",
    "                                       random_state=42)\n",
    "\n",
    "# Podział na zbiory treningowy i testowy\n",
    "X_train_lin, X_test_lin, y_train_lin, y_test_lin = train_test_split(\n",
    "    X_linear, y_linear, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardyzacja danych\n",
    "scaler = StandardScaler()\n",
    "X_train_lin_scaled = scaler.fit_transform(X_train_lin)\n",
    "X_test_lin_scaled = scaler.transform(X_test_lin)\n",
    "\n",
    "print(f\"Rozmiar zbioru treningowego: {X_train_lin_scaled.shape}\")\n",
    "print(f\"Rozmiar zbioru testowego: {X_test_lin_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec8c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trenowanie SVM z kernelem liniowym\n",
    "svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_linear.fit(X_train_lin_scaled, y_train_lin)\n",
    "\n",
    "# Predykcja i ocena\n",
    "y_pred_lin = svm_linear.predict(X_test_lin_scaled)\n",
    "accuracy_lin = accuracy_score(y_test_lin, y_pred_lin)\n",
    "\n",
    "print(f\"Dokładność SVM liniowego: {accuracy_lin:.3f}\")\n",
    "\n",
    "# Wizualizacja\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Dane oryginalne\n",
    "ax1.scatter(X_linear[:, 0], X_linear[:, 1], c=y_linear, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "ax1.set_title('Dane oryginalne')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "\n",
    "# Granica decyzyjna\n",
    "plot_decision_boundary(svm_linear, X_train_lin_scaled, y_train_lin, \n",
    "                      'SVM Linear - Granica decyzyjna', ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a4757",
   "metadata": {},
   "source": [
    "## Przykład 2: SVM na danych nieliniowo separowalnych\n",
    "\n",
    "Teraz sprawdzimy, jak SVM radzi sobie z danymi, które nie mogą być rozdzielone linią prostą. Użyjemy danych w kształcie okręgów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80943ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generowanie danych w kształcie okręgów (nieliniowo separowalne)\n",
    "X_circles, y_circles = make_circles(n_samples=300, factor=0.5, noise=0.1, random_state=42)\n",
    "\n",
    "# Podział na zbiory\n",
    "X_train_circ, X_test_circ, y_train_circ, y_test_circ = train_test_split(\n",
    "    X_circles, y_circles, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Rozmiar zbioru treningowego: {X_train_circ.shape}\")\n",
    "print(f\"Rozmiar zbioru testowego: {X_test_circ.shape}\")\n",
    "\n",
    "# Wizualizacja danych\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "plt.title('Dane w kształcie okręgów')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca93c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porównanie różnych kerneli SVM\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_value = 1.0\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, kernel in enumerate(kernels):\n",
    "    # Trenowanie modelu\n",
    "    if kernel == 'poly':\n",
    "        svm = SVC(kernel=kernel, C=C_value, degree=3, random_state=42)\n",
    "    else:\n",
    "        svm = SVC(kernel=kernel, C=C_value, random_state=42)\n",
    "    \n",
    "    svm.fit(X_train_circ, y_train_circ)\n",
    "    \n",
    "    # Predykcja i ocena\n",
    "    y_pred = svm.predict(X_test_circ)\n",
    "    accuracy = accuracy_score(y_test_circ, y_pred)\n",
    "    \n",
    "    # Wizualizacja\n",
    "    plot_decision_boundary(svm, X_circles, y_circles, \n",
    "                          f'SVM {kernel.capitalize()} (Acc: {accuracy:.3f})', axes[i])\n",
    "    \n",
    "    print(f\"Dokładność SVM {kernel}: {accuracy:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d1decc",
   "metadata": {},
   "source": [
    "## Wpływ hiperparametrów SVM\n",
    "\n",
    "![SVM Parameters](https://scikit-learn.org/stable/_images/sphx_glr_plot_rbf_parameters_001.png)\n",
    "\n",
    "### Parametr C (Regularyzacja)\n",
    "- **Wysokie C**: Model stara się klasyfikować wszystkie punkty poprawnie → może prowadzić do overfitting\n",
    "- **Niskie C**: Model akceptuje więcej błędów, ale ma szerszy margines → może prowadzić do underfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04fdade",
   "metadata": {},
   "source": [
    "\n",
    "### Parametr gamma (dla kernela RBF)\n",
    "- **Wysokie gamma**: Wpływ pojedynczego punktu jest bardzo lokalny → skomplikowane granice\n",
    "- **Niskie gamma**: Wpływ punktów jest szerszy → gładsze granice\n",
    "\n",
    "![Gamma Effect](https://miro.medium.com/v2/resize:fit:1400/1*vmLmExiS9n5pY4Xy9Z516g.png)\n",
    "\n",
    "### Zasady doboru parametrów:\n",
    "1. **Małe zbiory danych** → niższe C, wyższe gamma\n",
    "2. **Duże zbiory danych** → wyższe C, niższe gamma\n",
    "3. **Szumy w danych** → niższe C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a956fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Badanie wpływu parametru C\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, C in enumerate(C_values):\n",
    "    svm = SVC(kernel='rbf', C=C, random_state=42)\n",
    "    svm.fit(X_train_circ, y_train_circ)\n",
    "    \n",
    "    y_pred = svm.predict(X_test_circ)\n",
    "    accuracy = accuracy_score(y_test_circ, y_pred)\n",
    "    \n",
    "    plot_decision_boundary(svm, X_circles, y_circles, \n",
    "                          f'SVM RBF (C={C}, Acc: {accuracy:.3f})', axes[i])\n",
    "    \n",
    "    print(f\"C={C}: Dokładność = {accuracy:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b725c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Badanie wpływu parametru gamma\n",
    "gamma_values = [0.1, 1, 10, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, gamma in enumerate(gamma_values):\n",
    "    svm = SVC(kernel='rbf', C=1, gamma=gamma, random_state=42)\n",
    "    svm.fit(X_train_circ, y_train_circ)\n",
    "    \n",
    "    y_pred = svm.predict(X_test_circ)\n",
    "    accuracy = accuracy_score(y_test_circ, y_pred)\n",
    "    \n",
    "    plot_decision_boundary(svm, X_circles, y_circles, \n",
    "                          f'SVM RBF (γ={gamma}, Acc: {accuracy:.3f})', axes[i])\n",
    "    \n",
    "    print(f\"gamma={gamma}: Dokładność = {accuracy:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0756eec",
   "metadata": {},
   "source": [
    "## Optymalizacja hiperparametrów SVM\n",
    "\n",
    "Użyjemy Grid Search do znalezienia optymalnych parametrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c88cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search dla SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "svm_grid = SVC(random_state=42)\n",
    "grid_search = GridSearchCV(svm_grid, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_circ, y_train_circ)\n",
    "\n",
    "print(\"Najlepsze parametry:\", grid_search.best_params_)\n",
    "print(\"Najlepszy wynik CV:\", grid_search.best_score_)\n",
    "\n",
    "# Ocena na zbiorze testowym\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred_best = best_svm.predict(X_test_circ)\n",
    "test_accuracy = accuracy_score(y_test_circ, y_pred_best)\n",
    "print(f\"Dokładność na zbiorze testowym: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wizualizacja wyników Grid Search\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Tworzenie macierzy wyników\n",
    "C_vals = param_grid['C']\n",
    "gamma_vals = param_grid['gamma']\n",
    "scores = np.zeros((len(gamma_vals), len(C_vals)))\n",
    "\n",
    "for i, gamma in enumerate(gamma_vals):\n",
    "    for j, C in enumerate(C_vals):\n",
    "        # Znajdź wynik dla tej kombinacji\n",
    "        for result in grid_search.cv_results_['params']:\n",
    "            if result['C'] == C and result['gamma'] == gamma:\n",
    "                idx = grid_search.cv_results_['params'].index(result)\n",
    "                scores[i, j] = grid_search.cv_results_['mean_test_score'][idx]\n",
    "                break\n",
    "\n",
    "# Heatmapa\n",
    "sns.heatmap(scores, annot=True, fmt='.3f', \n",
    "            xticklabels=[f'C={c}' for c in C_vals],\n",
    "            yticklabels=[f'γ={g}' for g in gamma_vals],\n",
    "            cmap='viridis')\n",
    "plt.title('Grid Search Results - Średnia dokładność CV')\n",
    "plt.xlabel('Parametr C')\n",
    "plt.ylabel('Parametr gamma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8fb1f",
   "metadata": {},
   "source": [
    "![Boosting Process](https://cdn.corporatefinanceinstitute.com/assets/boosting1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c99bcca",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### XGBoost - ulepszenia:\n",
    "\n",
    "\n",
    "- **Wydajność**: Optymalizacje algorytmiczne i możliwość przetwarzania równoległego\n",
    "- **Regularyzacja**: Wbudowane mechanizmy przeciwdziałające overfitting\n",
    "- **Elastyczność**: Obsługa różnych typów problemów i metryk\n",
    "- **Braki danych**: Automatyczne radzenie sobie z missing values\n",
    "\n",
    "### Algorytm krok po kroku:\n",
    "1. **Inicjalizacja**: Zacznij z prostym modelem (np. średnia)\n",
    "2. **Oblicz residua**: Znajdź błędy obecnego modelu\n",
    "3. **Trenuj nowe drzewo**: Na residuach poprzedniego kroku\n",
    "4. **Dodaj do ensemble**: Z odpowiednią wagą (learning_rate)\n",
    "5. **Powtórz**: Aż do osiągnięcia założonej liczby drzew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cabeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład z danymi \"moons\" dla XGBoost\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "\n",
    "# Podział na zbiory\n",
    "X_train_moons, X_test_moons, y_train_moons, y_test_moons = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.3, random_state=42)\n",
    "\n",
    "# Wizualizacja danych\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "plt.title('Dane \"moons\"')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Rozmiar zbioru treningowego: {X_train_moons.shape}\")\n",
    "print(f\"Rozmiar zbioru testowego: {X_test_moons.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005be441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trenowanie XGBoost z domyślnymi parametrami\n",
    "xgb_default = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "xgb_default.fit(X_train_moons, y_train_moons)\n",
    "\n",
    "# Predykcja i ocena\n",
    "y_pred_xgb = xgb_default.predict(X_test_moons)\n",
    "accuracy_xgb = accuracy_score(y_test_moons, y_pred_xgb)\n",
    "\n",
    "print(f\"Dokładność XGBoost (domyślne): {accuracy_xgb:.3f}\")\n",
    "\n",
    "# Wizualizacja\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_boundary(xgb_default, X_moons, y_moons, \n",
    "                      f'XGBoost - Granica decyzyjna (Acc: {accuracy_xgb:.3f})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27111ca",
   "metadata": {},
   "source": [
    "## Kluczowe hiperparametry XGBoost\n",
    "\n",
    "\n",
    "### Parametry sterujące złożonością modelu:\n",
    "- **n_estimators**: Liczba drzew w ensemble\n",
    "- **max_depth**: Maksymalna głębokość pojedynczego drzewa\n",
    "- **learning_rate (eta)**: Tempo uczenia - skaluje wkład każdego drzewa\n",
    "\n",
    "### Wpływ max_depth:\n",
    "\n",
    "- **Głębokie drzewa (depth > 6)**: Mogą prowadzić do overfitting\n",
    "- **Płytkie drzewa (depth < 3)**: Mogą prowadzić do underfitting\n",
    "- **Optymalne (depth 3-6)**: Dobry balans bias-variance\n",
    "\n",
    "### Parametry regularyzacji:\n",
    "- **subsample**: Frakcja próbek używana do trenowania każdego drzewa\n",
    "- **colsample_bytree**: Frakcja cech używana do trenowania każdego drzewa\n",
    "- **reg_alpha**: Regularyzacja L1\n",
    "- **reg_lambda**: Regularyzacja L2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a8b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Badanie wpływu max_depth\n",
    "max_depths = [1, 3, 6, 10]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, max_depth in enumerate(max_depths):\n",
    "    xgb_model = XGBClassifier(max_depth=max_depth, n_estimators=100, \n",
    "                             learning_rate=0.1, random_state=42, eval_metric='logloss')\n",
    "    xgb_model.fit(X_train_moons, y_train_moons)\n",
    "    \n",
    "    # Ocena na zbiorach treningowym i testowym\n",
    "    train_acc = xgb_model.score(X_train_moons, y_train_moons)\n",
    "    test_acc = xgb_model.score(X_test_moons, y_test_moons)\n",
    "    \n",
    "    plot_decision_boundary(xgb_model, X_moons, y_moons, \n",
    "                          f'XGBoost (depth={max_depth})\\nTrain: {train_acc:.3f}, Test: {test_acc:.3f}', \n",
    "                          axes[i])\n",
    "    \n",
    "    print(f\"max_depth={max_depth}: Train={train_acc:.3f}, Test={test_acc:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb508d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Badanie wpływu learning_rate\n",
    "learning_rates = [0.01, 0.1, 0.3, 1.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    xgb_model = XGBClassifier(learning_rate=lr, n_estimators=100, \n",
    "                             max_depth=3, random_state=42, eval_metric='logloss')\n",
    "    xgb_model.fit(X_train_moons, y_train_moons)\n",
    "    \n",
    "    train_acc = xgb_model.score(X_train_moons, y_train_moons)\n",
    "    test_acc = xgb_model.score(X_test_moons, y_test_moons)\n",
    "    \n",
    "    plot_decision_boundary(xgb_model, X_moons, y_moons, \n",
    "                          f'XGBoost (lr={lr})\\nTrain: {train_acc:.3f}, Test: {test_acc:.3f}', \n",
    "                          axes[i])\n",
    "    \n",
    "    print(f\"learning_rate={lr}: Train={train_acc:.3f}, Test={test_acc:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457b023",
   "metadata": {},
   "source": [
    "## Optymalizacja hiperparametrów XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f63ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search dla XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "grid_search_xgb = GridSearchCV(xgb_grid, param_grid_xgb, cv=5, \n",
    "                              scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Rozpoczynam Grid Search dla XGBoost...\")\n",
    "grid_search_xgb.fit(X_train_moons, y_train_moons)\n",
    "\n",
    "print(\"Najlepsze parametry XGBoost:\", grid_search_xgb.best_params_)\n",
    "print(\"Najlepszy wynik CV:\", grid_search_xgb.best_score_)\n",
    "\n",
    "# Ocena na zbiorze testowym\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "y_pred_best_xgb = best_xgb.predict(X_test_moons)\n",
    "test_accuracy_xgb = accuracy_score(y_test_moons, y_pred_best_xgb)\n",
    "print(f\"Dokładność na zbiorze testowym: {test_accuracy_xgb:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059adbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porównanie wyników przed i po optymalizacji\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Model domyślny\n",
    "plot_decision_boundary(xgb_default, X_moons, y_moons, \n",
    "                      f'XGBoost - Domyślne parametry\\n(Acc: {accuracy_xgb:.3f})', ax1)\n",
    "\n",
    "# Model zoptymalizowany\n",
    "plot_decision_boundary(best_xgb, X_moons, y_moons, \n",
    "                      f'XGBoost - Zoptymalizowane parametry\\n(Acc: {test_accuracy_xgb:.3f})', ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67858312",
   "metadata": {},
   "source": [
    "## Analiza ważności cech w XGBoost\n",
    "\n",
    "XGBoost automatycznie oblicza ważność cech, co pomaga w interpretacji modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25fb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład z większą liczbą cech\n",
    "X_complex, y_complex = make_classification(n_samples=1000, n_features=10, \n",
    "                                         n_informative=5, n_redundant=2,\n",
    "                                         n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "feature_names = [f'Feature_{i+1}' for i in range(X_complex.shape[1])]\n",
    "\n",
    "# Podział danych\n",
    "X_train_comp, X_test_comp, y_train_comp, y_test_comp = train_test_split(\n",
    "    X_complex, y_complex, test_size=0.3, random_state=42)\n",
    "\n",
    "# Trenowanie modelu\n",
    "xgb_complex = XGBClassifier(n_estimators=100, max_depth=4, \n",
    "                           learning_rate=0.1, random_state=42, eval_metric='logloss')\n",
    "xgb_complex.fit(X_train_comp, y_train_comp)\n",
    "\n",
    "# Ważność cech\n",
    "feature_importance = xgb_complex.feature_importances_\n",
    "\n",
    "# Wizualizacja ważności cech\n",
    "plt.figure(figsize=(10, 6))\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "plt.bar(range(len(feature_importance)), feature_importance[indices])\n",
    "plt.xlabel('Cechy')\n",
    "plt.ylabel('Ważność')\n",
    "plt.title('Ważność cech w XGBoost')\n",
    "plt.xticks(range(len(feature_importance)), [feature_names[i] for i in indices], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Wydrukowanie ważności\n",
    "print(\"Ważność cech (posortowane):\")\n",
    "for i in indices:\n",
    "    print(f\"{feature_names[i]}: {feature_importance[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeb41ab",
   "metadata": {},
   "source": [
    "# Porównanie SVM vs XGBoost\n",
    "\n",
    "\n",
    "## Kiedy używać SVM:\n",
    "\n",
    "\n",
    "- **Dane o wysokiej wymiarowości** (np. text mining, image recognition)\n",
    "- **Małe zbiory danych** (SVM działa dobrze z mniejszą ilością danych)\n",
    "- **Potrzeba teoretycznego uzasadnienia** (silne fundamenty matematyczne)\n",
    "- **Problemy z wyraźną separacją klas**\n",
    "- **Stabilność wyników** (deterministyczny)\n",
    "\n",
    "## Kiedy używać XGBoost:\n",
    "\n",
    "\n",
    "- **Duże zbiory danych** (skaluje się lepiej)\n",
    "- **Dane tabelaryczne** (strukturalne, mixed features)\n",
    "- **Potrzeba interpretacji** (feature importance)\n",
    "- **Konkursy ML** (często wygrywa kaggle competitions)\n",
    "- **Automatic feature engineering** (może znajdować interakcje)\n",
    "- **Obsługa braków danych** (native missing value handling)\n",
    "\n",
    "## Zestawienie właściwości:\n",
    "\n",
    "| Cecha | SVM | XGBoost |\n",
    "|-------|-----|----------|\n",
    "| **Typ modelu** | Kernel-based | Tree ensemble |\n",
    "| **Interpretowalność** | Niska | Wysoka (feature importance) |\n",
    "| **Szybkość trenowania** | Wolna na dużych danych | Szybka |\n",
    "| **Odporność na overfitting** | Wysoka (z regularyzacją) | Średnia (wymaga tuningu) |\n",
    "| **Obsługa braków danych** | Nie | Tak |\n",
    "| **Hiperparametry** | Mało, ale kluczowe | Dużo opcji do tuningu |\n",
    "| **Pamięć** | Efektywna | Może być intensywna |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11972f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porównanie czasów trenowania i dokładności\n",
    "import time\n",
    "\n",
    "# Przygotowanie danych różnej wielkości\n",
    "sizes = [100, 500, 1000, 2000]\n",
    "svm_times = []\n",
    "xgb_times = []\n",
    "svm_accuracies = []\n",
    "xgb_accuracies = []\n",
    "\n",
    "for size in sizes:\n",
    "    print(f\"\\nTestowanie na {size} próbkach...\")\n",
    "    \n",
    "    # Generowanie danych\n",
    "    X_comp, y_comp = make_classification(n_samples=size, n_features=10, \n",
    "                                       n_informative=7, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_comp, y_comp, \n",
    "                                                       test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Standardyzacja dla SVM\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Test SVM\n",
    "    start_time = time.time()\n",
    "    svm = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    svm_time = time.time() - start_time\n",
    "    svm_acc = svm.score(X_test_scaled, y_test)\n",
    "    \n",
    "    # Test XGBoost\n",
    "    start_time = time.time()\n",
    "    xgb_model = XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_time = time.time() - start_time\n",
    "    xgb_acc = xgb_model.score(X_test, y_test)\n",
    "    \n",
    "    svm_times.append(svm_time)\n",
    "    xgb_times.append(xgb_time)\n",
    "    svm_accuracies.append(svm_acc)\n",
    "    xgb_accuracies.append(xgb_acc)\n",
    "    \n",
    "    print(f\"SVM: {svm_time:.3f}s, Acc: {svm_acc:.3f}\")\n",
    "    print(f\"XGB: {xgb_time:.3f}s, Acc: {xgb_acc:.3f}\")\n",
    "\n",
    "# Wizualizacja porównania\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Czasy trenowania\n",
    "ax1.plot(sizes, svm_times, 'o-', label='SVM', linewidth=2, markersize=8)\n",
    "ax1.plot(sizes, xgb_times, 's-', label='XGBoost', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Rozmiar zbioru danych')\n",
    "ax1.set_ylabel('Czas trenowania (s)')\n",
    "ax1.set_title('Porównanie czasów trenowania')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Dokładności\n",
    "ax2.plot(sizes, svm_accuracies, 'o-', label='SVM', linewidth=2, markersize=8)\n",
    "ax2.plot(sizes, xgb_accuracies, 's-', label='XGBoost', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Rozmiar zbioru danych')\n",
    "ax2.set_ylabel('Dokładność')\n",
    "ax2.set_title('Porównanie dokładności')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae7676",
   "metadata": {},
   "source": [
    "# Zadania praktyczne\n",
    "\n",
    "\n",
    "## Zadanie 1: SVM na różnych typach danych 🎯\n",
    "\n",
    "\n",
    "1. **Wygeneruj 3 różne typy syntetycznych danych**:\n",
    "   - Liniowo separowalne (`make_classification`)\n",
    "   - Okręgi (`make_circles`)\n",
    "   - Półksiężyce (`make_moons`)\n",
    "\n",
    "2. **Dla każdego typu danych**:\n",
    "   - Przetestuj różne kernele SVM (linear, rbf, poly)\n",
    "   - Znajdź optymalne parametry używając GridSearchCV\n",
    "   - Porównaj wyniki i zastanowi się, dlaczego niektóre kernele działają lepiej\n",
    "\n",
    "**Wskazówka**: Użyj funkcji `plot_decision_boundary` z notebooka!\n",
    "\n",
    "---\n",
    "\n",
    "## Zadanie 2: XGBoost - feature engineering 🔧\n",
    "\n",
    "\n",
    "1. **Załaduj prawdziwy zbiór danych** (np. z sklearn.datasets)\n",
    "2. **Stwórz nowe cechy przez**:\n",
    "   - Kombinacje liniowe istniejących cech (`feature1 + feature2`)\n",
    "   - Interakcje między cechami (`feature1 * feature2`)\n",
    "   - Transformacje nieliniowe (`log`, `sqrt`, `square`, itp.)\n",
    "3. **Porównaj wyniki** przed i po feature engineering\n",
    "4. **Przeanalizuj feature importance** - które nowe cechy są najważniejsze?\n",
    "\n",
    "---\n",
    "\n",
    "## Zadanie 3: Porównanie algorytmów ⚔️\n",
    "\n",
    "![Algorithm Comparison](https://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png)\n",
    "\n",
    "1. **Wybierz prawdziwy problem klasyfikacyjny**\n",
    "2. **Zaimplementuj pipeline dla obu algorytmów**:\n",
    "   ```python\n",
    "   # Przykład pipeline\n",
    "   svm_pipeline = Pipeline([\n",
    "       ('scaler', StandardScaler()),\n",
    "       ('svm', SVC())\n",
    "   ])\n",
    "   \n",
    "   xgb_pipeline = Pipeline([\n",
    "       ('xgb', XGBClassifier())\n",
    "   ])\n",
    "   ```\n",
    "3. **Porównaj**:\n",
    "   - Dokładność (accuracy, precision, recall, F1)\n",
    "   - Czas trenowania\n",
    "   - Interpretację wyników\n",
    "   - Stabilność wyników (uruchom kilka razy)\n",
    "\n",
    "---\n",
    "\n",
    "## Zadanie 4: Analiza błędów 🔍\n",
    "\n",
    "\n",
    "1. **Dla wybranego modelu przeanalizuj błędy klasyfikacji**:\n",
    "   - Stwórz confusion matrix\n",
    "   - Znajdź false positives i false negatives\n",
    "   - Wizualizuj błędnie sklasyfikowane próbki\n",
    "\n",
    "2. **Sprawdź, czy istnieje wzór w błędnie klasyfikowanych próbkach**:\n",
    "   - Czy mają podobne wartości cech?\n",
    "   - Czy są outlierami?\n",
    "   - Czy znajdują się blisko granicy decyzyjnej?\n",
    "\n",
    "3. **Zaproponuj sposoby poprawy modelu**:\n",
    "   - Feature engineering\n",
    "   - Zmiana hiperparametrów\n",
    "   - Usunięcie outlierów\n",
    "   - Zbieranie więcej danych\n",
    "\n",
    "---\n",
    "\n",
    "## Zadanie bonusowe: Ensemble obu algorytmów 🎆\n",
    "\n",
    "\n",
    "**Spróbuj połączyć SVM i XGBoost** w jedno rozwiązanie:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('svm', svm_pipeline), ('xgb', xgb_pipeline)],\n",
    "    voting='soft'  # lub 'hard'\n",
    ")\n",
    "```\n",
    "\n",
    "**Sprawdź, czy ensemble daje lepsze wyniki niż pojedyncze modele!**\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Wskazówki do wszystkich zadań:\n",
    "\n",
    "1. **Dokumentuj swoje eksperymenty** - zapisuj parametry i wyniki\n",
    "2. **Wizualizuj wyniki** - użyj matplotlib i seaborn\n",
    "3. **Interpretuj wyniki** - nie tylko patrzeć na accuracy\n",
    "4. **Testuj na różnych zbiorach danych** - każdy algoritm ma swoje mocne strony\n",
    "5. **Mierz czas wykonania** - w praktyce to często kluczowe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c2a1f",
   "metadata": {},
   "source": [
    "# Podsumowanie\n",
    "\n",
    "![ML Algorithms Comparison](https://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png)\n",
    "\n",
    "## SVM - Kluczowe punkty:\n",
    "\n",
    "![SVM Summary](https://scikit-learn.org/stable/_images/sphx_glr_plot_svm_margin_001.png)\n",
    "\n",
    "- **Znajduje optymalną granicę** maksymalizując margines\n",
    "- **Kernele** umożliwiają modelowanie nieliniowych zależności\n",
    "- **Parametr C** kontroluje trade-off między marginesem a błędami\n",
    "- **Parametr gamma (RBF)** kontroluje złożoność granicy decyzyjnej\n",
    "- **Działa dobrze** na wysokowymiarowych danych\n",
    "- **Deterministyczny** - zawsze te same wyniki dla tych samych danych\n",
    "\n",
    "## XGBoost - Kluczowe punkty:\n",
    "![XGBoost Summary ](https://miro.medium.com/v2/resize:fit:1400/1*Sc1OVfgV_jYTIDltlmirIw.png)\n",
    "\n",
    "\n",
    "- **Buduje model sekwencyjnie**, poprawiając błędy poprzedników\n",
    "- **Wiele mechanizmów regularyzacji** przeciwdziałających overfitting\n",
    "- **Automatycznie obsługuje braki danych**\n",
    "- **Zapewnia ważność cech** pomagającą w interpretacji\n",
    "- **Skalowalny i efektywny** na dużych zbiorach danych\n",
    "- **Elastyczny** - wiele opcji tuningu i customizacji\n",
    "\n",
    "## Wybór algorytmu - Praktyczne wskazówki:\n",
    "\n",
    "### Użyj **SVM** gdy:\n",
    "✅ Małe/średnie dane (< 10k próbek)  \n",
    "✅ Wysokie wymiary (> 1000 cech)  \n",
    "✅ Potrzebujesz stabilnych, powtarzalnych wyników  \n",
    "✅ Dane są czyste (bez braków)  \n",
    "✅ Teoretyczne uzasadnienie jest ważne  \n",
    "\n",
    "### Użyj **XGBoost** gdy:\n",
    "✅ Duże dane tabelaryczne (> 10k próbek)  \n",
    "✅ Potrzebujesz interpretacji (feature importance)  \n",
    "✅ Dane mają braki lub są zaszumione  \n",
    "✅ Chcesz szybkie prototypowanie  \n",
    "✅ Uczestniczysz w konkursach ML  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Pamiętaj**: Nie ma jednego \"najlepszego\" algorytmu. Skuteczność zależy od konkretnego problemu, danych i kontekstu biznesowego. Najlepsze podejście to przetestowanie kilku algorytmów i wybór tego, który najlepiej spełnia Twoje wymagania pod względem dokładności, interpretowalności i efektywności obliczeniowej."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
